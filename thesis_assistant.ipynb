{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNidlwLulEaclyALo8lAjAk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanjiadong0/chatbot-/blob/main/thesis_assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s04Q8trlh9nJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8008a577"
      },
      "source": [
        "# Task\n",
        "Program the ethics module for your thesis killer project based on the provided structure, focusing on authorship tracking, AI labelling, and human-in-the-loop prompts, orchestrated by an EthicsSupervisor Agent and utilizing the defined submodules (AI_Detector, Usage_Logger, HumanPromptChecker, AdvisorFeedbackSync, EthicalViolationAlert) and interfaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d2bc5dd"
      },
      "source": [
        "## Define the scope of the ethics module\n",
        "\n",
        "### Subtask:\n",
        "Clarify what aspects of ethics the module should address within the context of your thesis killer project, focusing on authorship tracking, AI labelling, and human-in-the-loop prompts, orchestrated by an EthicsSupervisor Agent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bdc5a79"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the ethical concerns, explain the relation of authorship tracking, AI labelling, and human-in-the-loop prompts, outline the responsibilities of the EthicsSupervisor Agent, and briefly explain the contribution of the submodules, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b709aa3a"
      },
      "source": [
        "# Task\n",
        "Program the ethics module for your thesis killer project based on the provided description, including the EthicsSupervisor Agent implemented as a Reinforcement Learning monitor, the specified submodules (AI_Detector, Usage_Logger, HumanPromptChecker, AdvisorFeedbackSync, EthicalViolationAlert), and the interfaces for connecting to external tools and logging information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ad5129f"
      },
      "source": [
        "## Define the scope of the ethics module\n",
        "\n",
        "### Subtask:\n",
        "Clarify what aspects of ethics the module should address within the context of your thesis killer project, focusing on authorship tracking, AI labelling, and human-in-the-loop prompts, orchestrated by an EthicsSupervisor Agent implemented as a Reinforcement Learning monitor.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0bd7ab6"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the ethical concerns, explain the relation of authorship tracking, AI labelling, and human-in-the-loop prompts, outline the responsibilities of the EthicsSupervisor Agent, and briefly explain the contribution of the submodules, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae65fbf7"
      },
      "source": [
        "## Identify relevant ethical guidelines or frameworks\n",
        "\n",
        "### Subtask:\n",
        "Research and select appropriate ethical principles or frameworks applicable to your project's domain, considering how they relate to the functions of the defined submodules and how these can be translated into states, actions, and reward signals for the RL-based EthicsSupervisor.\n",
        "\n",
        "**Reasoning**:\n",
        "Selecting appropriate ethical guidelines is crucial for ensuring the ethics module effectively addresses the challenges identified in the project's presentation. These guidelines will inform the design and implementation of the EthicsSupervisor and its submodules, as well as the definition of states, actions, and reward signals for the Reinforcement Learning approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1689d8f"
      },
      "source": [
        "## Design the module's structure\n",
        "\n",
        "### Subtask:\n",
        "Outline the components and functionalities of the ethics module, with the EthicsSupervisor Agent implemented as a Reinforcement Learning monitor that has access to agent decisions, user responses, LLM-generated content, timing logs, and human feedback loops. Define the roles and interactions of the submodules (AI_Detector, Usage_Logger, HumanPromptChecker, AdvisorFeedbackSync, and EthicalViolationAlert) and how they will utilize interfaces to connect with tools like GPTZero, Copyleaks, or custom DetectGPT, log timestamps, usage intent, and tool confidence, and use rules/classifiers for warnings and suggestions. Design how the information from these submodules and interfaces will be used as state, action, and reward signals for the RL model.\n",
        "\n",
        "**Reasoning**:\n",
        "A well-defined structure is essential for implementing a complex module like the ethics module. Clearly outlining the roles and interactions of the EthicsSupervisor, submodules, and interfaces, and specifically designing how information will be used for the RL model, will ensure a cohesive and functional design that addresses the identified ethical challenges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fa79dc9"
      },
      "source": [
        "## Design the module's structure\n",
        "\n",
        "### Subtask:\n",
        "Outline the components and functionalities of the ethics module, with the EthicsSupervisor Agent implemented as a Reinforcement Learning monitor that has access to agent decisions, user responses, LLM-generated content, timing logs, and human feedback loops. Define the roles and interactions of the submodules (AI_Detector, Usage_Logger, HumanPromptChecker, AdvisorFeedbackSync, and EthicalViolationAlert) and how they will utilize interfaces to connect with **AI detector tools**, log timestamps, usage intent, and tool confidence, and use rules/classifiers for warnings and suggestions. Design how the information from these submodules and interfaces will be used as state, action, and reward signals for the RL model.\n",
        "\n",
        "**Reasoning**:\n",
        "A well-defined structure is essential for implementing a complex module like the ethics module. Clearly outlining the roles and interactions of the EthicsSupervisor, submodules, and interfaces, and specifically designing how information will be used for the RL model, will ensure a cohesive and functional design that addresses the identified ethical challenges."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import OpenAIServerModel\n",
        "api_key = \"AIzaSyBNqQzrD75wV8WfGsV27VHUZ9j5ts5ihMg\"   # use some free api key\n",
        "model = OpenAIServerModel(\n",
        "    model_id=\"gemini-2.0-flash\", # the model I used\n",
        "    api_base=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        "    api_key=api_key,\n",
        ")"
      ],
      "metadata": {
        "id": "JsrtVPUa1qT5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import the OpenAI library\n",
        "from openai import OpenAI\n",
        "# Used to securely store your API key - uncomment if using Colab Secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get your OpenAI API key securely\n",
        "# Replace \"<YOUR_OPENAI_API_KEY>\" with your key, or use Colab Secrets\n",
        "# Or if using Colab Secrets:\n",
        "openai_api_key_secure = userdata.get('OPENAI_API_KEY')\n",
        "openai_organization = userdata.get('OPENAI_ORGANIZATION')\n",
        "openai_project = userdata.get('OPENAI_PROJECT_ID')\n",
        "\n",
        "# Set your project API key\n",
        "OpenAI.api_key = openai_api_key_secure\n",
        "# You must also set organization and project ID\n",
        "OpenAI.organization = openai_organization\n",
        "OpenAI.project = openai_project\n",
        "\n",
        "# Create the OpenAI client\n",
        "client = OpenAI(api_key= OpenAI.api_key)\n",
        "\n"
      ],
      "metadata": {
        "id": "5NDgGXE0tS15"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a request to the Chat Completions endpoint\n",
        "response = client.chat.completions.create(\n",
        "  # Specify the model\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    # Assign the correct role\n",
        "    {\"role\": \"user\",\n",
        "     \"content\": \"Write a polite reply accepting an AI Engineer job offer within 20 words.\"}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Brf7k9_ytsni",
        "outputId": "94f9c3c9-5fc4-4584-8e42-49472f3587d4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: Acceptance of Job Offer\n",
            "\n",
            "Dear [Hiring Manager's Name],\n",
            "\n",
            "I am thrilled to accept the AI Engineer position. Thank you for this opportunity!\n",
            "\n",
            "Best regards,  \n",
            "[Your Name]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a9d1e9e"
      },
      "source": [
        "import pandas as pd\n",
        "import time # Import time for simulating API call delay\n",
        "from openai import OpenAI # Import the OpenAI library\n",
        "# Used to securely store your API key\n",
        "# from google.colab import userdata # Uncomment if using Colab Secrets\n",
        "\n",
        "class EthicsModule:\n",
        "    def __init__(self, openai_client):\n",
        "        self.usage_logs = []\n",
        "        self.ai_detection_threshold = 0.7 # Simple threshold for AI detection\n",
        "        # Use the provided OpenAI client\n",
        "        self.client = openai_client\n",
        "\n",
        "\n",
        "    def log_usage(self, prompt, intent, thesis_stage=\"unknown\"):\n",
        "        \"\"\"Logs the usage of the thesis assistant with more details.\"\"\"\n",
        "        log_entry = {\n",
        "            'timestamp': pd.Timestamp.now(),\n",
        "            'prompt': prompt,\n",
        "            'intent': intent,\n",
        "            'thesis_stage': thesis_stage # Added thesis stage\n",
        "        }\n",
        "        self.usage_logs.append(log_entry)\n",
        "        print(f\"Usage logged: Timestamp={log_entry['timestamp']}, Prompt='{prompt}', Intent='{intent}', Thesis Stage='{thesis_stage}'\")\n",
        "\n",
        "    def generate_usage_embeddings(self):\n",
        "        \"\"\"Placeholder for generating embeddings from usage logs.\"\"\"\n",
        "        print(\"Generating usage log embeddings (placeholder)...\")\n",
        "        # In a real implementation, you would process self.usage_logs\n",
        "        # to create embeddings using an embedding model.\n",
        "        # These embeddings could then be used for analysis or as input to the RL model state.\n",
        "        pass # No actual embedding generation in this simple model\n",
        "\n",
        "\n",
        "    def detect_ai(self, text):\n",
        "        \"\"\"Uses the OpenAI LLM to assess if content is AI generated.\"\"\"\n",
        "        print(\"Using OpenAI LLM for AI detection...\")\n",
        "        try:\n",
        "            # Craft a prompt for the LLM to assess AI generation\n",
        "            # This prompt might need refinement for better results\n",
        "            prompt_text = f\"Assess the likelihood that the following text was generated by an AI. Respond ONLY with a score between 0 and 1, where 1 is highly likely to be AI generated, followed by a brief explanation on a new line.\\n\\nText to assess:\\n{text}\\n\\nScore:\"\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\", # Or another suitable model\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an AI text detection assistant.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt_text}\n",
        "                ],\n",
        "                max_tokens=50 # Restrict tokens to manage cost\n",
        "            )\n",
        "\n",
        "            # Attempt to parse the score from the LLM's response\n",
        "            response_text = response.choices[0].message.content.strip()\n",
        "            print(f\"LLM Raw Response: {response_text}\") # Print raw response for debugging\n",
        "            try:\n",
        "                # Assuming the LLM starts the response with the score on the first line\n",
        "                detection_score = float(response_text.splitlines()[0])\n",
        "            except (ValueError, IndexError):\n",
        "                print(f\"Could not parse score from LLM response: '{response_text}'. Assuming a default score.\")\n",
        "                detection_score = 0.5 # Default score if parsing fails\n",
        "\n",
        "            # Simulate some processing time (optional, but good for realism)\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            is_ai_generated = detection_score > self.ai_detection_threshold\n",
        "            print(f\"AI detection score from LLM: {detection_score:.2f}. Is likely AI: {is_ai_generated}\")\n",
        "            # Return the AI detection status, score, and potentially the full LLM response\n",
        "            return is_ai_generated, detection_score, response_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during OpenAI LLM AI detection: {e}\")\n",
        "            # Fallback in case of API errors\n",
        "            return False, 0.0, f\"Error: {e}\"\n",
        "\n",
        "\n",
        "    def check_ethical_usage(self, prompt, generated_text):\n",
        "        \"\"\"Basic check for ethical usage, combining prompt analysis and AI detection.\"\"\"\n",
        "        print(\"Checking for ethical usage...\")\n",
        "\n",
        "        # Basic Human Prompt Checker logic (simplified)\n",
        "        prompt_lower = prompt.lower()\n",
        "        if \"write my entire thesis\" in prompt_lower or \"do my whole thesis\" in prompt_lower:\n",
        "            print(\"Ethical Alert: Skeptical usage detected (attempting to write entire thesis). Encourage ethical use and own writing.\")\n",
        "        elif \"generate abstract\" in prompt_lower or \"write introduction\" in prompt_lower:\n",
        "             print(\"Ethical Note: AI used for structural writing. Remember to review and rephrase carefully.\")\n",
        "        elif \"analyze this concept\" in prompt_lower or \"explain this\" in prompt_lower:\n",
        "             print(\"Ethical Usage: AI used for understanding/analysis. Good practice!\")\n",
        "        else:\n",
        "            print(\"Prompt intent: Could be ethical, further analysis needed in a complex model.\")\n",
        "\n",
        "\n",
        "        # Basic Ethical Violation Alert logic (simplified, tied to AI detection and prompt analysis)\n",
        "        is_ai, score, llm_response = self.detect_ai(generated_text)\n",
        "\n",
        "        if is_ai and (\"write my entire thesis\" in prompt_lower or \"do my whole thesis\" in prompt_lower):\n",
        "            print(\"Ethical VIOLATION Alert: High potential for academic dishonesty due to prompt and AI content.\")\n",
        "        elif is_ai:\n",
        "            print(\"Ethical Alert: Potential AI-generated content detected. Encourage rephrasing.\")\n",
        "\n",
        "        # Example of checking for over-reliance (very basic) - in a real model, this would look at usage patterns over time\n",
        "        # This basic check uses the length of usage logs and checks recent prompts for \"generate\"\n",
        "        if len(self.usage_logs) > 5 and all(\"generate\" in entry['prompt'].lower() for entry in self.usage_logs[-5:]):\n",
        "             print(\"Ethical Alert: Potential over-reliance on AI generation detected. Encourage critical thinking and original writing.\")\n",
        "\n",
        "\n",
        "# Example Usage (after creating and initializing the openai client):\n",
        "# Make sure the 'client' object is defined from a previous cell\n",
        "# ethics_module = EthicsModule(openai_client=client)\n",
        "# ethics_module.log_usage(\"help me understand this concept\", \"research\", thesis_stage=\"literature review\")\n",
        "# is_ai, score, llm_response = ethics_module.detect_ai(\"The quick brown fox jumps over the lazy dog.\")\n",
        "# print(f\"Detect AI Result: Is AI: {is_ai}, Score: {score:.2f}, LLM Response: {llm_response}\")\n",
        "# is_ai, score, llm_response = ethics_module.detect_ai(\"As an AI language model, I can help with that.\")\n",
        "# print(f\"Detect AI Result: Is AI: {is_ai}, Score: {score:.2f}, LLM Response: {llm_response}\")\n",
        "# ethics_module.check_ethical_usage(\"write my entire thesis\", \"Here is a thesis.\")\n",
        "# ethics_module.check_ethical_usage(\"analyze this concept\", \"Based on my training data, this concept is...\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "372fdee9",
        "outputId": "503c3974-739e-4af1-ed0b-143d2f8ef970"
      },
      "source": [
        "# Example Usage (after creating and initializing the openai client):\n",
        "\n",
        "# Make sure the 'client' object from a previous cell (like cell 5NDgGXE0tS15) is defined\n",
        "# by running that cell before this one.\n",
        "\n",
        "if 'client' in locals():\n",
        "    # Initialize the ethics module with the created client\n",
        "    # Make sure the EthicsModule class is defined in a previous cell\n",
        "    ethics_module = EthicsModule(openai_client=client)\n",
        "\n",
        "    print(\"--- Testing log_usage ---\")\n",
        "    ethics_module.log_usage(\"Help me find papers on natural language processing\", \"research\", thesis_stage=\"literature review\")\n",
        "    ethics_module.log_usage(\"Generate an outline for my introduction\", \"writing_support\", thesis_stage=\"introduction\")\n",
        "    print(\"\\nCurrent usage logs:\")\n",
        "    display(pd.DataFrame(ethics_module.usage_logs))\n",
        "\n",
        "    print(\"\\n--- Testing detect_ai ---\")\n",
        "    # Test with human-like text (shorter)\n",
        "    is_ai_human, score_human, llm_response_human = ethics_module.detect_ai(\"The quick brown fox jumps over the lazy dog. This is a short sentence.\")\n",
        "    print(f\"Test 1 Result: Is AI: {is_ai_human}, Score: {score_human:.2f}, LLM Response: {llm_response_human}\")\n",
        "\n",
        "    # Test with text likely generated by an AI (shorter)\n",
        "    is_ai_ai, score_ai, llm_response_ai = ethics_module.detect_ai(\"As an AI language model, I can assist you.\")\n",
        "    print(f\"Test 2 Result: Is AI: {is_ai_ai}, Score: {score_ai:.2f}, LLM Response: {llm_response_ai}\")\n",
        "\n",
        "    # Test with some placeholder generated text (shorter)\n",
        "    is_ai_generated, score_generated, llm_response_generated = ethics_module.detect_ai(\"Generated text about a topic.\")\n",
        "    print(f\"Test 3 Result: Is AI: {is_ai_generated}, Score: {score_generated:.2f}, LLM Response: {llm_response_generated}\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Testing check_ethical_usage ---\")\n",
        "    # Test with an ethical prompt and seemingly human text (shorter)\n",
        "    ethics_module.check_ethical_usage(\"analyze this concept\", \"Based on my understanding, this concept is complex.\")\n",
        "\n",
        "    # Test with a skeptical prompt and seemingly AI text (shorter)\n",
        "    ethics_module.check_ethical_usage(\"write my entire thesis\", \"Here is a short thesis summary.\")\n",
        "\n",
        "    # Test with an ethical prompt and text likely flagged as AI (shorter)\n",
        "    ethics_module.check_ethical_usage(\"explain this theory\", \"Based on my training data, this theory is interesting.\")\n",
        "\n",
        "    # Test simple over-reliance check (might require more log entries to trigger)\n",
        "    print(\"\\n--- Testing potential over-reliance check (might need more logs) ---\")\n",
        "    # Add more \"generate\" prompts to usage logs to potentially trigger over-reliance alert (shorter prompts)\n",
        "    for _ in range(5):\n",
        "        ethics_module.log_usage(\"generate text\", \"writing_support\")\n",
        "    ethics_module.check_ethical_usage(\"continue writing\", \"More text.\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: 'client' object not found. Please run the cell to set up the OpenAI client first.\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing log_usage ---\n",
            "Usage logged: Timestamp=2025-06-19 15:58:34.555343, Prompt='Help me find papers on natural language processing', Intent='research', Thesis Stage='literature review'\n",
            "Usage logged: Timestamp=2025-06-19 15:58:34.555446, Prompt='Generate an outline for my introduction', Intent='writing_support', Thesis Stage='introduction'\n",
            "\n",
            "Current usage logs:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                   timestamp  \\\n",
              "0 2025-06-19 15:58:34.555343   \n",
              "1 2025-06-19 15:58:34.555446   \n",
              "\n",
              "                                              prompt           intent  \\\n",
              "0  Help me find papers on natural language proces...         research   \n",
              "1            Generate an outline for my introduction  writing_support   \n",
              "\n",
              "        thesis_stage  \n",
              "0  literature review  \n",
              "1       introduction  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57228404-1a26-496e-a0f9-2213fd59a450\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>prompt</th>\n",
              "      <th>intent</th>\n",
              "      <th>thesis_stage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-06-19 15:58:34.555343</td>\n",
              "      <td>Help me find papers on natural language proces...</td>\n",
              "      <td>research</td>\n",
              "      <td>literature review</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-06-19 15:58:34.555446</td>\n",
              "      <td>Generate an outline for my introduction</td>\n",
              "      <td>writing_support</td>\n",
              "      <td>introduction</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57228404-1a26-496e-a0f9-2213fd59a450')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-57228404-1a26-496e-a0f9-2213fd59a450 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-57228404-1a26-496e-a0f9-2213fd59a450');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-873314c5-ee40-41fe-a606-643747d7275f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-873314c5-ee40-41fe-a606-643747d7275f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-873314c5-ee40-41fe-a606-643747d7275f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"ethics_module\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-06-19 15:58:34.555343\",\n        \"max\": \"2025-06-19 15:58:34.555446\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2025-06-19 15:58:34.555446\",\n          \"2025-06-19 15:58:34.555343\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Generate an outline for my introduction\",\n          \"Help me find papers on natural language processing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"intent\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"writing_support\",\n          \"research\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thesis_stage\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"introduction\",\n          \"literature review\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing detect_ai ---\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.1  \n",
            "The text is a well-known pangram and consists of simple, straightforward sentences that are common in human writing. The lack of complexity and the use of a familiar expression suggest a low likelihood of AI generation.\n",
            "AI detection score from LLM: 0.10. Is likely AI: False\n",
            "Test 1 Result: Is AI: False, Score: 0.10, LLM Response: 0.1  \n",
            "The text is a well-known pangram and consists of simple, straightforward sentences that are common in human writing. The lack of complexity and the use of a familiar expression suggest a low likelihood of AI generation.\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "This text is quite generic and could be easily produced by both an AI and a human, making it less likely to be definitively AI-generated.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "Test 2 Result: Is AI: False, Score: 0.20, LLM Response: 0.2  \n",
            "This text is quite generic and could be easily produced by both an AI and a human, making it less likely to be definitively AI-generated.\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "The text is vague and doesn't exhibit typical markers of AI generation, such as overly complex structures or lack of coherence. It seems more like a placeholder than a fully developed piece.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "Test 3 Result: Is AI: False, Score: 0.20, LLM Response: 0.2  \n",
            "The text is vague and doesn't exhibit typical markers of AI generation, such as overly complex structures or lack of coherence. It seems more like a placeholder than a fully developed piece.\n",
            "\n",
            "--- Testing check_ethical_usage ---\n",
            "Checking for ethical usage...\n",
            "Ethical Usage: AI used for understanding/analysis. Good practice!\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "The text is simple and lacks the hallmark characteristics of AI-generated content, such as elaborate phrasing or unusual sentence structure, making it more likely to be human-written.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "Checking for ethical usage...\n",
            "Ethical Alert: Skeptical usage detected (attempting to write entire thesis). Encourage ethical use and own writing.\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "The text is very brief and lacks complexity, making it difficult to determine, but it does not exhibit typical AI-produced verbosity or elaborate structure often found in AI-generated content.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "Checking for ethical usage...\n",
            "Ethical Usage: AI used for understanding/analysis. Good practice!\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.3  \n",
            "The text is simple and lacks complexity, but it could plausibly be written by a human reflecting on a theory. The phrase \"based on my training data\" is suggestive of AI, yet itâ€™s not strong enough to definit\n",
            "AI detection score from LLM: 0.30. Is likely AI: False\n",
            "\n",
            "--- Testing potential over-reliance check (might need more logs) ---\n",
            "Usage logged: Timestamp=2025-06-19 15:58:47.393860, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Usage logged: Timestamp=2025-06-19 15:58:47.393927, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Usage logged: Timestamp=2025-06-19 15:58:47.393943, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Usage logged: Timestamp=2025-06-19 15:58:47.393956, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Usage logged: Timestamp=2025-06-19 15:58:47.393967, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Checking for ethical usage...\n",
            "Prompt intent: Could be ethical, further analysis needed in a complex model.\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.1\n",
            "\n",
            "The text is too brief and lacks complexity or context, making it more likely to be human-generated, possibly as a placeholder or casual notation.\n",
            "AI detection score from LLM: 0.10. Is likely AI: False\n",
            "Ethical Alert: Potential over-reliance on AI generation detected. Encourage critical thinking and original writing.\n"
          ]
        }
      ]
    }
  ]
}