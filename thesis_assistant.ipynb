{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMZr3+w5w19dQBHZvaQekE3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanjiadong0/chatbot-/blob/main/thesis_assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s04Q8trlh9nJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8008a577"
      },
      "source": [
        "# Task\n",
        "Program the ethics module for the thesis assistant project based on the provided structure, focusing on authorship tracking, AI labelling, and human-in-the-loop prompts, orchestrated by an EthicsSupervisor Agent and utilizing the defined submodules (AI_Detector, Usage_Logger, HumanPromptChecker, AdvisorFeedbackSync, EthicalViolationAlert) and interfaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d2bc5dd"
      },
      "source": [
        "## Define the scope of the ethics module\n",
        "\n",
        "### Subtask:\n",
        "Clarify what aspects of ethics the module should address within the context of your thesis killer project, focusing on authorship tracking, AI labelling, and human-in-the-loop prompts, orchestrated by an EthicsSupervisor Agent.\n",
        "\n",
        "**Reasoning**:\n",
        "Describe the ethical concerns, explain the relation of authorship tracking, AI labelling, and human-in-the-loop prompts, outline the responsibilities of the EthicsSupervisor Agent, and briefly explain the contribution of the submodules, as per the instructions.\n",
        "\n",
        "# Task\n",
        "Program the ethics module for your thesis killer project based on the provided description, including the EthicsSupervisor Agent implemented as a Reinforcement Learning monitor, the specified submodules (AI_Detector, Usage_Logger, HumanPromptChecker, AdvisorFeedbackSync, EthicalViolationAlert), and the interfaces for connecting to external tools and logging information.\n",
        "\n",
        "## Define the scope of the ethics module\n",
        "\n",
        "### Subtask:\n",
        "Clarify what aspects of ethics the module should address within the context of your thesis killer project, focusing on authorship tracking, AI labelling, and human-in-the-loop prompts, orchestrated by an EthicsSupervisor Agent implemented as a Reinforcement Learning monitor.\n",
        "\n",
        "**Reasoning**:\n",
        "Describe the ethical concerns, explain the relation of authorship tracking, AI labelling, and human-in-the-loop prompts, outline the responsibilities of the EthicsSupervisor Agent, and briefly explain the contribution of the submodules, as per the instructions.\n",
        "\n",
        "## Identify relevant ethical guidelines or frameworks\n",
        "\n",
        "### Subtask:\n",
        "Research and select appropriate ethical principles or frameworks applicable to your project's domain, considering how they relate to the functions of the defined submodules and how these can be translated into states, actions, and reward signals for the RL-based EthicsSupervisor.\n",
        "\n",
        "**Reasoning**:\n",
        "Selecting appropriate ethical guidelines is crucial for ensuring the ethics module effectively addresses the challenges identified in the project's presentation. These guidelines will inform the design and implementation of the EthicsSupervisor and its submodules, as well as the definition of states, actions, and reward signals for the Reinforcement Learning approach.\n",
        "\n",
        "## Design the module's structure\n",
        "\n",
        "### Subtask:\n",
        "Outline the components and functionalities of the ethics module, with the EthicsSupervisor Agent implemented as a Reinforcement Learning monitor that has access to agent decisions, user responses, LLM-generated content, timing logs, and human feedback loops. Define the roles and interactions of the submodules (AI_Detector, Usage_Logger, HumanPromptChecker, AdvisorFeedbackSync, and EthicalViolationAlert) and how they will utilize interfaces to connect with AI detector tools, log timestamps, usage intent, and tool confidence, and use rules/classifiers for warnings and suggestions. Design how the information from these submodules and interfaces will be used as state, action, and reward signals for the RL model.\n",
        "\n",
        "**Reasoning**:\n",
        "A well-defined structure is essential for implementing a complex module like the ethics module. Clearly outlining the roles and interactions of the EthicsSupervisor, submodules, and interfaces, and specifically designing how information will be used for the RL model, will ensure a cohesive and functional design that addresses the identified ethical challenges."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import the OpenAI library\n",
        "from openai import OpenAI\n",
        "# Used to securely store your API key - uncomment if using Colab Secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get your OpenAI API key securely\n",
        "# Replace \"<YOUR_OPENAI_API_KEY>\" with your key, or use Colab Secrets\n",
        "# Or if using Colab Secrets:\n",
        "openai_api_key_secure = userdata.get('OPENAI_API_KEY')\n",
        "openai_organization = userdata.get('OPENAI_ORGANIZATION')\n",
        "openai_project = userdata.get('OPENAI_PROJECT_ID')\n",
        "\n",
        "# Set your project API key\n",
        "OpenAI.api_key = openai_api_key_secure\n",
        "# You must also set organization and project ID\n",
        "OpenAI.organization = openai_organization\n",
        "OpenAI.project = openai_project\n",
        "\n",
        "# Create the OpenAI client\n",
        "client = OpenAI(api_key= OpenAI.api_key)\n",
        "\n"
      ],
      "metadata": {
        "id": "5NDgGXE0tS15"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a request to the Chat Completions endpoint\n",
        "response = client.chat.completions.create(\n",
        "  # Specify the model\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    # Assign the correct role\n",
        "    {\"role\": \"user\",\n",
        "     \"content\": \"Write a polite reply accepting an AI Engineer job offer within 20 words.\"}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Brf7k9_ytsni",
        "outputId": "c1b5fe69-b98c-4f49-cc93-480fd543da8c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear [Hiring Manager's Name], \n",
            "\n",
            "Thank you for the offer. I am excited to accept the AI Engineer position! \n",
            "\n",
            "Best regards,  \n",
            "[Your Name]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a9d1e9e"
      },
      "source": [
        "import pandas as pd\n",
        "import time # Import time for simulating API call delay\n",
        "from openai import OpenAI # Import the OpenAI library\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata # Uncomment if using Colab Secrets\n",
        "from scipy.spatial import distance # Assuming scipy is installed\n",
        "\n",
        "class EthicsModule:\n",
        "    def __init__(self, openai_client):\n",
        "        self.usage_logs = []\n",
        "        self.usage_embeddings = [] # Initialize list to store embeddings\n",
        "        self.ai_detection_threshold = 0.7 # Simple threshold for AI detection\n",
        "        # Use the provided OpenAI client\n",
        "        self.client = openai_client\n",
        "\n",
        "\n",
        "    def log_usage(self, prompt, intent, thesis_stage=\"unknown\"):\n",
        "        \"\"\"Logs the usage of the thesis assistant with more details.\"\"\"\n",
        "        log_entry = {\n",
        "            'timestamp': pd.Timestamp.now(),\n",
        "            'prompt': prompt,\n",
        "            'intent': intent,\n",
        "            'thesis_stage': thesis_stage # Added thesis stage\n",
        "        }\n",
        "        self.usage_logs.append(log_entry)\n",
        "        print(f\"Usage logged: Timestamp={log_entry['timestamp']}, Prompt='{prompt}', Intent='{intent}', Thesis Stage='{thesis_stage}'\")\n",
        "        # Optionally generate embedding for the new log entry immediately\n",
        "        # self._generate_embedding_for_log(log_entry)\n",
        "\n",
        "\n",
        "    def _generate_embedding_for_log(self, log_entry):\n",
        "         \"\"\"Generates embedding for a single log entry and stores it.\"\"\"\n",
        "         try:\n",
        "             prompt_text = log_entry['prompt']\n",
        "             response = self.client.embeddings.create(\n",
        "                 model=\"text-embedding-3-small\", # Use the embedding model\n",
        "                 input=prompt_text\n",
        "             )\n",
        "             embedding = response.data[0].embedding\n",
        "             # Store the embedding along with a reference to the original log index\n",
        "             self.usage_embeddings.append({'embedding': embedding, 'original_index': len(self.usage_logs) - 1})\n",
        "             print(f\"Generated embedding for log entry {len(self.usage_logs) - 1}\")\n",
        "         except Exception as e:\n",
        "             print(f\"Error generating embedding for log entry: {e}\")\n",
        "\n",
        "\n",
        "    def generate_all_usage_embeddings(self):\n",
        "        \"\"\"Generates embeddings for all usage logs using OpenAI API.\"\"\"\n",
        "        print(\"Generating embeddings for all usage logs using OpenAI API...\")\n",
        "        self.usage_embeddings = [] # Clear existing embeddings\n",
        "        for i, log_entry in enumerate(self.usage_logs):\n",
        "            try:\n",
        "                prompt_text = log_entry['prompt']\n",
        "                response = self.client.embeddings.create(\n",
        "                    model=\"text-embedding-3-small\", # Use the embedding model\n",
        "                    input=prompt_text\n",
        "                )\n",
        "                embedding = response.data[0].embedding\n",
        "                # Store the embedding along with a reference to the original log index\n",
        "                self.usage_embeddings.append({'embedding': embedding, 'original_index': i})\n",
        "            except Exception as e:\n",
        "                 print(f\"Error generating embedding for log entry {i}: {e}\")\n",
        "\n",
        "        print(f\"Generated {len(self.usage_embeddings)} embeddings.\")\n",
        "\n",
        "\n",
        "    def find_similar_usage(self, query_prompt, n=3):\n",
        "        \"\"\"\n",
        "        Finds the n most similar usage logs based on prompt embedding similarity.\n",
        "\n",
        "        Args:\n",
        "            query_prompt (str): The prompt to find similar usage for.\n",
        "            n (int): The number of closest usage logs to find.\n",
        "\n",
        "        Returns:\n",
        "            list of dict: A list of dictionaries for the n most similar usage logs,\n",
        "                          each containing 'distance', 'original_log' (the full log entry).\n",
        "                          Returns an empty list if no embeddings are available.\n",
        "        \"\"\"\n",
        "        if not self.usage_embeddings:\n",
        "            print(\"No usage embeddings available to query.\")\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Generate embedding for the query prompt\n",
        "            query_response = self.client.embeddings.create(\n",
        "                model=\"text-embedding-3-small\", # Use the embedding model\n",
        "                input=query_prompt\n",
        "            )\n",
        "            query_embedding = query_response.data[0].embedding\n",
        "\n",
        "            distances = []\n",
        "            for item in self.usage_embeddings:\n",
        "                dist = distance.cosine(query_embedding, item['embedding'])\n",
        "                distances.append({\n",
        "                    \"distance\": dist,\n",
        "                    \"original_index\": item['original_index']\n",
        "                    })\n",
        "\n",
        "            distances_sorted = sorted(distances, key=lambda x: x['distance'])\n",
        "\n",
        "            # Get the original log entries for the n closest\n",
        "            similar_logs = []\n",
        "            for item in distances_sorted[0:n]:\n",
        "                original_log = self.usage_logs[item['original_index']]\n",
        "                similar_logs.append({\n",
        "                    \"distance\": item['distance'],\n",
        "                    \"original_log\": original_log\n",
        "                })\n",
        "\n",
        "            return similar_logs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during similar usage query: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "    def detect_ai(self, text):\n",
        "        \"\"\"Uses the OpenAI LLM to assess if content is AI generated.\"\"\"\n",
        "        print(\"Using OpenAI LLM for AI detection...\")\n",
        "        try:\n",
        "            # Craft a prompt for the LLM to assess AI generation\n",
        "            # This prompt might need refinement for better results\n",
        "            prompt_text = f\"Assess the likelihood that the following text was generated by an AI. Respond ONLY with a score between 0 and 1, where 1 is highly likely to be AI generated, followed by a brief explanation on a new line.\\n\\nText to assess:\\n{text}\\n\\nScore:\"\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\", # Or another suitable model\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an AI text detection assistant.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt_text}\n",
        "                ],\n",
        "                max_tokens=50 # Restrict tokens to manage cost\n",
        "            )\n",
        "\n",
        "            # Attempt to parse the score from the LLM's response\n",
        "            response_text = response.choices[0].message.content.strip()\n",
        "            print(f\"LLM Raw Response: {response_text}\") # Print raw response for debugging\n",
        "            try:\n",
        "                # Assuming the LLM starts the response with the score on the first line\n",
        "                detection_score = float(response_text.splitlines()[0])\n",
        "            except (ValueError, IndexError):\n",
        "                print(f\"Could not parse score from LLM response: '{response_text}'. Assuming a default score.\")\n",
        "                detection_score = 0.5 # Default score if parsing fails\n",
        "\n",
        "            # Simulate some processing time (optional, but good for realism)\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            is_ai_generated = detection_score > self.ai_detection_threshold\n",
        "            print(f\"AI detection score from LLM: {detection_score:.2f}. Is likely AI: {is_ai_generated}\")\n",
        "            # Return the AI detection status, score, and potentially the full LLM response\n",
        "            return is_ai_generated, detection_score, response_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during OpenAI LLM AI detection: {e}\")\n",
        "            # Fallback in case of API errors\n",
        "            return False, 0.0, f\"Error: {e}\"\n",
        "\n",
        "\n",
        "    def check_ethical_usage(self, prompt, generated_text):\n",
        "        \"\"\"Basic check for ethical usage, combining prompt analysis and AI detection.\"\"\"\n",
        "        print(\"Checking for ethical usage...\")\n",
        "\n",
        "        # Basic Human Prompt Checker logic (simplified)\n",
        "        prompt_lower = prompt.lower()\n",
        "        if \"write my entire thesis\" in prompt_lower or \"do my whole thesis\" in prompt_lower:\n",
        "            print(\"Ethical Alert: Skeptical usage detected (attempting to write entire thesis). Encourage ethical use and own writing.\")\n",
        "        elif \"generate abstract\" in prompt_lower or \"write introduction\" in prompt_lower:\n",
        "             print(\"Ethical Note: AI used for structural writing. Remember to review and rephrase carefully.\")\n",
        "        elif \"analyze this concept\" in prompt_lower or \"explain this\" in prompt_lower:\n",
        "             print(\"Ethical Usage: AI used for understanding/analysis. Good practice!\")\n",
        "        else:\n",
        "            print(\"Prompt intent: Could be ethical, further analysis needed in a complex model.\")\n",
        "\n",
        "\n",
        "        # Basic Ethical Violation Alert logic (simplified, tied to AI detection and prompt analysis)\n",
        "        is_ai, score, llm_response = self.detect_ai(generated_text)\n",
        "\n",
        "        if is_ai and (\"write my entire thesis\" in prompt_lower or \"do my whole thesis\" in prompt_lower):\n",
        "            print(\"Ethical VIOLATION Alert: High potential for academic dishonesty due to prompt and AI content.\")\n",
        "        elif is_ai:\n",
        "            print(\"Ethical Alert: Potential AI-generated content detected. Encourage rephrasing.\")\n",
        "\n",
        "        # Example of checking for over-reliance (very basic) - in a real model, this would look at usage patterns over time\n",
        "        # This basic check uses the length of usage logs and checks recent prompts for \"generate\"\n",
        "        if len(self.usage_logs) > 5 and all(\"generate\" in entry['prompt'].lower() for entry in self.usage_logs[-5:]):\n",
        "             print(\"Ethical Alert: Potential over-reliance on AI generation detected. Encourage critical thinking and original writing.\")\n",
        "\n",
        "\n",
        "# Example Usage (after creating and initializing the openai client):\n",
        "# Make sure the 'client' object is defined from a previous cell\n",
        "# ethics_module = EthicsModule(openai_client=client)\n",
        "# ethics_module.log_usage(\"help me understand this concept\", \"research\", thesis_stage=\"literature review\")\n",
        "# ethics_module.generate_all_usage_embeddings() # Generate embeddings after logging\n",
        "# similar_logs = ethics_module.find_similar_usage(\"find papers on NLP\")\n",
        "# print(\"\\nSimilar Usage Logs:\")\n",
        "# for log in similar_logs:\n",
        "#      print(f\"  - Distance: {log['distance']:.4f}, Prompt: '{log['original_log']['prompt']}'\")\n",
        "# is_ai, score, llm_response = ethics_module.detect_ai(\"The quick brown fox jumps over the lazy dog.\")\n",
        "# print(f\"Detect AI Result: Is AI: {is_ai}, Score: {score:.2f}, LLM Response: {llm_response}\")\n",
        "# is_ai, score, llm_response = ethics_module.detect_ai(\"As an AI language model, I can help with that.\")\n",
        "# print(f\"Detect AI Result: Is AI: {is_ai}, Score: {score:.2f}, LLM Response: {llm_response}\")\n",
        "# ethics_module.check_ethical_usage(\"write my entire thesis\", \"Here is a thesis.\")\n",
        "# ethics_module.check_ethical_usage(\"analyze this concept\", \"Based on my training data, this concept is...\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "372fdee9",
        "outputId": "de7fc8d7-77de-4bef-900f-0284813afc64"
      },
      "source": [
        "\n",
        "# Example Usage (after creating and initializing the openai client):\n",
        "\n",
        "if 'client' in locals():\n",
        "    # Initialize the ethics module with the created client\n",
        "    # Make sure the EthicsModule class is defined in a previous cell\n",
        "    ethics_module = EthicsModule(openai_client=client)\n",
        "\n",
        "    print(\"--- Testing log_usage ---\")\n",
        "    ethics_module.log_usage(\"Help me find papers on natural language processing\", \"research\", thesis_stage=\"literature review\")\n",
        "    ethics_module.log_usage(\"Generate an outline for my introduction\", \"writing_support\", thesis_stage=\"introduction\")\n",
        "    print(\"\\nCurrent usage logs:\")\n",
        "    display(pd.DataFrame(ethics_module.usage_logs))\n",
        "\n",
        "    print(\"\\n--- Testing detect_ai ---\")\n",
        "    # Test with human-like text (shorter)\n",
        "    is_ai_human, score_human, llm_response_human = ethics_module.detect_ai(\"The quick brown fox jumps over the lazy dog. This is a short sentence.\")\n",
        "    print(f\"Test 1 Result: Is AI: {is_ai_human}, Score: {score_human:.2f}, LLM Response: {llm_response_human}\")\n",
        "\n",
        "    # Test with text likely generated by an AI (shorter)\n",
        "    is_ai_ai, score_ai, llm_response_ai = ethics_module.detect_ai(\"As an AI language model, I can assist you.\")\n",
        "    print(f\"Test 2 Result: Is AI: {is_ai_ai}, Score: {score_ai:.2f}, LLM Response: {llm_response_ai}\")\n",
        "\n",
        "    # Test with some placeholder generated text (shorter)\n",
        "    is_ai_generated, score_generated, llm_response_generated = ethics_module.detect_ai(\"Generated text about a topic.\")\n",
        "    print(f\"Test 3 Result: Is AI: {is_ai_generated}, Score: {score_generated:.2f}, LLM Response: {llm_response_generated}\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Testing check_ethical_usage ---\")\n",
        "    # Test with an ethical prompt and seemingly human text (shorter)\n",
        "    ethics_module.check_ethical_usage(\"analyze this concept\", \"Based on my understanding, this concept is complex.\")\n",
        "\n",
        "    # Test with a skeptical prompt and seemingly AI text (shorter)\n",
        "    ethics_module.check_ethical_usage(\"write my entire thesis\", \"Here is a short thesis summary.\")\n",
        "\n",
        "    # Test with an ethical prompt and text likely flagged as AI (shorter)\n",
        "    ethics_module.check_ethical_usage(\"explain this theory\", \"Based on my training data, this theory is interesting.\")\n",
        "\n",
        "    # Test simple over-reliance check (might require more log entries to trigger)\n",
        "    print(\"\\n--- Testing potential over-reliance check (might need more logs) ---\")\n",
        "    # Add more \"generate\" prompts to usage logs to potentially trigger over-reliance alert (shorter prompts)\n",
        "    for _ in range(5):\n",
        "        ethics_module.log_usage(\"generate text\", \"writing_support\")\n",
        "    ethics_module.check_ethical_usage(\"continue writing\", \"More text.\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: 'client' object not found. Please run the cell to set up the OpenAI client first.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing log_usage ---\n",
            "Usage logged: Timestamp=2025-06-26 09:57:35.111499, Prompt='Help me find papers on natural language processing', Intent='research', Thesis Stage='literature review'\n",
            "Usage logged: Timestamp=2025-06-26 09:57:35.111808, Prompt='Generate an outline for my introduction', Intent='writing_support', Thesis Stage='introduction'\n",
            "\n",
            "Current usage logs:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                   timestamp  \\\n",
              "0 2025-06-26 09:57:35.111499   \n",
              "1 2025-06-26 09:57:35.111808   \n",
              "\n",
              "                                              prompt           intent  \\\n",
              "0  Help me find papers on natural language proces...         research   \n",
              "1            Generate an outline for my introduction  writing_support   \n",
              "\n",
              "        thesis_stage  \n",
              "0  literature review  \n",
              "1       introduction  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e5fc3689-61f2-47d2-8be8-33defedc153b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>prompt</th>\n",
              "      <th>intent</th>\n",
              "      <th>thesis_stage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-06-26 09:57:35.111499</td>\n",
              "      <td>Help me find papers on natural language proces...</td>\n",
              "      <td>research</td>\n",
              "      <td>literature review</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-06-26 09:57:35.111808</td>\n",
              "      <td>Generate an outline for my introduction</td>\n",
              "      <td>writing_support</td>\n",
              "      <td>introduction</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e5fc3689-61f2-47d2-8be8-33defedc153b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e5fc3689-61f2-47d2-8be8-33defedc153b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e5fc3689-61f2-47d2-8be8-33defedc153b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-794b376c-077c-409f-b2ac-4904c0676520\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-794b376c-077c-409f-b2ac-4904c0676520')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-794b376c-077c-409f-b2ac-4904c0676520 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"Error: 'client' object not found\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-06-26 09:57:35.111499\",\n        \"max\": \"2025-06-26 09:57:35.111808\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2025-06-26 09:57:35.111808\",\n          \"2025-06-26 09:57:35.111499\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Generate an outline for my introduction\",\n          \"Help me find papers on natural language processing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"intent\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"writing_support\",\n          \"research\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thesis_stage\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"introduction\",\n          \"literature review\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing detect_ai ---\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.1  \n",
            "The text is a well-known pangram and is very simple, which makes it likely to be human-generated as it lacks complexity or a distinctive style typical of AI-generated content.\n",
            "AI detection score from LLM: 0.10. Is likely AI: False\n",
            "Test 1 Result: Is AI: False, Score: 0.10, LLM Response: 0.1  \n",
            "The text is a well-known pangram and is very simple, which makes it likely to be human-generated as it lacks complexity or a distinctive style typical of AI-generated content.\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "The phrase is quite generic and could easily be spoken by a human or generated by an AI. However, it lacks the complexity and variety typical of more sophisticated AI outputs.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "Test 2 Result: Is AI: False, Score: 0.20, LLM Response: 0.2  \n",
            "The phrase is quite generic and could easily be spoken by a human or generated by an AI. However, it lacks the complexity and variety typical of more sophisticated AI outputs.\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2\n",
            "\n",
            "The text is vague and generic, but lacks clear indicators of AI generation such as repetitive phrasing, unnatural structure, or overtly complex language. It could plausibly have been written by a human.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "Test 3 Result: Is AI: False, Score: 0.20, LLM Response: 0.2\n",
            "\n",
            "The text is vague and generic, but lacks clear indicators of AI generation such as repetitive phrasing, unnatural structure, or overtly complex language. It could plausibly have been written by a human.\n",
            "\n",
            "--- Testing check_ethical_usage ---\n",
            "Checking for ethical usage...\n",
            "Ethical Usage: AI used for understanding/analysis. Good practice!\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "The text is straightforward and lacks the complexity or unusual phrasing often associated with AI-generated content. It reads more like a human expressing a simple thought.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "Checking for ethical usage...\n",
            "Ethical Alert: Skeptical usage detected (attempting to write entire thesis). Encourage ethical use and own writing.\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "The text is minimal and could be a straightforward human-written communication, but its vague nature and lack of detail make it slightly more likely to be AI-generated.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "Checking for ethical usage...\n",
            "Ethical Usage: AI used for understanding/analysis. Good practice!\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.3  \n",
            "The text is somewhat generic and could be characteristic of AI-generated content, but its simplicity and lack of complexity suggest it could just as easily be written by a human.\n",
            "AI detection score from LLM: 0.30. Is likely AI: False\n",
            "\n",
            "--- Testing potential over-reliance check (might need more logs) ---\n",
            "Usage logged: Timestamp=2025-06-26 09:57:45.985785, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Usage logged: Timestamp=2025-06-26 09:57:45.985937, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Usage logged: Timestamp=2025-06-26 09:57:45.985961, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Usage logged: Timestamp=2025-06-26 09:57:45.985978, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Usage logged: Timestamp=2025-06-26 09:57:45.985993, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Checking for ethical usage...\n",
            "Prompt intent: Could be ethical, further analysis needed in a complex model.\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: Score: 0.1\n",
            "\n",
            "The text is too brief and lacks any complex structure or content that would typically indicate AI generation. It reads more like a placeholder or incomplete thought.\n",
            "Could not parse score from LLM response: 'Score: 0.1\n",
            "\n",
            "The text is too brief and lacks any complex structure or content that would typically indicate AI generation. It reads more like a placeholder or incomplete thought.'. Assuming a default score.\n",
            "AI detection score from LLM: 0.50. Is likely AI: False\n",
            "Ethical Alert: Potential over-reliance on AI generation detected. Encourage critical thinking and original writing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18bd052c"
      },
      "source": [
        "\n",
        "### RL Optimizer\n",
        "This thesis simulates a reinforcement learning framework for thesis-writing assistance, combining behavior modeling, ethical oversight, advisor interaction dynamics, and noisy writing quality approximations. While true real-world RLHF reward models remain out of reach at this stage, this controlled simulation provides a sandbox to explore adaptive policy learning for human-in-the-loop academic coaching.\n",
        "\n",
        "## Proposed RL Agent State Structure\n",
        "\n",
        "This outlines a potential structure for the State that the Reinforcement Learning agent (overseeing project evolution and ethics) would observe. This state combines information from the Ethics Module with broader thesis progress details.\n",
        "\n",
        "\n",
        "The state would likely be represented as a numerical vector or a structured object that the RL model can process.\n",
        "\n",
        "**Components of the State:**\n",
        "\n",
        "1.  **Ethical State Features (from Ethics Module):**\n",
        "    *   **AI Detection Score:** The score from the `detect_ai` function for the most recent generated content (e.g., a value between 0 and 1).\n",
        "    *   **Prompt Classification:** A categorical or numerical representation of the last prompt's ethical classification (e.g., 0 for ethical, 1 for structural, 2 for skeptical/dangerous).\n",
        "    *   **Usage Frequency:** Metrics on recent LLM usage (e.g., number of LLM interactions in the last hour/day, proportion of \"generate\" prompts).\n",
        "    *   **Embedding Similarity:** The similarity score from `find_similar_usage` when querying the current prompt against past usage logs (e.g., the distance to the most similar ethical/skeptical past interaction).\n",
        "    *   **Ethical Alert Status:** Flags indicating if any ethical violations or warnings are currently active (e.g., binary flags for over-reliance alert, academic dishonesty alert).\n",
        "    *   **Human Engagement:** Metrics on user interaction with previous ethical interventions (e.g., did the user rephrase AI content, did they engage with a reflection prompt).\n",
        "\n",
        "2.  **Thesis Progress Features:**\n",
        "    *   **Current Thesis Stage:** A categorical or numerical representation of the current stage of the thesis (e.g., 0 for planning, 1 for literature review, 2 for methodology, 3 for writing, etc.).\n",
        "    *   **Task Completion:** Percentage of planned tasks completed for the current stage or overall project.\n",
        "    *   **Time-based Metrics:** Time spent on the project recently, time remaining until deadlines.\n",
        "    *   **Advisor Feedback Status:** A flag or metric indicating the presence and recency of unaddressed advisor feedback.\n",
        "\n",
        "3.  **Performance Features:**\n",
        "    *   **Work Quality Score:** A metric representing the quality of recent thesis work (this would be challenging to define and might require human evaluation or proxy metrics).\n",
        "    *   **Progress Rate:** A measure of how quickly tasks are being completed or milestones are being reached.\n",
        "\n",
        "**Combining the State:**\n",
        "\n",
        "These individual features would be combined into a single state representation that the RL agent's model can process. For a neural network-based RL model, this would typically be a flattened numerical vector. Categorical features would need to be appropriately encoded (e.g., one-hot encoding).\n",
        "\n",
        "**Next Steps for Implementation (for later):**\n",
        "\n",
        "*   Define the specific numerical or categorical representation for each state feature.\n",
        "*   Develop the logic within the thesis assistant to collect and compile this information into the state vector at each time step.\n",
        "*   Ensure the Ethics Module submodules (Usage_Logger, AI_Detector, etc.) are providing the necessary data points in a format that can be easily integrated into the state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXkiYjcX9aRp",
        "outputId": "c0e80091-c082-4344-fa06-8c9b9aab6e2c"
      },
      "source": [
        "!pip install streamlit gymnasium stable-baselines3\n",
        "!pip install numpy # Ensure numpy is installed if not already\n",
        "!pip install pandas # Ensure pandas is installed if not already\n",
        "!pip install scipy # Ensure scipy is installed if not already"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.46.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.6.0+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.44.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.6.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.46.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.46.0 watchdog-6.0.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Configuration Manager\n",
        "\n",
        "The `RLConfigManager` class is responsible for managing the centralized configuration of the Reinforcement Learning system for the Thesis Assistant's ethics module. This configuration is dynamic, allowing developers to define and modify key aspects of the RL environment and agent behavior without directly altering the core code.\n",
        "\n",
        "**Purpose:**\n",
        "The primary purpose of this class is to provide a persistent and easily modifiable way to store the settings that govern the RL agent's learning and decision-making process. This includes defining the observable state variables, the available actions the agent can take, the reward values associated with different events, and how each action influences the state.\n",
        "\n",
        "**Key Components:**\n",
        "- `CONFIG_FILE`: A class variable specifying the name of the JSON file used for storing the configuration (`rl_config.json`).\n",
        "- `load_config()`: A class method that loads the configuration from the `CONFIG_FILE`. If the file does not exist, it creates a default configuration and saves it.\n",
        "- `save_config(config)`: A class method that saves a given configuration dictionary to the `CONFIG_FILE`.\n",
        "\n",
        "**How to Use:**\n",
        "- To get the current configuration, call `RLConfigManager.load_config()`. This will return a dictionary containing the settings.\n",
        "- To update the configuration, modify the dictionary obtained from `load_config()` and then call `RLConfigManager.save_config(updated_config)`.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **Developer Dashboard:** The `DeveloperDashboard` uses `RLConfigManager` to load and save the configuration, allowing developers to interactively modify the settings via a Streamlit interface.\n",
        "- **Data Preprocessor:** The `DataPreprocessor` uses the configuration (specifically, `state_variables` and `reward_config`) to determine how to convert raw usage logs into state vectors and compute rewards.\n",
        "- **RL Environment:** The `EthicsSupervisorEnv` is built dynamically based on the configuration loaded by `RLConfigManager`, defining its observation space, action space, and state transition logic (`action_effects`).\n",
        "- **RL Training Loop and Trainer:** These components implicitly rely on the configuration loaded by `RLConfigManager` via the environment and preprocessor.\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "The configuration is stored in a JSON file (`rl_config.json`) and represented in Python as a dictionary with the following structure:\n",
        "- \"state_variables\": A list of strings, where each string is the name of a variable that constitutes the state observed by the RL agent.\n",
        "- \"actions\": A list of strings, where each string is a human-readable label for an action the RL agent can take. The index of the action in this list is the action ID used by the RL model.\n",
        "- \"reward_config\": A dictionary mapping event names (as found in usage logs) to numerical reward values.\n",
        "- \"action_effects\": A nested dictionary defining how actions change state variables. The outer keys are string representations of action indices, and the inner dictionaries map state variable names to the delta value (change) applied to that variable when the action is taken.\n"
      ],
      "metadata": {
        "id": "lGMt9HgODAiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 1 — CONFIGURATION MANAGER\n",
        "# ===========================================================\n",
        "import json\n",
        "import os\n",
        "import gymnasium as gym\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import random\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "class RLConfigManager:\n",
        "    \"\"\"\n",
        "    Manages centralized configuration for the dynamic Reinforcement Learning system.\n",
        "\n",
        "    This class handles loading and saving configuration settings for the RL environment,\n",
        "    including state variables, action space definitions, reward shaping values,\n",
        "    and the effects of actions on state variables.\n",
        "    \"\"\"\n",
        "\n",
        "    CONFIG_FILE = \"rl_config.json\"\n",
        "\n",
        "    @classmethod\n",
        "    def load_config(cls):\n",
        "        \"\"\"\n",
        "        Load RL configuration from a JSON file.\n",
        "\n",
        "        If the configuration file does not exist, a default configuration is created\n",
        "        and saved to the file.\n",
        "\n",
        "        Returns:\n",
        "            config (dict): The loaded configuration dictionary.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(cls.CONFIG_FILE):\n",
        "            # Define a default configuration if the file is not found\n",
        "            default_config = {\n",
        "                \"state_variables\": [\"embedding_drift\", \"ai_usage\", \"ethical_flags\", \"advisor_feedback\", \"timestep\"],\n",
        "                \"actions\": [\"Allow prompt\", \"Suggest reflection\", \"Ethical warning\", \"Suggest rewriting\", \"Advisor feedback reminder\", \"Disable AI feature\"],\n",
        "                \"reward_config\": {\"user_revised\": 2, \"ai_violation\": -3, \"advisor_positive\": 3, \"rewrite_accepted\": 1, \"milestone_completed\": 5, \"hallucination_detected\": -2},\n",
        "                \"action_effects\": {\"1\": {\"ai_usage\": -0.05, \"embedding_drift\": -0.05}, \"3\": {\"ethical_flags\": -0.1}, \"4\": {\"advisor_feedback\": 0.1}}\n",
        "            }\n",
        "            cls.save_config(default_config)\n",
        "        # Load the configuration from the JSON file\n",
        "        with open(cls.CONFIG_FILE, \"r\") as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    @classmethod\n",
        "    def save_config(cls, config):\n",
        "        \"\"\"\n",
        "        Save the current configuration to the JSON file.\n",
        "\n",
        "        Args:\n",
        "            config (dict): The configuration dictionary to save.\n",
        "        \"\"\"\n",
        "        with open(cls.CONFIG_FILE, \"w\") as f:\n",
        "            json.dump(config, f, indent=2) # Save with indentation for readability\n"
      ],
      "metadata": {
        "id": "aHkvncRKH_ue"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Developer Dashboard\n",
        "\n",
        "The `DeveloperDashboard` class provides a graphical user interface (GUI) built with Streamlit, allowing developers to interactively configure the Reinforcement Learning system. This dashboard simplifies the process of modifying the state space, action space, reward shaping, and action effects without directly editing the configuration JSON file.\n",
        "\n",
        "**Purpose:**\n",
        "The main purpose is to offer a user-friendly way for developers to experiment with and tune the RL agent's behavior and the environment's dynamics. This is crucial during the development and testing phases of the RL ethics supervisor.\n",
        "\n",
        "**Key Components:**\n",
        "- `__init__()`: Initializes the dashboard by loading the current configuration using `RLConfigManager.load_config()`.\n",
        "- `launch()`: The main method to launch the Streamlit application. It sets the title and calls methods to display and edit different parts of the configuration.\n",
        "- `edit_action_space()`: Displays the current actions and provides an input field and button to add new actions to the configuration.\n",
        "- `edit_reward_shaping()`: Displays sliders for each item in the `reward_config`, allowing developers to adjust the reward values.\n",
        "- `edit_action_effects()`: Provides input fields to define how a specific action (identified by index) affects a specific state variable, allowing developers to add these effects to the `action_effects` dictionary in the configuration.\n",
        "- `save_button()`: Displays a button that, when clicked, saves the currently modified configuration back to the `rl_config.json` file using `RLConfigManager.save_config()`.\n",
        "\n",
        "**How to Use:**\n",
        "- To run the dashboard, execute the `launch()` method of an instance of `DeveloperDashboard`. Note that Streamlit applications are typically run from the command line using `streamlit run your_script_name.py`. In a notebook environment, you might need specific integrations or run the relevant cell and access the output via a provided URL.\n",
        "- Use the input fields, sliders, and buttons in the web interface to modify the configuration settings.\n",
        "- Click the \"Save Full Configuration\" button to persist the changes.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **Configuration Manager:** The `DeveloperDashboard` directly interacts with `RLConfigManager` to load the initial configuration when launched and to save the updated configuration.\n",
        "- **RL System Components:** The changes made through the dashboard directly influence how the `DataPreprocessor`, `EthicsSupervisorEnv`, and the RL agent (`EthicsSupervisorRL`) behave when they load the updated configuration.\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "The dashboard works directly with the dictionary structure managed by `RLConfigManager`. Changes made in the UI are reflected in the `self.config` dictionary within the `DeveloperDashboard` instance before being saved."
      ],
      "metadata": {
        "id": "gr6FY9KeDlO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: DEVELOPER DASHBOARD (Class-Based Streamlit Interface with Full Docstrings)\n",
        "# ===========================================================\n",
        "\n",
        "class DeveloperDashboard:\n",
        "    \"\"\"\n",
        "    Streamlit-based developer interface for interactively updating RL configuration.\n",
        "\n",
        "    This dashboard allows developers to view and modify the action space, reward shaping,\n",
        "    and action effects defined in the RL configuration file.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the dashboard by loading the current configuration.\n",
        "        \"\"\"\n",
        "        self.config = RLConfigManager.load_config()\n",
        "\n",
        "    def launch(self):\n",
        "        \"\"\"\n",
        "        Launch the Streamlit dashboard interface.\n",
        "        \"\"\"\n",
        "        st.title(\"🎯 Thesis RL Developer Dashboard\")\n",
        "        self.edit_action_space()\n",
        "        self.edit_reward_shaping()\n",
        "        self.edit_action_effects()\n",
        "        self.save_button()\n",
        "\n",
        "    def edit_action_space(self):\n",
        "        \"\"\"\n",
        "        Display and edit the RL action space.\n",
        "\n",
        "        Developers can add new high-level intervention actions here.\n",
        "        \"\"\"\n",
        "        st.header(\"1️⃣ Manage Action Space\")\n",
        "        st.write(\"Define high-level interventions available to RL agent:\")\n",
        "\n",
        "        st.subheader(\"Current Actions:\")\n",
        "        # Display current actions with their indices\n",
        "        for idx, action in enumerate(self.config[\"actions\"]):\n",
        "            st.write(f\"**{idx}:** {action}\")\n",
        "\n",
        "        new_action = st.text_input(\"Add New Action:\")\n",
        "        if st.button(\"Add Action\"):\n",
        "            if new_action.strip(): # Ensure the input is not empty or just whitespace\n",
        "                self.config[\"actions\"].append(new_action.strip())\n",
        "                st.success(f\"✅ Added action: '{new_action}'\")\n",
        "\n",
        "    def edit_reward_shaping(self):\n",
        "        \"\"\"\n",
        "        Display and edit the reward shaping values.\n",
        "\n",
        "        Developers can adjust the reward values associated with key supervision signals.\n",
        "        \"\"\"\n",
        "        st.header(\"2️⃣ Edit Reward Shaping\")\n",
        "        st.write(\"Adjust reward values for key supervision signals:\")\n",
        "\n",
        "        # Create sliders for each reward configuration item\n",
        "        for key, val in self.config[\"reward_config\"].items():\n",
        "            new_val = st.slider(f\"Reward for {key}:\", -10, 10, val)\n",
        "            self.config[\"reward_config\"][key] = new_val\n",
        "\n",
        "    def edit_action_effects(self):\n",
        "        \"\"\"\n",
        "        Display and define the effects of actions on state variables.\n",
        "\n",
        "        Developers can specify how choosing a particular action changes the values\n",
        "        of specific state variables.\n",
        "        \"\"\"\n",
        "        st.header(\"3️⃣ Define Action Effects\")\n",
        "        st.write(\"Specify which state variables are influenced by actions:\")\n",
        "\n",
        "        action_idx = st.text_input(\"Action Index (integer):\", value=\"1\")\n",
        "        variable_name = st.text_input(\"State Variable (e.g. ai_usage):\")\n",
        "        delta = st.number_input(\"Delta Change (+/-):\", step=0.01, value=0.0)\n",
        "\n",
        "        if st.button(\"Add Effect\"):\n",
        "            # Add the defined effect to the configuration\n",
        "            effects = self.config.setdefault(\"action_effects\", {}) # Get or create action_effects dictionary\n",
        "            action_effect = effects.setdefault(str(action_idx), {}) # Get or create effects for the specific action index\n",
        "            action_effect[variable_name] = delta\n",
        "            st.success(f\"✅ Effect added: Action {action_idx} → {variable_name} += {delta}\")\n",
        "\n",
        "    def save_button(self):\n",
        "        \"\"\"\n",
        "        Provide a save button to persist updated configuration back to disk.\n",
        "        \"\"\"\n",
        "        if st.button(\"Save Full Configuration\"):\n",
        "            RLConfigManager.save_config(self.config)\n",
        "            st.success(\"✅ All changes saved successfully!\")\n"
      ],
      "metadata": {
        "id": "woYe1lCLIASQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Data Preprocessor\n",
        "\n",
        "The `DataPreprocessor` class is a crucial component of the RL training pipeline. Its role is to bridge the gap between the raw usage logs generated by the thesis assistant and the structured input required by the Reinforcement Learning environment and agent.\n",
        "\n",
        "**Purpose:**\n",
        "The main purpose is to transform detailed log entries, which capture various events and state information from a user's interaction with the assistant, into a standardized numerical state vector that the RL agent can observe and a corresponding reward signal based on the events in the log.\n",
        "\n",
        "**Key Components:**\n",
        "- `__init__(config)`: Initializes the preprocessor with the current RL configuration, which includes definitions of state variables and reward mapping.\n",
        "- `extract_state(log_entry)`: Takes a single log entry (a dictionary) and converts it into a normalized NumPy array representing the state vector. It selects the relevant information from the log entry based on the `state_variables` defined in the configuration.\n",
        "- `compute_reward(log_entry)`: Calculates the reward associated with a log entry. It looks for specific event keys within the log entry (as defined in the `reward_config` in the configuration) and sums up the corresponding reward values.\n",
        "\n",
        "**How to Use:**\n",
        "- Create an instance of `DataPreprocessor`, passing the loaded RL configuration: `preprocessor = DataPreprocessor(config)`.\n",
        "- For each raw usage log entry (a dictionary), call `preprocessor.extract_state(log_entry)` to get the state vector and `preprocessor.compute_reward(log_entry)` to get the reward.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **Configuration Manager:** The `DataPreprocessor` relies heavily on the configuration loaded by `RLConfigManager` to know which log attributes correspond to state variables and how to map events to reward values.\n",
        "- **RL Training Loop:** The `RLTrainingLoop` uses the `DataPreprocessor` to process batches of logs before feeding the resulting state and reward information (or using it to update the environment's state and compute rewards in a more interactive simulation) to the RL agent for training.\n",
        "- **Simulators (Synthetic Cohort/Student):** The simulator classes generate log entries that are in a format expected by the `DataPreprocessor`.\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "- It processes input in the form of dictionaries (log entries).\n",
        "- It outputs a NumPy array for the state and a float for the reward.\n",
        "- It uses the `state_variables` and `reward_config` from the RL configuration dictionary to perform the conversion and computation. The keys in `reward_config` are expected to potentially appear as boolean flags or other relevant values in the input `log_entry` dictionaries."
      ],
      "metadata": {
        "id": "tv9IOabADzxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 3 — DATA PREPROCESSOR (LOG TO STATE CONVERSION)\n",
        "# ===========================================================\n",
        "\n",
        "class DataPreprocessor:\n",
        "    \"\"\"\n",
        "    Converts raw usage logs into RL state vectors and reward labels.\n",
        "\n",
        "    This class is responsible for transforming the detailed log entries\n",
        "    from the thesis assistant's usage into a format (state vectors and rewards)\n",
        "    that the Reinforcement Learning agent can understand and use for training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initialize with the current RL configuration structure.\n",
        "\n",
        "        Args:\n",
        "            config (dict): Loaded RL configuration dictionary.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "\n",
        "    def extract_state(self, log_entry):\n",
        "        \"\"\"\n",
        "        Convert a single log entry into an RL state vector.\n",
        "\n",
        "        The state vector is constructed based on the state variables defined\n",
        "        in the loaded configuration. Values are normalized where appropriate.\n",
        "\n",
        "        Args:\n",
        "            log_entry (dict): A single usage log entry.\n",
        "\n",
        "        Returns:\n",
        "            state (np.ndarray): The normalized RL state vector.\n",
        "        \"\"\"\n",
        "        state = []\n",
        "        # Iterate through state variables defined in the config\n",
        "        for var in self.config[\"state_variables\"]:\n",
        "            if var == \"timestep\":\n",
        "                # For timestep, use deadline_ratio from the log entry and normalize (assuming max timestep is 100 for normalization)\n",
        "                state.append(log_entry.get(\"deadline_ratio\", 0.0))\n",
        "            else:\n",
        "                # For other variables, get the value directly from the log entry (defaulting to 0.0 if not present)\n",
        "                value = log_entry.get(var, 0.0)\n",
        "                # Ensure the value is a float for consistency\n",
        "                state.append(float(value))\n",
        "        return np.array(state, dtype=np.float32) # Ensure float32 dtype for compatibility with RL libraries\n",
        "\n",
        "\n",
        "    def compute_reward(self, log_entry):\n",
        "        \"\"\"\n",
        "        Compute the shaped reward for a given log event.\n",
        "\n",
        "        The reward is calculated based on the 'reward_config' in the loaded\n",
        "        configuration and the presence of specific keys (representing events)\n",
        "        in the log entry.\n",
        "\n",
        "        Args:\n",
        "            log_entry (dict): A single usage log entry.\n",
        "\n",
        "        Returns:\n",
        "            reward (float): The computed reward value.\n",
        "        \"\"\"\n",
        "        reward = 0.0\n",
        "        # Iterate through the reward configuration items\n",
        "        for key, value in self.config[\"reward_config\"].items():\n",
        "            # Check if the key exists in the log entry and its value is True (for boolean events)\n",
        "            # This assumes the reward_config keys correspond to boolean flags in the log entry\n",
        "            if key in log_entry and log_entry.get(key) is True:\n",
        "                reward += value\n",
        "        return reward"
      ],
      "metadata": {
        "id": "wRlBWM72IBBr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: RL Environment (EthicsSupervisorEnv)\n",
        "\n",
        "The `EthicsSupervisorEnv` class is a custom Reinforcement Learning environment built using the Gymnasium library. It is designed to simulate the interaction between the Thesis Assistant's ethics module and a student's thesis writing process, allowing an RL agent to learn optimal intervention policies. A key feature is its dynamic nature, where the state space, action space, rewards, and state transitions are defined by the loaded configuration.\n",
        "\n",
        "**Purpose:**\n",
        "To provide a simulated environment where the RL agent (the Ethics Supervisor) can learn through trial and error. The environment presents states representing the current situation (ethical flags, AI usage, progress, etc.) and provides feedback (rewards) based on the agent's chosen actions and the resulting changes in the simulated student's state.\n",
        "\n",
        "**Key Components:**\n",
        "- `__init__(ethics_module, config)`: Initializes the environment. It takes a simulated or actual `ethics_module` object (which holds the state variables) and the RL configuration. It dynamically defines the `observation_space` and `action_space` based on the configuration.\n",
        "- `reset(seed=None)`: Resets the environment to an initial state at the beginning of a new simulation episode. It also resets the internal timestep counter.\n",
        "- `step(action)`: Executes one step in the environment based on the `action` taken by the RL agent. It computes a reward, applies the effects of the action to the state (using `_apply_action_effects`), increments the timestep, and determines if the episode is done.\n",
        "- `_get_state()`: Constructs the current state vector observed by the agent. It gathers the values of the state variables from the `ethics_module` object based on the configuration and normalizes them.\n",
        "- `_compute_reward(action)`: Calculates the reward signal for the current step. The provided implementation is a placeholder that considers the timestep and a simple cost related to the action index. In a more complete system, this would incorporate ethical outcomes, user feedback, advisor input, etc.\n",
        "- `_apply_action_effects(action)`: Updates the state variables in the `ethics_module` object based on the effects defined for the taken action in the configuration.\n",
        "\n",
        "**How to Use:**\n",
        "- Initialize the environment: `env = EthicsSupervisorEnv(mock_ethics_module_instance, config)`. The `mock_ethics_module_instance` should be an object (like `MockEthicsModule`) that holds the current values of the state variables defined in the config.\n",
        "- Call `env.reset()` to start a new episode.\n",
        "- In a training or inference loop, get an action from the RL agent and call `env.step(action)` to advance the simulation. The `step` method returns the next state, reward, and episode status.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **Mock Ethics Module:** The environment directly reads state variable values from and writes updated values to an instance of `MockEthicsModule` (or a similar object representing the system state).\n",
        "- **Configuration Manager:** The environment's fundamental structure (state and action spaces, action effects) is determined by the configuration loaded by `RLConfigManager`.\n",
        "- **PPO Supervisor (RL Agent):** The `EthicsSupervisorRL` class interacts with the `EthicsSupervisorEnv` to train the PPO model by calling `reset()` and `step()` and receiving state and reward information.\n",
        "- **Data Preprocessor:** While not directly used by the environment during a `step`, the state variables and reward structure defined in the configuration used by the environment are consistent with what the `DataPreprocessor` expects when processing raw logs.\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "- The environment's state is represented as a NumPy array (`observation_space`).\n",
        "- Actions are discrete integers (`action_space`).\n",
        "- It uses the `state_variables`, `actions`, and `action_effects` dictionaries from the RL configuration to define its behavior.\n",
        "- The `_compute_reward` method is a placeholder and would ideally use the `reward_config` from the configuration and events from the `ethics_module` state."
      ],
      "metadata": {
        "id": "nWuTD8KREBAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 4: RL ENVIRONMENT (Fully Dynamic Gym-Compatible Environment)\n",
        "# ===========================================================\n",
        "#\n",
        "# This module defines the RL environment the PPO agent interacts with.\n",
        "#\n",
        "# - Fully config-driven:\n",
        "#   - The state space (variables used)\n",
        "#   - The action space (interventions)\n",
        "#   - The reward function\n",
        "#   - The state update effects per action\n",
        "#\n",
        "# -----------------------------------------------------------\n",
        "# ✅ WHY FULLY DYNAMIC STATE?\n",
        "# -----------------------------------------------------------\n",
        "# - Allows easy expansion of system complexity.\n",
        "# - Developers can add new state features via config without touching any code.\n",
        "# - Keeps RL model compatible with evolving assistant behavior.\n",
        "#\n",
        "# -----------------------------------------------------------\n",
        "# ✅ KEY CONCEPTS (NOW FULLY DYNAMIC):\n",
        "# -----------------------------------------------------------\n",
        "# - State Variables: loaded from `state_variables` in config\n",
        "# - Action Effects: loaded from `action_effects` in config\n",
        "# - Observation Space: dynamically computed based on config\n",
        "# ===========================================================\n",
        "\n",
        "class EthicsSupervisorEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Fully dynamic Gym-compatible RL environment for the Thesis Assistant Ethics Supervisor.\n",
        "\n",
        "    This environment simulates the state of a student's thesis progress and ethical\n",
        "    interactions, allowing an RL agent to learn intervention policies. The environment's\n",
        "    structure (state space, action space, rewards, and transitions) is dynamically\n",
        "    defined by the provided configuration dictionary.\n",
        "\n",
        "    Attributes:\n",
        "        ethics_module (MockEthicsModule): A simulated or actual system state object\n",
        "                                           that holds the current values of the state variables.\n",
        "        config (dict): The loaded RL configuration dictionary.\n",
        "        state_variables (list): List of state variable names defined in the config.\n",
        "        actions (list): List of available action labels defined in the config.\n",
        "        action_effects (dict): Dictionary specifying how actions affect state variables (from config).\n",
        "        observation_space (gym.spaces.Box): Dynamically computed continuous state space.\n",
        "        action_space (gym.spaces.Discrete): Discrete action space based on the number of actions.\n",
        "        timestep (int): The current simulation step count within an episode.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ethics_module, config):\n",
        "        \"\"\"\n",
        "        Initialize the environment.\n",
        "\n",
        "        Args:\n",
        "            ethics_module (MockEthicsModule): An external system state simulator\n",
        "                                               (or actual system interface).\n",
        "            config (dict): The full RL configuration dictionary.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.ethics_module = ethics_module\n",
        "        self.config = config\n",
        "\n",
        "        self.state_variables = config[\"state_variables\"]\n",
        "        self.actions = config[\"actions\"]\n",
        "        self.action_effects = config.get(\"action_effects\", {})\n",
        "\n",
        "        # Fully dynamic observation space size based on the number of state variables\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=1, shape=(len(self.state_variables),), dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Discrete action space based on the number of defined actions\n",
        "        self.action_space = gym.spaces.Discrete(len(self.actions))\n",
        "        self.timestep = 0\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        \"\"\"\n",
        "        Reset the environment at the start of a new episode.\n",
        "\n",
        "        Args:\n",
        "            seed (int, optional): Seed for random number generation. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the initial state and an info dictionary.\n",
        "                   (state (np.array), info (dict))\n",
        "        \"\"\"\n",
        "        super().reset(seed=seed) # Call the parent reset method\n",
        "        self.timestep = 0\n",
        "        # Reset the ethics module state for a new episode (assuming ethics_module has a reset method)\n",
        "        if hasattr(self.ethics_module, 'reset'):\n",
        "             self.ethics_module.reset()\n",
        "        return self._get_state(), {} # Return the initial state and an empty info dict\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute one interaction step in the environment.\n",
        "\n",
        "        The agent selects an action, which may affect the environment's state.\n",
        "        A reward is computed, and the environment transitions to the next state.\n",
        "\n",
        "        Args:\n",
        "            action (int): The action index selected by the RL agent.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the next state, the reward, a 'done' flag,\n",
        "                   a 'truncated' flag, and an info dictionary.\n",
        "                   (state (np.array), reward (float), done (bool), truncated (bool), info (dict))\n",
        "        \"\"\"\n",
        "        # Compute the reward for the chosen action\n",
        "        reward = self._compute_reward(action)\n",
        "        # Apply the effects of the action to the environment's state\n",
        "        self._apply_action_effects(action)\n",
        "        self.timestep += 1\n",
        "        # Determine if the episode is finished (e.g., based on timestep limit)\n",
        "        done = (self.timestep >= 100) # Example termination condition\n",
        "        return self._get_state(), reward, done, False, {} # truncated is False, info is empty dict\n",
        "\n",
        "    def _get_state(self):\n",
        "        \"\"\"\n",
        "        Construct the normalized state vector fully dynamically.\n",
        "\n",
        "        This method gathers the current values of the state variables from the\n",
        "        `ethics_module` object based on the configuration and formats them\n",
        "        into a normalized NumPy array.\n",
        "\n",
        "        Returns:\n",
        "            np.array: The normalized state vector based on config-defined variables.\n",
        "        \"\"\"\n",
        "        state = []\n",
        "        # Iterate through the state variables defined in the configuration\n",
        "        for var in self.state_variables:\n",
        "            if var == \"timestep\":\n",
        "                # Normalize timestep (assuming maximum 100 steps for normalization)\n",
        "                state.append(self.timestep / 100.0)\n",
        "            else:\n",
        "                # Get the value from the ethics_module object, defaulting to 0.0 if the attribute doesn't exist\n",
        "                value = getattr(self.ethics_module, var, 0.0)\n",
        "                state.append(value)\n",
        "        return np.array(state, dtype=np.float32) # Ensure float32 dtype for compatibility with RL libraries\n",
        "\n",
        "    def _compute_reward(self, action):\n",
        "        \"\"\"\n",
        "        Compute the reward for the chosen action.\n",
        "\n",
        "        This is a placeholder reward function. In a real system, this would\n",
        "        be more complex, potentially incorporating feedback from the user,\n",
        "        advisor, and ethical violation signals.\n",
        "\n",
        "        Args:\n",
        "            action (int): The selected action index.\n",
        "\n",
        "        Returns:\n",
        "            float: The computed reward value.\n",
        "        \"\"\"\n",
        "        base_reward = 1.0\n",
        "        # Scale reward based on timestep (encouraging progress over time)\n",
        "        time_scaling = 1 + 2.0 * min(1.0, self.timestep / 80.0)\n",
        "        # Simple API cost based on action index (assuming higher indices are more \"costly\" actions)\n",
        "        api_cost = 0.002 * action\n",
        "        lambda_cost = 10 # Weight for the API cost\n",
        "        return base_reward * time_scaling - lambda_cost * api_cost\n",
        "\n",
        "    def _apply_action_effects(self, action):\n",
        "        \"\"\"\n",
        "        Dynamically apply the effects of the selected action to the system state.\n",
        "\n",
        "        Based on the 'action_effects' defined in the configuration, this method\n",
        "        updates the corresponding state variables in the `ethics_module` object.\n",
        "\n",
        "        Args:\n",
        "            action (int): The selected action index.\n",
        "        \"\"\"\n",
        "        # Get the effects defined for the chosen action (if any)\n",
        "        effects = self.action_effects.get(str(action), {})\n",
        "        # Iterate through the variables and their corresponding delta changes for this action\n",
        "        for variable, delta in effects.items():\n",
        "            current_value = getattr(self.ethics_module, variable, None)\n",
        "            if current_value is not None:\n",
        "                # Update the state variable, clipping the value between 0.0 and 1.0\n",
        "                updated_value = np.clip(current_value + delta, 0.0, 1.0)\n",
        "                setattr(self.ethics_module, variable, updated_value)\n"
      ],
      "metadata": {
        "id": "M4UvshTCIBv3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "055bb835"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because I am still incorrectly using `code_block` for markdown content. I will create a markdown cell to document the PPO Supervisor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: PPO Supervisor (EthicsSupervisorRL)\n",
        "\n",
        "The `EthicsSupervisorRL` class is responsible for managing the Proximal Policy Optimization (PPO) agent, which serves as the core Reinforcement Learning component of the ethics module. This class handles the initialization, training, saving, loading, and action recommendation (inference) for the PPO model.\n",
        "\n",
        "**Purpose:**\n",
        "To implement and control the RL agent that learns an optimal policy for intervening in the thesis writing process to promote ethical behavior and positive outcomes. It trains the agent by interacting with the `EthicsSupervisorEnv` and provides action recommendations based on the current state.\n",
        "\n",
        "**Key Components:**\n",
        "- `__init__(config, model_path=\"ethics_rl_model\")`: Initializes the PPO supervisor. It creates an instance of the `MockEthicsModule` (to represent the system state), initializes the `EthicsSupervisorEnv` using the provided configuration, and either loads a pre-trained PPO model from `model_path` or initializes a new PPO model if no saved model is found.\n",
        "- `train(timesteps=50000)`: Trains the PPO model for a specified number of environment interaction steps. It calls the `learn()` method of the Stable Baselines3 PPO model, which handles the data collection (interacting with the environment), policy optimization, and value function updates. After training, it saves the updated model.\n",
        "- `recommend_action()`: Takes the current state from the environment, feeds it to the trained PPO model's policy, and returns the recommended action index. This is the method used during online operation to get the agent's decision.\n",
        "\n",
        "**How to Use:**\n",
        "- Initialize the supervisor: `rl_supervisor = EthicsSupervisorRL(config, model_path=\"my_model\")`. This will either load an existing model or create a new one.\n",
        "- To train the model, call `rl_supervisor.train(timesteps=100000)`.\n",
        "- To get an action recommendation based on the current environment state, call `action = rl_supervisor.recommend_action()`.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **Configuration Manager:** The supervisor uses the configuration (loaded indirectly via the environment initialization) to define the RL problem (state/action spaces).\n",
        "- **RL Environment:** The supervisor interacts directly with the `EthicsSupervisorEnv` during training (calling `env.step()`) and inference (getting the state via `env._get_state()`).\n",
        "- **Mock Ethics Module:** The supervisor initializes an instance of `MockEthicsModule` and passes it to the environment. The environment then uses this object to manage the state.\n",
        "- **RL Training Loop:** The `RLTrainingLoop` uses the `EthicsSupervisorRL` instance to perform the actual training (`trainer.train()`) and potentially get action recommendations (`trainer.recommend_action()`) within its training orchestration.\n",
        "- **Synthetic Pretrainer:** The `SyntheticRLPretrainer` uses the `RLTrainingLoop`, which in turn uses `EthicsSupervisorRL`, to train the model on synthetic data.\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "- It manages a `stable_baselines3.PPO` model.\n",
        "- It interacts with the environment using NumPy arrays for states and integers for actions.\n",
        "- The PPO model's policy and value function are learned from the experience collected by interacting with the `EthicsSupervisorEnv`, which is configured using the dictionary loaded by `RLConfigManager`.\n",
        "\n",
        "\n",
        "===========================================================\n",
        "\n",
        "APPENDIX — PPO SUITABILITY ANALYSIS FOR THESIS ASSISTANT\n",
        "\n",
        "===========================================================\n",
        "\n",
        "Analysis: Strengths and Limitations of PPO for Thesis Assistant RL System\n",
        "\n",
        "✅ PROS (Why PPO is suitable globally):\n",
        "\n",
        "Stable policy optimization even in high-dimensional state spaces.\n",
        "\n",
        "Supports multiple complex actions (advisory interventions, ethical warnings, etc).\n",
        "\n",
        "Optimizes long-term reward (handles delayed ethical consequences).\n",
        "\n",
        "Clipping mechanism stabilizes policy updates (critical for safe ethical behavior).\n",
        "\n",
        "Can be trained globally across many users for a general ethical baseline.\n",
        "\n",
        "\n",
        "⚠ CONS (Limitations for personalization scenario):\n",
        "\n",
        "Requires many training samples to fully converge (sample inefficient).\n",
        "\n",
        "Slow adaptation when applied directly to new individual students.\n",
        "\n",
        "May not personalize fast enough during limited thesis timeframe (6-12 months).\n",
        "\n",
        "Potential difficulty adapting to individual personality shifts quickly.\n",
        "\n",
        "PPO only indirectly receives feedback via reward — few-shot adaptation is hard.\n",
        "\n",
        "\n",
        "✅ RECOMMENDED STRATEGY:\n",
        "\n",
        "Use PPO for global pretraining across many students (shared ethical policy).\n",
        "\n",
        "Introduce a lightweight per-student adaptation layer (small fine-tuning component).\n",
        "\n",
        "Combine PPO with human-in-the-loop reward shaping for rapid personalization.\n",
        "\n",
        "Consider hybrid architecture with PPO + bandits or meta-RL elements for few-shot adjustments.\n",
        "\n",
        "\n",
        "This hybrid design balances PPO’s global stability with efficient short-term personalization needs of the thesis assistant. \"\"\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QrXdtqPRETV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 5: PPO SUPERVISOR (Reinforcement Learning Agent Controller)\n",
        "# ===========================================================\n",
        "#\n",
        "# This module manages the PPO RL agent training, saving, loading, and inference.\n",
        "#\n",
        "# - Clean separation of agent control logic from environment definition.\n",
        "# - Compatible with stable-baselines3 PPO implementation.\n",
        "# - Supports continual training and model persistence.\n",
        "#\n",
        "# -----------------------------------------------------------\n",
        "# ✅ KEY CONCEPTS:\n",
        "# -----------------------------------------------------------\n",
        "# - PPO (Proximal Policy Optimization): modern stable RL algorithm\n",
        "# - Continual training: keep refining policy incrementally\n",
        "# - Safe reloading: easily resume training from saved checkpoints\n",
        "# ===========================================================\n",
        "\n",
        "class MockEthicsModule:\n",
        "    \"\"\"\n",
        "    A mock class simulating the state of the ethics module and thesis progress.\n",
        "\n",
        "    This class is used by the RL environment to represent the system state.\n",
        "    It includes attributes that correspond to the state variables defined\n",
        "    in the RL configuration.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the mock ethics module with random initial states.\n",
        "        \"\"\"\n",
        "        # Initialize attributes that the environment might try to access\n",
        "        self.embedding_drift = np.random.rand()\n",
        "        self.ai_usage = np.random.rand()\n",
        "        self.ethical_flags = np.random.rand()\n",
        "        self.advisor_feedback = np.random.rand()\n",
        "        self.deadline_ratio = np.random.rand() # Represents progress towards deadline (0.0 to 1.0)\n",
        "        self.emotion_state = np.random.rand() # Added emotion_state attribute\n",
        "        # Add attributes required by DataPreprocessor.extract_state (used for reward calculation)\n",
        "        self.user_revised = random.choice([True, False])\n",
        "        self.ai_violation = random.choice([True, False])\n",
        "        self.advisor_positive = random.choice([True, False])\n",
        "        self.rewrite_accepted = random.choice([True, False])\n",
        "        self.milestone_completed = random.choice([True, False])\n",
        "        self.hallucination_detected = random.choice([True, False])\n",
        "        self.prompt = \"mock prompt\" # Placeholder for prompt text\n",
        "        self.intent = \"mock intent\" # Placeholder for intent\n",
        "        self.thesis_stage = \"mock stage\" # Placeholder for thesis stage\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the mock ethics module state for a new episode.\n",
        "        \"\"\"\n",
        "        self.embedding_drift = np.random.rand()\n",
        "        self.ai_usage = np.random.rand()\n",
        "        self.ethical_flags = np.random.rand()\n",
        "        self.advisor_feedback = np.random.rand()\n",
        "        self.deadline_ratio = np.random.rand()\n",
        "        self.emotion_state = np.random.rand() # Added emotion_state attribute to reset\n",
        "        self.user_revised = random.choice([True, False])\n",
        "        self.ai_violation = random.choice([True, False])\n",
        "        self.advisor_positive = random.choice([True, False])\n",
        "        self.rewrite_accepted = random.choice([True, False])\n",
        "        self.milestone_completed = random.choice([True, False])\n",
        "        self.hallucination_detected = random.choice([True, False])\n",
        "        self.prompt = \"mock prompt\"\n",
        "        self.intent = \"mock intent\"\n",
        "        self.thesis_stage = \"mock stage\"\n",
        "\n",
        "\n",
        "class EthicsSupervisorRL:\n",
        "    \"\"\"\n",
        "    PPO Supervisor class controlling RL training and inference for the Ethics Supervisor.\n",
        "\n",
        "    This class initializes, trains, saves, and loads the PPO model that acts\n",
        "     as the RL agent for the ethics module. It interacts with the\n",
        "    `EthicsSupervisorEnv` to learn the optimal intervention policy.\n",
        "\n",
        "    Attributes:\n",
        "        ethics_module (MockEthicsModule): Simulated system state object.\n",
        "        env (EthicsSupervisorEnv): The RL environment instance.\n",
        "        model (PPO): The Stable Baselines3 PPO policy model.\n",
        "        model_path (str): The file path for saving and loading the PPO model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, model_path=\"ethics_rl_model\"):\n",
        "        \"\"\"\n",
        "        Initialize the PPO agent.\n",
        "\n",
        "        Args:\n",
        "            config (dict): The RL configuration dictionary.\n",
        "            model_path (str): The storage path for saving/loading the PPO model.\n",
        "                              Defaults to \"ethics_rl_model\".\n",
        "        \"\"\"\n",
        "\n",
        "        self.ethics_module = MockEthicsModule() # Initialize the mock ethics module\n",
        "        self.env = EthicsSupervisorEnv(self.ethics_module, config) # Initialize the RL environment\n",
        "        self.model_path = model_path\n",
        "\n",
        "        # Load an existing model if available, otherwise initialize a new one\n",
        "        if os.path.exists(model_path + \".zip\"):\n",
        "            self.model = PPO.load(model_path, env=self.env)\n",
        "            print(\"Loaded pretrained RL model.\")\n",
        "        else:\n",
        "            # Initialize a new PPO model with an MlpPolicy\n",
        "            self.model = PPO(\"MlpPolicy\", self.env, verbose=0) # verbose=0 to suppress training output\n",
        "            print(\"Initialized new PPO model.\")\n",
        "\n",
        "    def train(self, timesteps=50000):\n",
        "        \"\"\"\n",
        "        Train the PPO model for a specified number of timesteps.\n",
        "\n",
        "        The model interacts with the environment to collect experience and update\n",
        "        its policy.\n",
        "\n",
        "        Args:\n",
        "            timesteps (int): The total number of environment steps to train for.\n",
        "                             Defaults to 50000.\n",
        "        \"\"\"\n",
        "        self.model.learn(total_timesteps=timesteps)\n",
        "        self.model.save(self.model_path) # Save the model after training\n",
        "        print(\"Training complete and model saved.\")\n",
        "\n",
        "    def recommend_action(self):\n",
        "        \"\"\"\n",
        "        Predict the next action based on the current state of the environment.\n",
        "\n",
        "        This method uses the trained PPO policy to select an action given the\n",
        "        current observation from the environment.\n",
        "\n",
        "        Returns:\n",
        "            int: The action index selected by the PPO policy.\n",
        "        \"\"\"\n",
        "        state = self.env._get_state() # Get the current state from the environment\n",
        "        # Predict the action using the trained model in deterministic mode\n",
        "        action, _ = self.model.predict(state, deterministic=True)\n",
        "        # The action is returned as a NumPy array, so extract the scalar value\n",
        "        return int(action)"
      ],
      "metadata": {
        "id": "joWEdcaAICiM"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Continual RL Training Loop\n",
        "\n",
        "The `RLTrainingLoop` class is designed to orchestrate the process of continually training the Reinforcement Learning agent using batches of usage logs. It acts as a bridge between the raw data (logs) and the RL training process managed by the `EthicsSupervisorRL`.\n",
        "\n",
        "**Purpose:**\n",
        "To facilitate the training of the RL agent using collected usage data. It takes batches of logs, processes them into states and rewards using the `DataPreprocessor`, and then triggers the training of the PPO agent managed by the `EthicsSupervisorRL`. This allows for updating the agent's policy as new data becomes available.\n",
        "\n",
        "**Key Components:**\n",
        "- `__init__(config, model_path=\"ppo_ethics_model\")`: Initializes the training loop. It loads the configuration, initializes a `DataPreprocessor` instance, and initializes an `EthicsSupervisorRL` instance (which handles the PPO model).\n",
        "- `run_training_day(log_batch)`: The main method for processing a batch of logs. It iterates through each log entry in the `log_batch`, uses the `DataPreprocessor` to extract the state and compute the reward, prints this information, and then calls the `train()` method of the `EthicsSupervisorRL` instance to update the PPO model using the processed data. It also demonstrates getting a recommended action after training.\n",
        "\n",
        "**How to Use:**\n",
        "- Initialize the training loop: `training_loop = RLTrainingLoop(config)`.\n",
        "- Provide a batch of usage logs (a list of dictionaries) to the `run_training_day()` method: `training_loop.run_training_day(my_log_batch)`.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **Configuration Manager:** The training loop loads the configuration via `RLConfigManager` during its initialization.\n",
        "- **Data Preprocessor:** It uses the `DataPreprocessor` instance to convert raw log entries into state vectors and reward values.\n",
        "- **PPO Supervisor (RL Agent):** It uses the `EthicsSupervisorRL` instance to perform the actual training of the PPO model using the processed log data.\n",
        "- **Synthetic Pretrainer and RL Training Launcher:** These classes utilize the `RLTrainingLoop` to execute the training process, providing it with batches of logs (either synthetic or real).\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "- It takes a list of dictionaries (log entries) as input for training.\n",
        "- It uses the `DataPreprocessor` to work with state vectors (NumPy arrays) and reward values (floats).\n",
        "- The training process itself is managed by the `EthicsSupervisorRL` class, which interacts with the `EthicsSupervisorEnv` based on the configuration."
      ],
      "metadata": {
        "id": "WUK7NhspIN8Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "559dcaef"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because I am still incorrectly using `code_block` for markdown content. I will create a markdown cell to document the Continual RL Training Loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iiNne3wXIDt-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7jIbaC1_aCt"
      },
      "source": [
        "\n",
        "# PART 6 — CONTINUAL RL TRAINING LOOP\n",
        "# ===========================================================\n",
        "\n",
        "class RLTrainingLoop:\n",
        "    \"\"\"\n",
        "    Orchestrates the continual RL training process using batches of usage logs.\n",
        "\n",
        "    This class manages the flow of data from usage logs to the RL training process.\n",
        "    It uses the `DataPreprocessor` to convert logs into states and rewards and\n",
        "    the `EthicsSupervisorRL` to train the PPO agent.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, model_path=\"ppo_ethics_model\"):\n",
        "        \"\"\"\n",
        "        Initialize training loop components.\n",
        "\n",
        "        Args:\n",
        "            config (dict): The loaded RL configuration.\n",
        "            model_path (str): The path to the PPO model storage. Defaults to \"ppo_ethics_model\".\n",
        "        \"\"\"\n",
        "        self.config = RLConfigManager.load_config() # Load configuration\n",
        "        self.preprocessor = DataPreprocessor(self.config) # Initialize data preprocessor\n",
        "        # Initialize the RL trainer, passing the config and model path\n",
        "        self.trainer = EthicsSupervisorRL(self.config, model_path)\n",
        "\n",
        "    def run_training_day(self, log_batch):\n",
        "        \"\"\"\n",
        "        Process one batch of usage logs and train the PPO agent.\n",
        "\n",
        "        This method iterates through the provided log batch, processes each log\n",
        "        into a state and reward, and then uses these to train the RL agent.\n",
        "        Note: In a true online RL setting, training would occur more frequently\n",
        "        and interactively with the environment. This simulates batch training\n",
        "        from collected logs.\n",
        "\n",
        "        Args:\n",
        "            log_batch (list of dict): A list of usage logs for one training day or batch.\n",
        "        \"\"\"\n",
        "        print(f\"Processing batch of {len(log_batch)} logs for training...\")\n",
        "        # In a real RL loop, you would accumulate experiences (state, action, reward, next_state, done)\n",
        "        # from interacting with the environment, and then train the agent on these experiences.\n",
        "        # For this simulated training loop, we will process each log entry and call trainer.train\n",
        "        # with a small number of timesteps based on the batch size. This is a simplification\n",
        "        # and not a standard RL training loop, but demonstrates the integration points.\n",
        "        for log_entry in log_batch:\n",
        "            state_vector = self.preprocessor.extract_state(log_entry)\n",
        "            reward = self.preprocessor.compute_reward(log_entry)\n",
        "            # In a real scenario, you would step the environment with an action and get the next state and reward\n",
        "            # For this simulated training loop, we'll just print the processed info\n",
        "            print(f\"Processed Log → State: {state_vector}, Reward: {reward}\")\n",
        "\n",
        "        # The training method in EthicsSupervisorRL is named 'train'\n",
        "        # In a real RL loop, you would accumulate experiences and then train\n",
        "        # For this simulation, we'll call train after processing the batch, using the batch size as timesteps\n",
        "        # A more realistic approach would involve running episodes in the environment\n",
        "        # and training on the collected trajectories.\n",
        "        self.trainer.train(timesteps=len(log_batch)) # Train for the number of logs in the batch\n",
        "        # The prediction method in EthicsSupervisorRL is named 'recommend_action'\n",
        "        # This is just an example of how to use the trained model after training the batch\n",
        "        action = self.trainer.recommend_action()\n",
        "        print(f\"Recommended action after training on batch: {action}\")\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 7: Synthetic Thesis Student Simulator (ThesisStudentSimulator)\n",
        "\n",
        "The `ThesisStudentSimulator` class provides a way to generate synthetic data that mimics the progression and ethical interactions of a thesis student using the assistant. This simulator is essential for generating datasets to pre-train and test the Reinforcement Learning ethics supervisor in a controlled environment.\n",
        "\n",
        "**Purpose:**\n",
        "To create realistic (though simplified) sequences of events and state changes that a thesis student might experience over the course of their project. This synthetic data includes simulated metrics like AI usage, ethical flags, advisor feedback, and progress towards the deadline, which are used to train the RL agent.\n",
        "\n",
        "**Key Components:**\n",
        "- `__init__(student_type=\"Stable Performer\")`: Initializes the simulator for a specific type of student (e.g., \"Conservative\", \"Aggressive\", \"Struggling\", \"Stable Performer\"). Each student type has different tendencies influencing how their state variables evolve.\n",
        "- `evolve_one_step()`: Simulates one step in the student's thesis journey. It updates the state variables based on the student type and progress towards the deadline and generates a dictionary representing a single log entry for this step.\n",
        "- `grade_final_outcome(last_state)`: A static method that calculates a final grade and reward for a simulated thesis trajectory based on the state of the simulator at the end of the trajectory. This provides a terminal reward signal for the simulation.\n",
        "- `generate_full_trajectory_with_grading(student_type, trajectory_length=30)`: A static method that runs a full simulation trajectory for a specified student type and length, collecting all log entries and returning the list of logs along with the final grade and terminal reward.\n",
        "\n",
        "**How to Use:**\n",
        "- To simulate a single student's journey, create an instance: `student_sim = ThesisStudentSimulator(\"Struggling\")` and repeatedly call `student_sim.evolve_one_step()` to get step-by-step logs.\n",
        "- To generate a complete trajectory and its grading, call the static method: `logs, grade, reward = ThesisStudentSimulator.generate_full_trajectory_with_grading(\"Aggressive\", trajectory_length=50)`.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **Data Preprocessor:** The `evolve_one_step()` method generates log entries in a dictionary format that is compatible with the input expected by the `DataPreprocessor`.\n",
        "- **Thesis Cohort Simulator:** The `ThesisCohortSimulator` uses the `generate_full_trajectory_with_grading` method to create datasets for multiple students.\n",
        "- **Synthetic RL Pretrainer:** The `SyntheticRLPretrainer` utilizes the data generated by the simulator (via the `ThesisCohortSimulator`) to train the RL agent.\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "- The simulator maintains internal state variables (e.g., `ai_usage`, `ethical_flags`) as numerical values, typically floats between 0.0 and 1.0.\n",
        "- It generates output as dictionaries, where each dictionary represents a single usage log entry containing the state variables and simulated event flags.\n",
        "- The behavior is influenced by the `student_type` string."
      ],
      "metadata": {
        "id": "S_v_cF3uITia"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhLUsrnO_dDW"
      },
      "source": [
        "# PART 7 — SYNTHETIC THESIS STUDENT SIMULATOR WITH OUTCOME GRADING\n",
        "# ===========================================================\n",
        "\n",
        "class ThesisStudentSimulator:\n",
        "    \"\"\"\n",
        "    Simulates the progress and ethical behavior of a synthetic thesis student.\n",
        "\n",
        "    This class generates synthetic usage logs and simulates changes in state\n",
        "    variables (like AI usage, ethical flags, etc.) over time, based on\n",
        "    predefined student types. It also includes a method to grade the final\n",
        "    outcome of a simulated thesis trajectory.\n",
        "    \"\"\"\n",
        "    def __init__(self, student_type=\"Stable Performer\"):\n",
        "        \"\"\"\n",
        "        Initialize the simulator for a specific type of student.\n",
        "\n",
        "        Args:\n",
        "            student_type (str): The type of student to simulate\n",
        "                                 (\"Conservative\", \"Aggressive\", \"Struggling\",\n",
        "                                  \"Stable Performer\"). Defaults to \"Stable Performer\".\n",
        "        \"\"\"\n",
        "        self.student_type = student_type\n",
        "        # Initialize state variables\n",
        "        self.embedding_drift = 0.2\n",
        "        self.ai_usage = 0.3\n",
        "        self.ethical_flags = 0.05\n",
        "        self.advisor_feedback = 0.6\n",
        "        self.deadline_ratio = 0.0 # Represents progress towards deadline (0.0 to 1.0)\n",
        "\n",
        "    def evolve_one_step(self):\n",
        "        \"\"\"\n",
        "        Simulate one step of the student's thesis progress and generate a log entry.\n",
        "\n",
        "        State variables evolve based on the student type and time progression.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary representing a single usage log entry with updated state.\n",
        "        \"\"\"\n",
        "        # Simulate progress towards the deadline\n",
        "        self.deadline_ratio = min(self.deadline_ratio + 0.03, 1.0) # Increment deadline ratio\n",
        "\n",
        "        # Simulate changes in state variables based on student type\n",
        "        if self.student_type == \"Conservative\":\n",
        "            self.ai_usage += np.random.normal(0.01, 0.02)\n",
        "            self.ethical_flags += np.random.normal(0.0, 0.01)\n",
        "            self.advisor_feedback += np.random.normal(0.02, 0.05)\n",
        "            self.embedding_drift += np.random.normal(0.01, 0.02)\n",
        "        elif self.student_type == \"Aggressive\":\n",
        "            self.ai_usage += np.random.normal(0.05, 0.05)\n",
        "            self.ethical_flags += np.random.normal(0.02, 0.03)\n",
        "            self.advisor_feedback += np.random.normal(-0.02, 0.05)\n",
        "            self.embedding_drift += np.random.normal(0.03, 0.05)\n",
        "        elif self.student_type == \"Struggling\":\n",
        "            self.ai_usage += np.random.normal(0.03, 0.03)\n",
        "            self.ethical_flags += np.random.normal(0.05, 0.05)\n",
        "            self.advisor_feedback += np.random.normal(-0.03, 0.05)\n",
        "            self.embedding_drift += np.random.normal(0.04, 0.05)\n",
        "        elif self.student_type == \"Stable Performer\":\n",
        "            self.ai_usage += np.random.normal(0.02, 0.02)\n",
        "            self.ethical_flags += np.random.normal(0.01, 0.01)\n",
        "            self.advisor_feedback += np.random.normal(0.03, 0.04)\n",
        "            self.embedding_drift += np.random.normal(0.02, 0.02)\n",
        "\n",
        "        # Increase ethical flags towards the end of the project (simulating pressure)\n",
        "        if self.deadline_ratio > 0.8:\n",
        "            self.ethical_flags += 0.02\n",
        "\n",
        "        # Simulate some convergence towards target values as the deadline approaches\n",
        "        convergence_factor = self.deadline_ratio\n",
        "        self.ai_usage += (0.5 - self.ai_usage) * 0.1 * convergence_factor\n",
        "        self.ethical_flags += (0.1 - self.ethical_flags) * 0.1 * convergence_factor\n",
        "        self.advisor_feedback += (0.8 - self.advisor_feedback) * 0.1 * convergence_factor\n",
        "        self.embedding_drift += (0.3 - self.embedding_drift) * 0.05 * convergence_factor\n",
        "\n",
        "        # Clip state variables to remain within the [0, 1] range\n",
        "        self.ai_usage = np.clip(self.ai_usage, 0, 1.0)\n",
        "        self.ethical_flags = np.clip(self.ethical_flags, 0, 1.0)\n",
        "        self.advisor_feedback = np.clip(self.advisor_feedback, 0, 1.0)\n",
        "        self.embedding_drift = np.clip(self.embedding_drift, 0, 1.0)\n",
        "\n",
        "        # Create a log entry with the current state and some simulated events (for reward calculation)\n",
        "        log_entry = {\n",
        "            \"embedding_drift\": self.embedding_drift,\n",
        "            \"ai_usage\": self.ai_usage,\n",
        "            \"ethical_flags\": self.ethical_flags,\n",
        "            \"advisor_feedback\": self.advisor_feedback,\n",
        "            \"deadline_ratio\": self.deadline_ratio,\n",
        "            # Simulate boolean event flags based on probabilities or state\n",
        "            \"user_revised\": random.random() < 0.6, # Probability of user revising content\n",
        "            \"ai_violation\": random.random() < self.ethical_flags, # Higher ethical flags increase chance of violation\n",
        "            \"advisor_positive\": random.random() < self.advisor_feedback, # Higher feedback increases chance of positive advisor event\n",
        "            \"rewrite_accepted\": random.random() < 0.7, # Probability of rewrite suggestion being accepted\n",
        "            \"milestone_completed\": random.random() < 0.4, # Probability of completing a milestone\n",
        "            \"hallucination_detected\": random.random() < 0.1 # Probability of detecting a hallucination\n",
        "        }\n",
        "        return log_entry\n",
        "\n",
        "    @staticmethod\n",
        "    def grade_final_outcome(last_state):\n",
        "        \"\"\"\n",
        "        Grades the final outcome of a simulated thesis based on the last state.\n",
        "\n",
        "        This is a simplified grading function for simulation purposes.\n",
        "\n",
        "        Args:\n",
        "            last_state (dict): The final state of the simulator after a trajectory.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the grade (\"Excellent\", \"Acceptable\", \"Failed\")\n",
        "                   and a corresponding numerical reward.\n",
        "        \"\"\"\n",
        "        # Calculate penalties and bonuses based on the final state values\n",
        "        ai_penalty = (last_state[\"ai_usage\"] - 0.5) * 0.5 # Penalty if AI usage is high relative to 0.5\n",
        "        ethics_penalty = last_state[\"ethical_flags\"] * 1.5 # Penalty for ethical flags\n",
        "        advisor_bonus = last_state[\"advisor_feedback\"] * 2.0 # Bonus for positive advisor feedback\n",
        "        embedding_penalty = last_state[\"embedding_drift\"] * 0.3 # Penalty for high embedding drift\n",
        "        # Calculate total score\n",
        "        total_score = advisor_bonus - ethics_penalty - ai_penalty - embedding_penalty\n",
        "\n",
        "        # Assign grade and reward based on the total score\n",
        "        if total_score > 1.2:\n",
        "            return \"Excellent\", 5.0\n",
        "        elif total_score > 0:\n",
        "            return \"Acceptable\", 2.0\n",
        "        else:\n",
        "            return \"Failed\", -5.0\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_full_trajectory_with_grading(student_type, trajectory_length=30):\n",
        "        \"\"\"\n",
        "        Generates a full simulated thesis trajectory for a student and grades it.\n",
        "\n",
        "        Args:\n",
        "            student_type (str): The type of student to simulate.\n",
        "            trajectory_length (int): The number of steps in the simulation trajectory.\n",
        "                                     Defaults to 30.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                   - logs (list of dict): The list of log entries generated during the trajectory.\n",
        "                   - grade (str): The final grade (\"Excellent\", \"Acceptable\", \"Failed\").\n",
        "                   - reward (float): The terminal reward associated with the final grade.\n",
        "        \"\"\"\n",
        "        student = ThesisStudentSimulator(student_type)\n",
        "        logs = []\n",
        "        # Evolve the student's state for the specified trajectory length\n",
        "        for _ in range(trajectory_length):\n",
        "            logs.append(student.evolve_one_step())\n",
        "        # Grade the final outcome based on the last state in the trajectory\n",
        "        grade, terminal_reward = ThesisStudentSimulator.grade_final_outcome(logs[-1])\n",
        "        return logs, grade, terminal_reward\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 8: Multi-Student Synthetic Cohort Generator (ThesisCohortSimulator)\n",
        "\n",
        "The `ThesisCohortSimulator` class is designed to generate a dataset of simulated thesis trajectories for an entire cohort of diverse synthetic students. This aggregated dataset is crucial for the initial pre-training of the Reinforcement Learning ethics supervisor, providing a broad range of scenarios and behaviors.\n",
        "\n",
        "**Purpose:**\n",
        "To efficiently create a large, varied dataset of synthetic student interactions and outcomes. This dataset is used to train the RL agent to develop a generalizable ethical policy across different student types before any potential per-student fine-tuning.\n",
        "\n",
        "**Key Components:**\n",
        "- `STUDENT_TYPES`: A class attribute list defining the different types of students that can be simulated (\"Conservative\", \"Aggressive\", \"Struggling\", \"Stable Performer\").\n",
        "- `generate_cohort_dataset(num_students=100, trajectory_length=30)`: A static method that is the primary function of this class. It generates the specified number of synthetic students, each with a randomly assigned type, runs a full simulation trajectory for each using the `ThesisStudentSimulator`, and collects all the resulting logs, final grades, and terminal rewards into a single dataset. It also prints a summary of the final grades distribution within the generated cohort.\n",
        "\n",
        "**How to Use:**\n",
        "- To generate a dataset for a cohort of 100 students with trajectories of 30 steps each, simply call the static method: `cohort_data = ThesisCohortSimulator.generate_cohort_dataset(num_students=100, trajectory_length=30)`. The returned `cohort_data` is a list where each element is a dictionary containing the student type, their full trajectory of logs, their final grade, and the terminal reward.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **Thesis Student Simulator:** The `ThesisCohortSimulator` relies heavily on the `ThesisStudentSimulator.generate_full_trajectory_with_grading` static method to produce individual student trajectories and outcomes.\n",
        "- **Synthetic RL Pretrainer:** The `SyntheticRLPretrainer` class uses the `generate_cohort_dataset` method to obtain the large pool of synthetic logs required for pre-training the RL agent. It then flattens the trajectories from this dataset into a single list of logs for the training process.\n",
        "- **RLTrainingLoop:** Although not directly interacted with by this class, the dataset generated here is ultimately fed into the `RLTrainingLoop` by the `SyntheticRLPretrainer`.\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "- The class uses the predefined `STUDENT_TYPES` list.\n",
        "- The output is a list of dictionaries, each representing a simulated student with their complete `trajectory` (a list of log dictionaries), their `grade` (string), and `final_reward` (float).\n"
      ],
      "metadata": {
        "id": "KWUxbeEDIcAf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d8623a5"
      },
      "source": [
        "# ===========================================================\n",
        "# PART 8 — MULTI-STUDENT SYNTHETIC COHORT GENERATOR\n",
        "# ===========================================================\n",
        "\n",
        "class ThesisCohortSimulator:\n",
        "    \"\"\"\n",
        "    Generates a dataset of simulated thesis trajectories for a cohort of students.\n",
        "\n",
        "    This class uses the `ThesisStudentSimulator` to create trajectories for\n",
        "    multiple students of different types, providing a dataset for training\n",
        "    and evaluating the RL agent.\n",
        "    \"\"\"\n",
        "    STUDENT_TYPES = [\"Conservative\", \"Aggressive\", \"Struggling\", \"Stable Performer\"]\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_cohort_dataset(num_students=100, trajectory_length=30):\n",
        "        \"\"\"\n",
        "        Generates a dataset of simulated thesis trajectories for a cohort.\n",
        "\n",
        "        Args:\n",
        "            num_students (int): The number of students to simulate. Defaults to 100.\n",
        "            trajectory_length (int): The number of steps in each student's trajectory.\n",
        "                                     Defaults to 30.\n",
        "\n",
        "        Returns:\n",
        "            list of dict: A list of dictionaries, where each dictionary represents\n",
        "                          a student and contains their trajectory, final grade,\n",
        "                          and terminal reward.\n",
        "        \"\"\"\n",
        "        dataset = []\n",
        "        grade_summary = {\"Excellent\": 0, \"Acceptable\": 0, \"Failed\": 0}\n",
        "        # Generate trajectories for the specified number of students\n",
        "        for _ in range(num_students):\n",
        "            # Randomly select a student type\n",
        "            student_type = random.choice(ThesisCohortSimulator.STUDENT_TYPES)\n",
        "            # Generate a full trajectory and grade for the student\n",
        "            logs, grade, terminal_reward = ThesisStudentSimulator.generate_full_trajectory_with_grading(\n",
        "                student_type, trajectory_length)\n",
        "            dataset.append({\n",
        "                \"student_type\": student_type,\n",
        "                \"trajectory\": logs,\n",
        "                \"grade\": grade,\n",
        "                \"final_reward\": terminal_reward\n",
        "            })\n",
        "            grade_summary[grade] += 1 # Count the grades for summary\n",
        "\n",
        "        # Print a summary of the generated cohort grades\n",
        "        print(\"Cohort Generation Complete:\")\n",
        "        for grade, count in grade_summary.items():\n",
        "            print(f\"  {grade}: {count} students\")\n",
        "        return dataset\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81e3dee2"
      },
      "source": [
        "## Part 9: Synthetic RL Pretraining Pipeline (SyntheticRLPretrainer)\n",
        "\n",
        "The `SyntheticRLPretrainer` class orchestrates the process of pre-training the Reinforcement Learning ethics supervisor using a large synthetic dataset generated by the `ThesisCohortSimulator`. This step is typically done before applying the RL agent to real student data to provide it with an initial understanding of the environment and ethical considerations.\n",
        "\n",
        "**Purpose:**\n",
        "To automate the generation of a comprehensive synthetic dataset and use it to train the `EthicsSupervisorRL` agent, establishing a foundational policy for ethical guidance.\n",
        "\n",
        "**Key Components:**\n",
        "- `__init__(config, model_path=\"ppo_ethics_model\")`: Initializes the pretrainer. It takes the RL configuration and the desired model path as input, and internally initializes an `RLTrainingLoop` instance, which in turn manages the `EthicsSupervisorRL` agent.\n",
        "- `run_synthetic_pretraining(num_students=100, trajectory_length=30)`: The main method to trigger the pretraining process. It first calls the `ThesisCohortSimulator.generate_cohort_dataset` method to get the synthetic data, then flattens the trajectories from all students into a single list of logs, and finally passes this aggregated list of logs to the `RLTrainingLoop.run_training_day` method to train the RL agent.\n",
        "\n",
        "**How to Use:**\n",
        "- To run the synthetic pretraining with default settings (100 students, 30 steps per trajectory), after initializing the `RLTrainingLauncher` (which initializes the `SyntheticRLPretrainer`), you would call `launcher.run_synthetic_full_pretraining()`. If you are using the `SyntheticRLPretrainer` directly, you would initialize it with a config and then call `pretrainer.run_synthetic_pretraining(num_students=200, trajectory_length=40)` to specify different parameters.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **RLConfigManager:** The pretrainer is initialized with the configuration loaded by `RLConfigManager`, which is then passed down to the `RLTrainingLoop` and `EthicsSupervisorRL`.\n",
        "- **ThesisCohortSimulator:** It directly calls the `ThesisCohortSimulator.generate_cohort_dataset` static method to obtain the synthetic training data.\n",
        "- **RLTrainingLoop:** It uses an instance of `RLTrainingLoop` to handle the actual process of feeding logs to the `DataPreprocessor` and training the `EthicsSupervisorRL` agent on this data.\n",
        "- **EthicsSupervisorRL:** The training of the PPO agent is managed by the `RLTrainingLoop` instance held within the pretrainer.\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "- It works with the list of dictionaries returned by the `ThesisCohortSimulator`, processing the `trajectory` lists within that dataset.\n",
        "- The training parameters (like the number of students and trajectory length) are passed as arguments to the `run_synthetic_pretraining` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3mkrlya_k3O"
      },
      "source": [
        "# ===========================================================\n",
        "# PART 9 — SYNTHETIC RL PRETRAINING PIPELINE\n",
        "# ===========================================================\n",
        "\n",
        "class SyntheticRLPretrainer:\n",
        "    \"\"\"\n",
        "    Manages the pretraining of the RL agent using synthetic thesis student data.\n",
        "\n",
        "    This class uses the `ThesisCohortSimulator` to generate a large dataset\n",
        "    of synthetic logs and then trains the RL agent (`EthicsSupervisorRL`)\n",
        "    using this data via the `RLTrainingLoop`.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, model_path=\"ppo_ethics_model\"):\n",
        "        \"\"\"\n",
        "        Initialize the synthetic pretrainer.\n",
        "\n",
        "        Args:\n",
        "            config (dict): The loaded RL configuration.\n",
        "            model_path (str): The path to the PPO model storage. Defaults to \"ppo_ethics_model\".\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        # Initialize the training loop with the config and model path\n",
        "        self.training_loop = RLTrainingLoop(config, model_path)\n",
        "\n",
        "    def run_synthetic_pretraining(self, num_students=100, trajectory_length=30):\n",
        "        \"\"\"\n",
        "        Runs the full synthetic pretraining pipeline.\n",
        "\n",
        "        Generates a synthetic cohort dataset and trains the RL agent on the\n",
        "        collected trajectories.\n",
        "\n",
        "        Args:\n",
        "            num_students (int): The number of synthetic students to generate data for.\n",
        "                                Defaults to 100.\n",
        "            trajectory_length (int): The length of each student's trajectory.\n",
        "                                     Defaults to 30.\n",
        "        \"\"\"\n",
        "        print(\"\\nGenerating synthetic cohort dataset for pretraining...\")\n",
        "        # Generate the synthetic dataset\n",
        "        dataset = ThesisCohortSimulator.generate_cohort_dataset(num_students, trajectory_length)\n",
        "        # Flatten the trajectories from all students into a single list of logs\n",
        "        all_logs = []\n",
        "        for student in dataset:\n",
        "            all_logs.extend(student[\"trajectory\"])\n",
        "\n",
        "        print(f\"Total synthetic logs for PPO training: {len(all_logs)}\")\n",
        "        # Run the training loop on the collected synthetic logs\n",
        "        self.training_loop.run_training_day(all_logs)\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1acaeb77"
      },
      "source": [
        "## Part 10: Final Launcher (RLTrainingLauncher)\n",
        "\n",
        "The `RLTrainingLauncher` class serves as the main entry point and master system launcher for the entire thesis RL assistant training and configuration system. It brings together all the previously defined components and provides different modes of operation for development, simulation, and training with real or synthetic data.\n",
        "\n",
        "**Purpose:**\n",
        "To provide a single interface for initializing the RL system components, launching the developer dashboard, running simulated training, handling real data training, managing online incremental updates, and executing the full synthetic pretraining pipeline.\n",
        "\n",
        "**Key Components:**\n",
        "- `__init__()`: Initializes the launcher by loading the RL configuration using `RLConfigManager`, initializing the `DataPreprocessor`, creating an instance of the `RLTrainingLoop` (which includes the `EthicsSupervisorRL` agent), and initializing the `SyntheticRLPretrainer`.\n",
        "- `launch_dashboard()`: Launches the Streamlit-based `DeveloperDashboard` for interactive configuration of the RL system.\n",
        "- `run_simulated_training(days=3, batch_size=5)`: Runs a step-by-step simulation of training using a `MockSimulator` to generate small batches of logs over several simulated \"days\". This mode is useful for basic testing and debugging of the training loop.\n",
        "- `run_real_training(real_logs)`: Takes a list of actual collected usage logs (`real_logs`) and feeds them into the `RLTrainingLoop` for training the RL agent on real-world data.\n",
        "- `run_online_incremental_training(incremental_logs)`: Designed for online learning scenarios. It takes a list of newly collected `incremental_logs` and uses the `RLTrainingLoop` to fine-tune the existing RL model with this new data.\n",
        "- `run_synthetic_full_pretraining(num_students=100, trajectory_length=30)`: Triggers the full synthetic pretraining pipeline by calling the `run_synthetic_pretraining` method of the `SyntheticRLPretrainer` instance. This generates a large synthetic dataset and trains the RL agent on it.\n",
        "\n",
        "**How to Use:**\n",
        "- Instantiate the launcher: `launcher = RLTrainingLauncher()`.\n",
        "- Select a mode of operation based on user input or script logic:\n",
        "    - `launcher.launch_dashboard()`: To start the configuration dashboard (requires Streamlit).\n",
        "    - `launcher.run_simulated_training(days=5, batch_size=10)`: To run a short simulated training session.\n",
        "    - `launcher.run_real_training(my_real_logs)`: To train with your collected real logs.\n",
        "    - `launcher.run_online_incremental_training(new_logs)`: To perform incremental online updates with new data.\n",
        "    - `launcher.run_synthetic_full_pretraining(num_students=200, trajectory_length=50)`: To run the comprehensive synthetic pretraining.\n",
        "- The `if __name__ == \"__main__\":` block provides a command-line-like interface to select the mode when the script is run directly.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **RLConfigManager:** Used during initialization to load the system configuration.\n",
        "- **DataPreprocessor:** An instance is held and used by the `RLTrainingLoop` for processing logs.\n",
        "- **RLTrainingLoop:** An instance is held and used by the launcher to perform training with both simulated and real/incremental logs.\n",
        "- **SyntheticRLPretrainer:** An instance is held and used to execute the full synthetic pretraining pipeline.\n",
        "- **DeveloperDashboard:** An instance is created and launched when the 'dashboard' mode is selected.\n",
        "- **MockSimulator:** An internal mock class used specifically by `run_simulated_training` to generate synthetic logs for that mode.\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "- Relies on the dictionary structure of the RL configuration loaded by `RLConfigManager`.\n",
        "- Processes lists of log dictionaries, as generated by the simulators or collected from real usage.\n",
        "- Uses numerical parameters (like `days`, `batch_size`, `num_students`, `trajectory_length`) to control the simulation and training processes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d228b936",
        "outputId": "bf9e3946-153c-4588-b3ce-4d2be03e5fe4"
      },
      "source": [
        "# ===========================================================\n",
        "# PART 10 — FINAL LAUNCHER\n",
        "# ===========================================================\n",
        "\n",
        "class RLTrainingLauncher:\n",
        "    \"\"\"\n",
        "    The main entry point and master system launcher for the thesis RL assistant training.\n",
        "\n",
        "    This class orchestrates the different training modes (dashboard, simulated,\n",
        "    real data, online, synthetic full pretraining) and initializes the\n",
        "    necessary components (`RLConfigManager`, `DataPreprocessor`,\n",
        "    `RLTrainingLoop`, `SyntheticRLPretrainer`).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the launcher by loading configuration and components.\n",
        "        \"\"\"\n",
        "        # Load the RL configuration\n",
        "        self.config = RLConfigManager.load_config()\n",
        "        # Initialize the data preprocessor\n",
        "        self.preprocessor = DataPreprocessor(self.config)\n",
        "        # Initialize the RL training loop (this also initializes the RL agent)\n",
        "        self.training_loop = RLTrainingLoop(self.config)\n",
        "        # Initialize the synthetic pretrainer\n",
        "        self.pretrainer = SyntheticRLPretrainer(self.config)\n",
        "\n",
        "    def launch_dashboard(self):\n",
        "        \"\"\"\n",
        "        Launch the Streamlit-based developer interface for configuring the RL system.\n",
        "        \"\"\"\n",
        "        dashboard = DeveloperDashboard() # Initialize the DeveloperDashboard\n",
        "        dashboard.launch() # Launch the dashboard\n",
        "\n",
        "    def run_simulated_training(self, days=3, batch_size=5):\n",
        "        \"\"\"\n",
        "        Simulate RL model training using synthetic logs generated step-by-step.\n",
        "\n",
        "        Args:\n",
        "            days (int): Number of training days to simulate. Defaults to 3.\n",
        "            batch_size (int): Number of logs to generate per day. Defaults to 5.\n",
        "        \"\"\"\n",
        "        print(\"\\nRunning Simulated Step-by-Step Training...\")\n",
        "        class MockSimulator:\n",
        "            \"\"\"\n",
        "            A mock simulator to generate synthetic log batches for step-by-step training.\n",
        "            \"\"\"\n",
        "            def generate_batch(self, batch_size):\n",
        "                \"\"\"\n",
        "                Generates a batch of mock log entries.\n",
        "\n",
        "                Args:\n",
        "                    batch_size (int): The number of logs to generate in the batch.\n",
        "\n",
        "                Returns:\n",
        "                    list of dict: A list of mock log entries.\n",
        "                \"\"\"\n",
        "                print(\"Generating mock log batch...\")\n",
        "                mock_logs = []\n",
        "                for _ in range(batch_size):\n",
        "                    # Generate placeholder data structured to match DataPreprocessor expectations\n",
        "                    mock_logs.append({\n",
        "                        \"embedding_drift\": np.random.rand(),\n",
        "                        \"ai_usage\": np.random.rand(),\n",
        "                        \"ethical_flags\": np.random.rand(),\n",
        "                        \"advisor_feedback\": np.random.rand(),\n",
        "                        \"deadline_ratio\": np.random.rand(),\n",
        "                        \"user_revised\": random.random() < 0.6,\n",
        "                        \"ai_violation\": random.random() < 0.1,\n",
        "                        \"advisor_positive\": random.random() < 0.8,\n",
        "                        \"rewrite_accepted\": random.random() < 0.7,\n",
        "                        \"milestone_completed\": random.random() < 0.4,\n",
        "                        \"hallucination_detected\": random.random() < 0.1,\n",
        "                        \"prompt\": \"mock prompt\",\n",
        "                        \"intent\": \"mock intent\",\n",
        "                        \"thesis_stage\": \"mock stage\"\n",
        "                    })\n",
        "                return mock_logs\n",
        "\n",
        "        self.simulator = MockSimulator() # Initialize the mock simulator\n",
        "\n",
        "        # Run simulation for the specified number of days\n",
        "        for day in range(days):\n",
        "            print(f\"\\nSimulated Day {day + 1}\")\n",
        "            logs = self.simulator.generate_batch(batch_size=batch_size)\n",
        "            # Run the training loop on the generated batch of logs\n",
        "            self.training_loop.run_training_day(logs)\n",
        "\n",
        "    def run_real_training(self, real_logs):\n",
        "        \"\"\"\n",
        "        Train the RL model using real usage logs.\n",
        "\n",
        "        Args:\n",
        "            real_logs (list of dict): Collected real usage logs to use in training.\n",
        "        \"\"\"\n",
        "        print(\"\\nTraining with Real Logs...\")\n",
        "        # Run the training loop on the provided real logs\n",
        "        self.training_loop.run_training_day(real_logs)\n",
        "\n",
        "    def run_online_incremental_training(self, incremental_logs):\n",
        "        \"\"\"\n",
        "        Run online incremental updates using newly gathered data.\n",
        "\n",
        "        This method simulates receiving new logs incrementally and using them\n",
        "        to fine-tune the already trained RL model.\n",
        "\n",
        "        Args:\n",
        "            incremental_logs (list of dict): New usage logs collected for fine-tuning.\n",
        "        \"\"\"\n",
        "        print(\"\\nIncremental Online Training...\")\n",
        "        # Run the training loop on the incremental logs for fine-tuning\n",
        "        self.training_loop.run_training_day(incremental_logs)\n",
        "\n",
        "    def run_synthetic_full_pretraining(self, num_students=100, trajectory_length=30):\n",
        "        \"\"\"\n",
        "        Runs the full synthetic pretraining pipeline using a cohort simulator.\n",
        "\n",
        "        Args:\n",
        "            num_students (int): The number of synthetic students to generate data for.\n",
        "                                Defaults to 100.\n",
        "            trajectory_length (int): The length of each student's trajectory.\n",
        "                                     Defaults to 30.\n",
        "        \"\"\"\n",
        "        print(\"\\nRunning Full Synthetic PPO Pretraining...\")\n",
        "        # Use the pretrainer component to run the synthetic pretraining\n",
        "        self.pretrainer.run_synthetic_pretraining(num_students, trajectory_length)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    launcher = RLTrainingLauncher() # Initialize the main launcher\n",
        "    print(\"RL Training System Entry Point\")\n",
        "    print(\"Modes: [dashboard] [train_simulated] [train_real] [train_online] [train_synthetic_full]\")\n",
        "    # Use a default mode for automated execution, or keep input() for interactive use\n",
        "    mode = \"train_synthetic_full\" # Set a default mode for testing\n",
        "    # mode = input(\"Mode: \").strip() # Uncomment for interactive mode\n",
        "\n",
        "    # Execute the selected mode\n",
        "    if mode == \"dashboard\":\n",
        "         # DeveloperDashboard is defined in this cell, so it can be launched directly.\n",
        "         # Note: Running Streamlit in a standard Jupyter cell might require specific setup\n",
        "         # or will just print a message indicating how to launch it externally.\n",
        "         launcher.launch_dashboard()\n",
        "    elif mode == \"train_simulated\":\n",
        "        launcher.run_simulated_training()\n",
        "    elif mode == \"train_real\":\n",
        "        print(\"Load your real usage logs into 'real_logs' and call launcher.run_real_training(real_logs)\")\n",
        "        # Example usage (commented out):\n",
        "        # real_logs = [...] # Load your real logs here\n",
        "        # launcher.run_real_training(real_logs)\n",
        "    elif mode == \"train_online\":\n",
        "        print(\"Load new incremental logs into 'incremental_logs' and call launcher.run_online_incremental_training(incremental_logs)\")\n",
        "        # Example usage (commented out):\n",
        "        # incremental_logs = [...] # Load your new logs here\n",
        "        # launcher.run_online_incremental_training(incremental_logs)\n",
        "    elif mode == \"train_synthetic_full\":\n",
        "        launcher.run_synthetic_full_pretraining()\n",
        "    else:\n",
        "        print(\"Invalid mode selected.\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized new PPO model.\n",
            "Initialized new PPO model.\n",
            "RL Training System Entry Point\n",
            "Modes: [dashboard] [train_simulated] [train_real] [train_online] [train_synthetic_full]\n",
            "\n",
            "Running Full Synthetic PPO Pretraining...\n",
            "\n",
            "Generating synthetic cohort dataset for pretraining...\n",
            "Cohort Generation Complete:\n",
            "  Excellent: 35 students\n",
            "  Acceptable: 22 students\n",
            "  Failed: 43 students\n",
            "Total synthetic logs for PPO training: 3000\n",
            "Processing batch of 3000 logs for training...\n",
            "Processed Log → State: [0.20184743 0.34422487 0.0121243  0.58310103 0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.24922457 0.36928666 0.         0.56264746 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.3032931  0.35751632 0.08334048 0.496303   0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.32839128 0.35030112 0.13932703 0.4532886  0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.39316162 0.37485874 0.1427046  0.31569722 0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.4506792  0.49342668 0.19132361 0.33509338 0.18      ], Reward: 6.0\n",
            "Processed Log → State: [0.45994866 0.5470283  0.27811402 0.36697996 0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.5206247  0.59025925 0.35935023 0.27050853 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.52698964 0.57442915 0.4085317  0.1837542  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.57656026 0.65246147 0.50945204 0.23007411 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.6006314  0.6482578  0.5243605  0.17783357 0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.55837524 0.64462525 0.53375214 0.1918001  0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.65380436 0.7011427  0.65282273 0.15812711 0.39      ], Reward: 6.0\n",
            "Processed Log → State: [0.6139057  0.74002177 0.73506665 0.20811707 0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.6553066  0.72191757 0.67307365 0.20092082 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.59760076 0.71040195 0.75058264 0.24119541 0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.66296077 0.78368497 0.7087899  0.3032083  0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.7331282  0.7950351  0.71913546 0.36752018 0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.78622377 0.8771436  0.803639   0.41668928 0.57      ], Reward: 0.0\n",
            "Processed Log → State: [0.9285451  0.87223095 0.81010556 0.40458506 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.97704816 0.92893827 0.790587   0.4628019  0.63      ], Reward: 7.0\n",
            "Processed Log → State: [0.95634496 0.93060654 0.84722054 0.41980436 0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.94624215 0.93646854 0.7963673  0.39153296 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.96020764 0.9343208  0.780811   0.44752687 0.72      ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.8949845  0.88538957 0.38037175 0.75      ], Reward: 5.0\n",
            "Processed Log → State: [0.9780493  0.89205897 0.99720687 0.27550125 0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.9687288  0.87414676 0.9097719  0.2517761  0.81      ], Reward: 7.0\n",
            "Processed Log → State: [0.89145595 0.862049   0.8808055  0.26418638 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.9070707  0.86146754 0.8230809  0.22249497 0.87      ], Reward: 5.0\n",
            "Processed Log → State: [0.9268238  0.855272   0.7935253  0.26240173 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.269356   0.32559296 0.06002363 0.6597857  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.27733436 0.3528197  0.10080208 0.5779859  0.06      ], Reward: 5.0\n",
            "Processed Log → State: [0.34628424 0.408075   0.16659842 0.62595034 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.32889873 0.43084094 0.25773084 0.56158704 0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.19092312 0.46400732 0.36766955 0.5097816  0.15      ], Reward: 6.0\n",
            "Processed Log → State: [0.19727424 0.5092993  0.43293852 0.5297789  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.17338093 0.5085567  0.4511319  0.5842466  0.21      ], Reward: 0.0\n",
            "Processed Log → State: [0.18527326 0.56187046 0.45979172 0.6152074  0.24      ], Reward: 4.0\n",
            "Processed Log → State: [0.1762008  0.6280256  0.55082875 0.6068204  0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.22561067 0.6668946  0.64984906 0.5936565  0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.2656134 0.6427302 0.7313116 0.5636698 0.33     ], Reward: 2.0\n",
            "Processed Log → State: [0.36069992 0.67452765 0.7883436  0.5844587  0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.42800727 0.66976374 0.8512298  0.5707689  0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.49032807 0.70188606 0.83403546 0.46992114 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.51495075 0.72145224 0.87714225 0.3684674  0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.49611622 0.71713275 0.90744406 0.290317   0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.51481396 0.75598073 0.9598561  0.18543446 0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.53260887 0.75378615 0.94419074 0.15214951 0.54      ], Reward: 4.0\n",
            "Processed Log → State: [0.60915166 0.7739148  0.94705534 0.15353976 0.57      ], Reward: 7.0\n",
            "Processed Log → State: [0.6163275  0.739      0.93744457 0.14261284 0.6       ], Reward: 6.0\n",
            "Processed Log → State: [0.658642   0.7264764  0.9410964  0.14215305 0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.7644201  0.7271466  0.98089087 0.04412315 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.8684571  0.7073358  0.9846533  0.05539133 0.69      ], Reward: 0.0\n",
            "Processed Log → State: [0.8965062  0.74543893 0.92544544 0.08000857 0.72      ], Reward: 6.0\n",
            "Processed Log → State: [0.98492527 0.747602   0.93715334 0.13021003 0.75      ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.7602069  0.8505459  0.18246894 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.759356   0.9006554  0.24828012 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.7986265  0.90887785 0.33889228 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.8113068  0.86138016 0.34764326 0.87      ], Reward: 7.0\n",
            "Processed Log → State: [1.         0.81885326 0.8983957  0.41345197 0.9       ], Reward: 5.0\n",
            "Processed Log → State: [0.30085933 0.3001004  0.06504719 0.5790962  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.3796824  0.26814649 0.11746974 0.5739048  0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.44345272 0.29036155 0.15172563 0.5475749  0.09      ], Reward: 7.0\n",
            "Processed Log → State: [0.45740697 0.28812018 0.2081002  0.5688205  0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.45060983 0.29580507 0.27326167 0.6034349  0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.51046306 0.36818612 0.33066693 0.54553545 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.50004786 0.4287728  0.3323987  0.5366507  0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.5580546  0.50920177 0.30231807 0.5187531  0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.55975527 0.53427345 0.36818725 0.6307074  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.59904534 0.55587214 0.32503012 0.5709936  0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.65542865 0.66993046 0.24643408 0.47860032 0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.66788507 0.77253467 0.2712208  0.49531353 0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.74339163 0.9230467  0.26180902 0.4794215  0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.76439196 0.97584486 0.30145368 0.5414796  0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.79183125 1.         0.35140684 0.5975383  0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.8513267  0.97396845 0.37337333 0.64964414 0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.8738454  0.9803358  0.33030698 0.5846352  0.51      ], Reward: 4.0\n",
            "Processed Log → State: [0.867615   1.         0.27405792 0.5625342  0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.9520559  1.         0.30490977 0.6258679  0.57      ], Reward: 7.0\n",
            "Processed Log → State: [0.94337106 1.         0.343299   0.7257926  0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.9684964  1.         0.33344975 0.7479792  0.63      ], Reward: 2.0\n",
            "Processed Log → State: [1.         1.         0.31974992 0.71598476 0.66      ], Reward: 7.0\n",
            "Processed Log → State: [0.9636499  0.9984162  0.34616143 0.6441535  0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.97480035 1.         0.34257552 0.650375   0.72      ], Reward: 6.0\n",
            "Processed Log → State: [0.9555044  1.         0.40545428 0.6201875  0.75      ], Reward: 0.0\n",
            "Processed Log → State: [0.99545956 1.         0.42830867 0.5510421  0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.92308426 1.         0.44138294 0.58612996 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.93154246 1.         0.45083877 0.519047   0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.9242963  0.9894597  0.48725995 0.4662288  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.8883626  0.98807067 0.5003561  0.44803688 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.23561455 0.30508757 0.05232745 0.66920364 0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.21300074 0.28446892 0.04342617 0.77629423 0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.22683662 0.32384542 0.03466643 0.73402935 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.21615651 0.310953   0.04526432 0.78296655 0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.22160639 0.32263595 0.04921005 0.875434   0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.19817644 0.35989195 0.03875109 0.8636042  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.20849858 0.38664958 0.0274974  0.9013279  0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.23825611 0.41518518 0.0211937  0.9051807  0.24      ], Reward: -1.0\n",
            "Processed Log → State: [0.29074726 0.43708187 0.01969557 0.8931649  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.3186314  0.4416961  0.02724919 0.93457854 0.3       ], Reward: 0.0\n",
            "Processed Log → State: [0.313213   0.48929387 0.02259375 0.9460532  0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.33924732 0.49463186 0.0276847  0.9192402  0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.3728316  0.51039135 0.02573831 0.8752394  0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.43588895 0.5003217  0.02568842 0.849704   0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.40856057 0.51924443 0.03190216 0.87459767 0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.418017   0.50939804 0.03707135 0.9259836  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.42489532 0.5126644  0.05058711 0.890965   0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.42550367 0.5088135  0.06251571 0.91944164 0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.3797667  0.519513   0.0512474  0.91246986 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.36702058 0.53455836 0.05961125 0.9685188  0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.3927699  0.54481244 0.03934844 0.96758    0.63      ], Reward: 2.0\n",
            "Processed Log → State: [0.45285365 0.5468979  0.03534464 1.         0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.47531542 0.5360499  0.04009252 1.         0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.4624258  0.54746306 0.04650781 0.92012244 0.72      ], Reward: 5.0\n",
            "Processed Log → State: [0.48146516 0.5427776  0.04351341 0.9524606  0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.48398414 0.57433873 0.05704946 0.94975495 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.47176746 0.56553674 0.08329518 1.         0.81      ], Reward: 0.0\n",
            "Processed Log → State: [0.45560476 0.5439642  0.10781486 0.9174312  0.84      ], Reward: 7.0\n",
            "Processed Log → State: [0.44900867 0.5376383  0.12848437 0.91986597 0.87      ], Reward: 2.0\n",
            "Processed Log → State: [0.49027088 0.5381995  0.14609902 0.898373   0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.25192922 0.33534667 0.05987622 0.5652402  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.28876942 0.25066176 0.044215   0.554645   0.06      ], Reward: 0.0\n",
            "Processed Log → State: [0.29857892 0.34954762 0.07730145 0.5783293  0.09      ], Reward: 7.0\n",
            "Processed Log → State: [0.2647754  0.40432698 0.12002499 0.64260846 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.34137395 0.4183967  0.09637555 0.52070093 0.15      ], Reward: 0.0\n",
            "Processed Log → State: [0.32121724 0.47330567 0.11360119 0.5205453  0.18      ], Reward: 0.0\n",
            "Processed Log → State: [0.32831487 0.4767081  0.14688934 0.5474849  0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.30752528 0.61379606 0.12290761 0.51382864 0.24      ], Reward: 0.0\n",
            "Processed Log → State: [0.24948661 0.68527436 0.14816023 0.49180278 0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.2209533  0.7279744  0.11890918 0.50388414 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.1765477  0.7303054  0.11209471 0.4949969  0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.22997709 0.7441352  0.1322315  0.41115335 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.1368347  0.7290727  0.12596363 0.41645563 0.39      ], Reward: 0.0\n",
            "Processed Log → State: [0.16887626 0.7640253  0.15596521 0.44927177 0.42      ], Reward: 0.0\n",
            "Processed Log → State: [0.23122689 0.81496537 0.1527025  0.40908465 0.45      ], Reward: 0.0\n",
            "Processed Log → State: [0.23604436 0.81416583 0.16640848 0.3957155  0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.1832761  0.7755134  0.12041283 0.40422153 0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.07367973 0.80393744 0.17303286 0.3690315  0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.08032884 0.8724129  0.18636096 0.38074785 0.57      ], Reward: 7.0\n",
            "Processed Log → State: [0.16908076 0.980801   0.15300247 0.4271965  0.6       ], Reward: 5.0\n",
            "Processed Log → State: [0.19211037 0.9524236  0.15370882 0.4297384  0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.24474384 0.9554412  0.12544474 0.5037196  0.66      ], Reward: 7.0\n",
            "Processed Log → State: [0.23656921 0.97512805 0.14158952 0.60307693 0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.18482101 0.9311212  0.17351344 0.51868045 0.72      ], Reward: 0.0\n",
            "Processed Log → State: [0.21563844 0.9507836  0.22582044 0.54834926 0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.3089946  1.         0.24263754 0.57761896 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.3952992  1.         0.25108716 0.5268687  0.81      ], Reward: 0.0\n",
            "Processed Log → State: [0.467837  1.        0.3096109 0.5375869 0.84     ], Reward: 7.0\n",
            "Processed Log → State: [0.5060315  1.         0.34928998 0.47860757 0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.5559613  1.         0.34140828 0.49228856 0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.22710763 0.2983559  0.03669138 0.560519   0.03      ], Reward: 7.0\n",
            "Processed Log → State: [0.26156285 0.28352666 0.02179596 0.5251589  0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.241122   0.25203028 0.02750658 0.4860464  0.09      ], Reward: 2.0\n",
            "Processed Log → State: [0.2582378  0.27247018 0.02184383 0.5829819  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.2988782  0.27099866 0.00919815 0.6165594  0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.30623055 0.34665567 0.01077214 0.63973194 0.18      ], Reward: 6.0\n",
            "Processed Log → State: [0.34079173 0.39553145 0.03989185 0.6170132  0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.35293487 0.41604105 0.03533321 0.6819543  0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.39051786 0.44805825 0.04243346 0.6976107  0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.39588907 0.4621277  0.039633   0.76654834 0.3       ], Reward: 2.0\n",
            "Processed Log → State: [0.38848186 0.47131792 0.04714949 0.84281725 0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.40115818 0.47385985 0.05478448 0.9812246  0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.40872958 0.4817646  0.04331    1.         0.39      ], Reward: 7.0\n",
            "Processed Log → State: [0.41962907 0.5031083  0.04192351 1.         0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.41900772 0.53638756 0.04499041 1.         0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.44672233 0.5323926  0.06312626 1.         0.48      ], Reward: 5.0\n",
            "Processed Log → State: [0.45633477 0.52713084 0.0591197  1.         0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.44774225 0.51188475 0.03088714 0.9204183  0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.47571406 0.5191616  0.04813066 0.9411513  0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.504761   0.55920476 0.0505421  0.99777967 0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.49737248 0.57572776 0.04793198 0.9645607  0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.5126081  0.57582545 0.05480404 0.9516799  0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.50529    0.59765756 0.05073575 1.         0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.4946354  0.57798624 0.06473271 1.         0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.5183358  0.59518284 0.05449086 0.9801203  0.75      ], Reward: 0.0\n",
            "Processed Log → State: [0.51881105 0.59497553 0.06383388 0.96945155 0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.5474304  0.5979212  0.0812482  0.99028045 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.5540814  0.58686733 0.11148521 0.98657435 0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.56051016 0.6066914  0.12640722 0.9741441  0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.5695316  0.6307062  0.15083611 1.         0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.14633107 0.3067348  0.10145915 0.68836004 0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.12068816 0.39989352 0.19036338 0.6839345  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.20146915 0.40169775 0.13940424 0.69870436 0.09      ], Reward: 5.0\n",
            "Processed Log → State: [0.23582667 0.44677684 0.1452878  0.729661   0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.22238195 0.4286325  0.1643585  0.6461854  0.15      ], Reward: 7.0\n",
            "Processed Log → State: [0.24169911 0.54251915 0.20019333 0.58509046 0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.21686013 0.62681746 0.21340176 0.5852717  0.21      ], Reward: 7.0\n",
            "Processed Log → State: [0.21691965 0.68058777 0.2517613  0.6340309  0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.29434282 0.6885294  0.22201557 0.53067994 0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.3373372  0.6836141  0.2790359  0.47256112 0.3       ], Reward: 7.0\n",
            "Processed Log → State: [0.44641376 0.77002186 0.25147718 0.54692256 0.33      ], Reward: 2.0\n",
            "Processed Log → State: [0.45307925 0.7922335  0.24135827 0.5718222  0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.41687638 0.840196   0.22017631 0.50791186 0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.45789358 0.832843   0.21538638 0.37528938 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.50459176 0.9054584  0.19206405 0.39472985 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.5004288  0.94730324 0.21044162 0.33296087 0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.5888274  0.9780176  0.25007784 0.307774   0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.65402335 0.9689119  0.21791685 0.28316736 0.54      ], Reward: 0.0\n",
            "Processed Log → State: [0.7732649  0.9434144  0.29255491 0.30164942 0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.671385   0.91410905 0.31477675 0.28624544 0.6       ], Reward: 4.0\n",
            "Processed Log → State: [0.6680641  0.8893399  0.37208334 0.3565791  0.63      ], Reward: 7.0\n",
            "Processed Log → State: [0.7458756 0.8720865 0.3649931 0.3968753 0.66     ], Reward: 5.0\n",
            "Processed Log → State: [0.7505642  0.877763   0.39405328 0.45269042 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.8501799  0.9610801  0.42249042 0.4595491  0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.85877645 1.         0.44185382 0.43459195 0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.87980056 1.         0.39389506 0.37452    0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.9260622  0.99150425 0.40808856 0.31712258 0.81      ], Reward: 2.0\n",
            "Processed Log → State: [1.         1.         0.39731237 0.34725052 0.84      ], Reward: 6.0\n",
            "Processed Log → State: [1.         1.         0.4099554  0.36578792 0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.9481725  1.         0.44485852 0.39530468 0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.2034148  0.34381458 0.06041392 0.66087353 0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.24039136 0.38268772 0.08679059 0.7167222  0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.23958467 0.41617903 0.11002023 0.7121421  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.26292187 0.39050686 0.10660776 0.76861393 0.12      ], Reward: 0.0\n",
            "Processed Log → State: [0.30608496 0.4244606  0.13221276 0.8116205  0.15      ], Reward: -1.0\n",
            "Processed Log → State: [0.30831745 0.45575178 0.1357952  0.8655009  0.18      ], Reward: 7.0\n",
            "Processed Log → State: [0.32400355 0.48670995 0.14274035 0.945171   0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.39168835 0.4925982  0.16688982 0.9479137  0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.37854117 0.49200526 0.15548727 0.9426206  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.39707613 0.52668315 0.15305299 0.9755114  0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.4026044  0.581551   0.16365358 0.9488555  0.33      ], Reward: 2.0\n",
            "Processed Log → State: [0.40211472 0.60775524 0.1712596  0.9735434  0.36      ], Reward: 7.0\n",
            "Processed Log → State: [0.44315732 0.65567446 0.16996863 0.96766746 0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.49302897 0.6547089  0.19272576 0.98327816 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.4750002 0.675614  0.1931681 1.        0.45     ], Reward: 0.0\n",
            "Processed Log → State: [0.46958512 0.69262934 0.18306391 1.         0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.4664936  0.707865   0.18018174 0.97208744 0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.52112395 0.71345234 0.17560366 0.98038805 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.554927   0.7162469  0.17779376 1.         0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.5691266  0.6994013  0.17263198 1.         0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.5749003  0.7236484  0.17314467 1.         0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.5943545 0.7322309 0.1896464 1.        0.66     ], Reward: 8.0\n",
            "Processed Log → State: [0.59039766 0.7302251  0.20209837 0.9604682  0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.5946993  0.6953296  0.21527377 0.9396007  0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.579074   0.7098784  0.22330084 0.9891427  0.75      ], Reward: 0.0\n",
            "Processed Log → State: [0.5791046  0.7162484  0.21650831 1.         0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.58332956 0.7233567  0.23727962 1.         0.81      ], Reward: 6.0\n",
            "Processed Log → State: [0.61105555 0.7249368  0.27013198 0.9811142  0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.6163651 0.7020312 0.2898166 1.        0.87     ], Reward: 8.0\n",
            "Processed Log → State: [0.5905928 0.7035074 0.2967454 1.        0.9      ], Reward: 7.0\n",
            "Processed Log → State: [0.2247474  0.3374436  0.04747153 0.60812426 0.03      ], Reward: 4.0\n",
            "Processed Log → State: [0.247782   0.34602138 0.06795201 0.54227763 0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.26283634 0.43405482 0.11555095 0.5349914  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.244665   0.40066335 0.17195149 0.4120787  0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.31397724 0.43777317 0.2289617  0.39115304 0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.42417488 0.49147508 0.31678107 0.35833478 0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.55873007 0.50899607 0.2962242  0.32964006 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.5382665  0.5833738  0.35952783 0.38814366 0.24      ], Reward: -1.0\n",
            "Processed Log → State: [0.6056851  0.6264309  0.34894165 0.32123044 0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.6467053  0.6352957  0.35519248 0.27369562 0.3       ], Reward: 2.0\n",
            "Processed Log → State: [0.68456453 0.6383183  0.3987657  0.28052104 0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.646363   0.6491894  0.34603426 0.25033703 0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.6663321  0.70638454 0.27291027 0.23400594 0.39      ], Reward: 7.0\n",
            "Processed Log → State: [0.761441   0.70592713 0.3309473  0.2065164  0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.763683   0.7169702  0.47939837 0.2161181  0.45      ], Reward: 0.0\n",
            "Processed Log → State: [0.7512393  0.7749758  0.5148521  0.14949113 0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.8166915  0.8192823  0.5632708  0.19389275 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.8743512  0.83244693 0.5440985  0.28358114 0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.9789886  0.8219113  0.49692374 0.22755578 0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.91044205 0.8135472  0.47498983 0.21158312 0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.97909725 0.7697073  0.5472988  0.21221316 0.63      ], Reward: 6.0\n",
            "Processed Log → State: [0.9859791  0.7978822  0.63507813 0.16335295 0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.96018976 0.7995422  0.61303675 0.1330381  0.69      ], Reward: 2.0\n",
            "Processed Log → State: [1.         0.7994106  0.6613506  0.13307503 0.72      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.8154778  0.5782958  0.15431874 0.75      ], Reward: 7.0\n",
            "Processed Log → State: [1.         0.83188885 0.56263644 0.15067519 0.78      ], Reward: -1.0\n",
            "Processed Log → State: [1.         0.8266199  0.6203186  0.18482667 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.98981565 0.83519423 0.6586242  0.2781716  0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.97877246 0.85286754 0.7410784  0.3077532  0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.9115516  0.82736015 0.6628715  0.32766432 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.1903409  0.34373426 0.04412227 0.6433725  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.15316375 0.3310541  0.04242508 0.6140361  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.17350304 0.30496106 0.04107655 0.5896442  0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.1752023  0.30480012 0.03480608 0.5739433  0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.18301569 0.32032317 0.0337287  0.5107301  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.20683882 0.35746318 0.04027731 0.5268033  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.2233658  0.37888122 0.05359481 0.5619308  0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.24791576 0.38419464 0.05367314 0.62058187 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.24044809 0.40010515 0.0483316  0.65739876 0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.26367    0.4194409  0.05444555 0.77385646 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.25068325 0.41116658 0.05857147 0.7716242  0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.28959826 0.4351068  0.05320146 0.8505512  0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.34307283 0.44260395 0.04130115 0.9230724  0.39      ], Reward: -1.0\n",
            "Processed Log → State: [0.32780457 0.45906138 0.06302796 0.9457329  0.42      ], Reward: 0.0\n",
            "Processed Log → State: [0.35345316 0.49375278 0.07243501 0.97212857 0.45      ], Reward: 0.0\n",
            "Processed Log → State: [0.3977452  0.4907718  0.07368048 0.9906357  0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.42121756 0.5283769  0.07715426 0.87345034 0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.43212345 0.58900493 0.08665183 0.91474557 0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.4267844 0.6452536 0.0899509 0.8851363 0.57     ], Reward: 7.0\n",
            "Processed Log → State: [0.45216778 0.63756657 0.08451223 0.8443977  0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.44600648 0.64984566 0.08945046 0.932983   0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.4525045  0.6692446  0.09858053 0.92996734 0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.48480636 0.67970496 0.10372256 0.94646204 0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.45474714 0.6901749  0.11256058 0.9050162  0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.44061807 0.72606814 0.10690543 0.8563459  0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.4554988  0.7244332  0.09920424 0.8475179  0.78      ], Reward: 6.0\n",
            "Processed Log → State: [0.45276368 0.7108025  0.11297494 0.79455686 0.81      ], Reward: 0.0\n",
            "Processed Log → State: [0.46427915 0.70143926 0.14187221 0.78644764 0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.45988217 0.6930526  0.15586427 0.8349863  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.46432817 0.6650711  0.1472431  0.9498741  0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.18973567 0.33873734 0.07073212 0.6634875  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.24643299 0.36497614 0.07910423 0.65899915 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.22017257 0.39630038 0.09523094 0.69816345 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.23916407 0.3963109  0.11156736 0.762991   0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.25140655 0.4105036  0.11000326 0.7490778  0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.27667806 0.41314176 0.10893921 0.7771296  0.18      ], Reward: 5.0\n",
            "Processed Log → State: [0.27728537 0.45896932 0.12305502 0.75997794 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.2746451  0.4698198  0.13746907 0.80169815 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.28957772 0.4751638  0.15310794 0.85596555 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.34730825 0.50496083 0.16156352 0.86790246 0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.36667967 0.53888774 0.16041672 0.9541934  0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.38497752 0.532611   0.1591633  0.967942   0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.40178677 0.5556867  0.1769062  0.98599106 0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.40361366 0.5571026  0.18442361 1.         0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.43355635 0.59179807 0.20911656 0.9141894  0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.44744644 0.5887686  0.21825613 0.95333004 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.4631377 0.5942472 0.2326736 0.9544739 0.51     ], Reward: 8.0\n",
            "Processed Log → State: [0.46677616 0.62965703 0.24652751 0.93603355 0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.4927006 0.64882   0.239337  0.8962555 0.57     ], Reward: 0.0\n",
            "Processed Log → State: [0.49877754 0.6641592  0.23851874 0.9153417  0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.51799315 0.6539395  0.23512687 0.9739718  0.63      ], Reward: 5.0\n",
            "Processed Log → State: [0.52411324 0.682896   0.2527581  1.         0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.54306287 0.6652737  0.24891311 0.9718513  0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.56296456 0.644116   0.25492924 0.979178   0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.5560885  0.66188455 0.25626194 1.         0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.5466879  0.66988635 0.24787338 0.98585975 0.78      ], Reward: 5.0\n",
            "Processed Log → State: [0.54444075 0.69781494 0.2886749  0.9082647  0.81      ], Reward: 7.0\n",
            "Processed Log → State: [0.5554638  0.70290947 0.301393   0.9194058  0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.57324076 0.71127766 0.31370398 0.94885653 0.87      ], Reward: 0.0\n",
            "Processed Log → State: [0.57159907 0.74790317 0.31826538 0.93489975 0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.20863515 0.3186656  0.06233059 0.6436289  0.03      ], Reward: 0.0\n",
            "Processed Log → State: [0.131361   0.35353768 0.11596436 0.69237    0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.1754441  0.37474373 0.19271462 0.77931863 0.09      ], Reward: 0.0\n",
            "Processed Log → State: [0.14153188 0.4336964  0.17403767 0.6998221  0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.17967336 0.4495317  0.2523532  0.6308065  0.15      ], Reward: 5.0\n",
            "Processed Log → State: [0.21057928 0.52349186 0.4017454  0.6017686  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.29819244 0.61012006 0.45011547 0.5911408  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.3681191  0.6463239  0.43107322 0.65391135 0.24      ], Reward: 7.0\n",
            "Processed Log → State: [0.4555135  0.66650677 0.41467574 0.6775972  0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.5623069  0.75121266 0.37103853 0.6885751  0.3       ], Reward: 7.0\n",
            "Processed Log → State: [0.5504685  0.8395217  0.36635694 0.6234255  0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.58659196 0.84253603 0.40461248 0.57928336 0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.6290681  0.8620092  0.43388745 0.5601723  0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.7087656  0.8839479  0.4589454  0.54795754 0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.75363207 0.9057172  0.54962915 0.56986916 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.76233673 0.85089475 0.5837805  0.51393336 0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.8922317  0.8121678  0.5698667  0.50890523 0.51      ], Reward: 7.0\n",
            "Processed Log → State: [0.92812485 0.85618526 0.6704617  0.58188426 0.54      ], Reward: 0.0\n",
            "Processed Log → State: [0.8931713  0.86117804 0.69287515 0.58399975 0.57      ], Reward: 2.0\n",
            "Processed Log → State: [0.93759996 0.84448195 0.671864   0.6438409  0.6       ], Reward: 5.0\n",
            "Processed Log → State: [0.9960015  0.85565996 0.71774197 0.6030697  0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.9773848 0.7967409 0.6855293 0.5356441 0.66     ], Reward: 6.0\n",
            "Processed Log → State: [0.975164   0.74464434 0.67519724 0.5557671  0.69      ], Reward: 8.0\n",
            "Processed Log → State: [0.922526   0.7272839  0.70555556 0.4763539  0.72      ], Reward: 5.0\n",
            "Processed Log → State: [0.97501296 0.7024958  0.7215094  0.4992217  0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.953517   0.6885577  0.70932716 0.4493074  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.6500942  0.73440033 0.42464763 0.81      ], Reward: 5.0\n",
            "Processed Log → State: [1.         0.6451195  0.7387684  0.38696697 0.84      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.6317728  0.77941316 0.42987955 0.87      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.62671685 0.8333339  0.46455568 0.9       ], Reward: 6.0\n",
            "Processed Log → State: [0.24142098 0.37394357 0.07261736 0.61580604 0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.27247807 0.43500134 0.02729324 0.5789412  0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.4272501  0.50964415 0.03529697 0.63674885 0.09      ], Reward: 2.0\n",
            "Processed Log → State: [0.379191   0.56296486 0.0786854  0.6755856  0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.36209825 0.56855166 0.09479419 0.614037   0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.3478325  0.67993647 0.10151483 0.5599217  0.18      ], Reward: 4.0\n",
            "Processed Log → State: [0.36382815 0.6496278  0.11569758 0.5199246  0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.40704033 0.6852712  0.11480696 0.47072095 0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.4871455  0.6444958  0.15869388 0.4882623  0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.47462103 0.6498842  0.16571103 0.58352375 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.5396137  0.7432005  0.17463471 0.5242886  0.33      ], Reward: 7.0\n",
            "Processed Log → State: [0.53439975 0.7340713  0.17987648 0.5599603  0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.58941126 0.82085073 0.18210591 0.58465797 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.72075677 0.8352551  0.20570953 0.6664056  0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.77693033 0.9442949  0.1661519  0.6697079  0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.7476162  0.9507402  0.18862344 0.6030875  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.7232819  0.97786695 0.1552129  0.5532832  0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.74101907 0.96948963 0.17520365 0.54925483 0.54      ], Reward: 5.0\n",
            "Processed Log → State: [0.7494965  0.91350883 0.19949095 0.54045117 0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.8154537  0.8212619  0.22502887 0.43125445 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.91436857 0.8652432  0.2515388  0.46942747 0.63      ], Reward: 6.0\n",
            "Processed Log → State: [0.9187629  0.9054782  0.27750927 0.48748955 0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.88648564 0.9545651  0.30837104 0.42063344 0.69      ], Reward: 0.0\n",
            "Processed Log → State: [0.910258   0.9747012  0.31181207 0.4041339  0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.8642698 1.        0.3185396 0.4412213 0.75     ], Reward: 0.0\n",
            "Processed Log → State: [0.83869886 1.         0.30790058 0.4342772  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.8556443  0.9590522  0.35295853 0.43906906 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.8162469  0.9294515  0.33972293 0.4861504  0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.8739959  0.90693456 0.3362464  0.4999816  0.87      ], Reward: 5.0\n",
            "Processed Log → State: [0.9076866  0.8570333  0.37994576 0.55517924 0.9       ], Reward: 4.0\n",
            "Processed Log → State: [0.1954965  0.2975998  0.01407862 0.58360064 0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.22420044 0.2707384  0.09493122 0.54414284 0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.29728964 0.33146286 0.12303453 0.5556401  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.27319112 0.322891   0.08230978 0.45185417 0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.29774475 0.3518092  0.126144   0.4540574  0.15      ], Reward: -1.0\n",
            "Processed Log → State: [0.3499852  0.40605298 0.1325426  0.4326303  0.18      ], Reward: 0.0\n",
            "Processed Log → State: [0.45895854 0.38495734 0.12203845 0.46591654 0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.47061676 0.439677   0.1106782  0.44614902 0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.55068487 0.54245746 0.09598157 0.45228854 0.27      ], Reward: 4.0\n",
            "Processed Log → State: [0.5194655  0.5263707  0.13757013 0.49551576 0.3       ], Reward: 2.0\n",
            "Processed Log → State: [0.5043048  0.66689664 0.1324704  0.56785357 0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.5302335  0.80379736 0.1474759  0.5565381  0.36      ], Reward: 5.0\n",
            "Processed Log → State: [0.53118485 0.7612172  0.22876483 0.56920236 0.39      ], Reward: -1.0\n",
            "Processed Log → State: [0.59579533 0.8271326  0.2477125  0.49314022 0.42      ], Reward: 0.0\n",
            "Processed Log → State: [0.67554754 0.8024475  0.2530822  0.50343674 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.6817296  0.82442254 0.27573448 0.47640786 0.48      ], Reward: 6.0\n",
            "Processed Log → State: [0.63169414 0.87419623 0.33817238 0.5386143  0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.61974734 0.9010065  0.31842336 0.5447425  0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.698103   0.96604043 0.31121463 0.60693544 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.7562187  0.93111455 0.31592312 0.5839857  0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.7376     0.9025542  0.32924786 0.5616894  0.63      ], Reward: 6.0\n",
            "Processed Log → State: [0.6641504  0.96522564 0.34829324 0.53956413 0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.67598706 1.         0.3768137  0.5652158  0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.69272006 1.         0.34049615 0.5693938  0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.7380869  1.         0.36692274 0.5923704  0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.82296    0.95526373 0.39322925 0.6117829  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.8627376  0.94947886 0.432915   0.66596    0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.9188116  0.9143791  0.46800303 0.63317853 0.84      ], Reward: 5.0\n",
            "Processed Log → State: [0.8367448  0.8117435  0.43515545 0.68215734 0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.84982723 0.7606146  0.4362513  0.68760747 0.9       ], Reward: 7.0\n",
            "Processed Log → State: [0.17770992 0.37127113 0.17930852 0.6310284  0.03      ], Reward: 0.0\n",
            "Processed Log → State: [0.22068694 0.37457997 0.24794981 0.65304816 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.39020294 0.39748454 0.28935817 0.6262413  0.09      ], Reward: 5.0\n",
            "Processed Log → State: [0.4039214  0.33486915 0.2929228  0.6563092  0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.4956037  0.34265164 0.24843158 0.61464256 0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.52142876 0.35148433 0.3457776  0.6501911  0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.6203812 0.4134526 0.4392014 0.6188358 0.21     ], Reward: 3.0\n",
            "Processed Log → State: [0.6297106  0.47951254 0.5637025  0.5979694  0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.65894246 0.46525803 0.64389884 0.5962559  0.27      ], Reward: -1.0\n",
            "Processed Log → State: [0.7028362  0.52065796 0.6867026  0.6276432  0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.69650626 0.5669581  0.7871916  0.57149065 0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.6763966  0.6243657  0.8630795  0.55877316 0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.76815754 0.6424638  0.869219   0.52173746 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.79571617 0.6647671  0.9589747  0.46159133 0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.8536197  0.6696672  0.9801877  0.46277788 0.45      ], Reward: 6.0\n",
            "Processed Log → State: [0.8570976  0.75085676 1.         0.493519   0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.86616915 0.8015626  1.         0.47405034 0.51      ], Reward: 2.0\n",
            "Processed Log → State: [0.78781897 0.82782924 1.         0.3804853  0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.8728485  0.83594733 0.9991713  0.40397924 0.57      ], Reward: 2.0\n",
            "Processed Log → State: [0.96264684 0.8605265  0.97917145 0.40071768 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.9917245  0.8690652  0.98662066 0.48583332 0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.9352556  0.9213978  1.         0.50711936 0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.96276695 0.95516235 0.91952014 0.44699985 0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.95745224 0.9807112  0.8173413  0.4399301  0.72      ], Reward: 7.0\n",
            "Processed Log → State: [0.9398002  0.94550246 0.80456465 0.40304652 0.75      ], Reward: 7.0\n",
            "Processed Log → State: [0.9697     0.8809521  0.7972393  0.43837768 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.9295738  0.85122347 0.81377655 0.3931273  0.81      ], Reward: 0.0\n",
            "Processed Log → State: [0.8539288 0.8052216 0.8017831 0.4585455 0.84     ], Reward: 3.0\n",
            "Processed Log → State: [0.85473394 0.8202113  0.7984069  0.5219702  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.82539004 0.82200617 0.8415793  0.5859302  0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.17452365 0.2786244  0.04624536 0.62704366 0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.19650218 0.35204393 0.01477171 0.70780855 0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.13279949 0.39485228 0.06800196 0.7348742  0.09      ], Reward: 5.0\n",
            "Processed Log → State: [0.11568382 0.41663527 0.10569898 0.75755876 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.15542246 0.56688535 0.13140886 0.7276624  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.12054753 0.6335039  0.10292654 0.68502927 0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.11765612 0.6348094  0.17119902 0.66265297 0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.06539906 0.6391881  0.21791655 0.60352695 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.0313474  0.6608248  0.2840183  0.55718935 0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.21760927 0.63501894 0.2502166  0.5127847  0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.3399654 0.6106557 0.2576031 0.5202012 0.33     ], Reward: 1.0\n",
            "Processed Log → State: [0.342013   0.76075155 0.2795904  0.52612615 0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.3758964  0.77432406 0.3234178  0.47282082 0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.33038333 0.8029731  0.29762813 0.48181406 0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.336149   0.71810555 0.32103172 0.48894882 0.45      ], Reward: 6.0\n",
            "Processed Log → State: [0.33783838 0.73173326 0.32994348 0.50498146 0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.3496842  0.80132854 0.33419147 0.5081268  0.51      ], Reward: 2.0\n",
            "Processed Log → State: [0.3787552  0.7690583  0.33963707 0.5177197  0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.43372637 0.7669013  0.41345918 0.50776225 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.44610474 0.79032207 0.44285953 0.4692692  0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.50827    0.82895005 0.40074465 0.5002504  0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.5399666 0.8832499 0.3485595 0.5002432 0.66     ], Reward: 5.0\n",
            "Processed Log → State: [0.4404282  0.89142096 0.3645232  0.56112736 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.39069661 0.8779241  0.322935   0.5699381  0.72      ], Reward: 7.0\n",
            "Processed Log → State: [0.36605972 0.9287207  0.32192513 0.57405925 0.75      ], Reward: 7.0\n",
            "Processed Log → State: [0.36950776 0.889578   0.29214516 0.61225545 0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.4413234 0.9095324 0.2725868 0.6217573 0.81     ], Reward: 8.0\n",
            "Processed Log → State: [0.50425875 0.9053256  0.2893855  0.546995   0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.4851729  0.914226   0.34532583 0.5539107  0.87      ], Reward: 2.0\n",
            "Processed Log → State: [0.48755217 0.9575777  0.36046976 0.50071645 0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.19630042 0.32658264 0.12093713 0.56970793 0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.28081104 0.34613734 0.16657145 0.61410093 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.29866832 0.34856415 0.20685734 0.50566435 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.37956157 0.365645   0.26444468 0.5495196  0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.39690742 0.42621186 0.37375543 0.602067   0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.29103208 0.48076317 0.5003987  0.6197527  0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.26481214 0.5462275  0.54082924 0.6003774  0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.35987043 0.6008703  0.48583192 0.6025078  0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.44201478 0.65003866 0.57241464 0.6126354  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.5624744 0.6873961 0.5399633 0.5372087 0.3      ], Reward: 3.0\n",
            "Processed Log → State: [0.5101464  0.6687137  0.5495947  0.50068307 0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.56366855 0.70619345 0.59781355 0.54894763 0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.6314258  0.73365074 0.64711064 0.51839334 0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.7221035  0.75886697 0.6940976  0.4306313  0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.76315916 0.799409   0.7570702  0.44295633 0.45      ], Reward: -1.0\n",
            "Processed Log → State: [0.7992388  0.8288507  0.68352044 0.38717726 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.817638   0.76008326 0.6772825  0.31811556 0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.79556936 0.7748604  0.61684823 0.29582462 0.54      ], Reward: 0.0\n",
            "Processed Log → State: [0.83663267 0.8378812  0.56603557 0.3885842  0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.8204814  0.82072496 0.6375953  0.30490342 0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.8649893  0.8651231  0.60899895 0.33297378 0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.9080732  0.87766784 0.5648894  0.34987932 0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.9483648  0.89492095 0.54387426 0.3201175  0.69      ], Reward: 5.0\n",
            "Processed Log → State: [0.9721636 0.8891143 0.5820639 0.3915986 0.72     ], Reward: 8.0\n",
            "Processed Log → State: [0.9582965  0.90743864 0.61360466 0.37460026 0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.94121534 0.90171915 0.62123924 0.33222932 0.78      ], Reward: -1.0\n",
            "Processed Log → State: [0.95198643 0.9053777  0.61185133 0.28311747 0.81      ], Reward: 6.0\n",
            "Processed Log → State: [0.94299763 0.94453984 0.5940249  0.23428097 0.84      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.91068774 0.63403654 0.19901969 0.87      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.90042466 0.7466672  0.2307013  0.9       ], Reward: 7.0\n",
            "Processed Log → State: [0.24311393 0.34220573 0.04424525 0.6772266  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.2745369  0.34346452 0.05895054 0.7046241  0.06      ], Reward: 5.0\n",
            "Processed Log → State: [0.29239783 0.41167116 0.07591335 0.7647502  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.3130874  0.4524028  0.08242425 0.88064504 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.33684042 0.4648839  0.09311154 0.9204926  0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.34913772 0.47286782 0.10682085 1.         0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.37133923 0.48858395 0.12068573 1.         0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.41244864 0.5151961  0.1117143  1.         0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.446676   0.54671776 0.12609653 1.         0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.46550083 0.5860502  0.14002223 0.9921301  0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.4883035  0.6095933  0.14153998 1.         0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.49687913 0.62557036 0.15424657 1.         0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.47573572 0.6254279  0.15976822 1.         0.39      ], Reward: 6.0\n",
            "Processed Log → State: [0.5069638  0.6191795  0.16316608 1.         0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.5417878  0.6151011  0.17362252 0.97982436 0.45      ], Reward: 6.0\n",
            "Processed Log → State: [0.5889169  0.6456674  0.16571195 1.         0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.59965837 0.6506136  0.17501207 1.         0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.58237827 0.6725028  0.1802461  1.         0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.60959095 0.6849429  0.20891775 0.99411917 0.57      ], Reward: 2.0\n",
            "Processed Log → State: [0.64937043 0.6930016  0.1937461  0.965094   0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.6286878  0.7090176  0.18846793 1.         0.63      ], Reward: 6.0\n",
            "Processed Log → State: [0.61134917 0.7196136  0.18567301 1.         0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.6290398  0.74944913 0.18331784 0.97916424 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.6394928  0.7772798  0.19179465 0.9836636  0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.67132336 0.7718509  0.1918806  0.9994555  0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.64876837 0.767546   0.19960515 1.         0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.66975194 0.741178   0.22189422 0.9860003  0.81      ], Reward: -1.0\n",
            "Processed Log → State: [0.66693586 0.75679934 0.25784388 0.9713739  0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.6481202  0.75577515 0.28406063 0.9541476  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.632569   0.7437019  0.28912693 0.9644144  0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.24098507 0.39541808 0.06173343 0.47061422 0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.32154685 0.41166794 0.08316301 0.47965738 0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.38205925 0.49028823 0.10674065 0.4686104  0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.40108156 0.5398601  0.12039565 0.5248068  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.38873747 0.65044683 0.17647175 0.5118933  0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.41070125 0.69291186 0.17059223 0.44165668 0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.5050199  0.72253007 0.19675721 0.40193167 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.50350577 0.81228983 0.19953582 0.33205086 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.5881594  0.825192   0.14460109 0.33674505 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.6215989  0.83558494 0.15221363 0.33079502 0.3       ], Reward: 4.0\n",
            "Processed Log → State: [0.5983372  0.8467423  0.14418642 0.3539651  0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.6286268  0.773778   0.17718834 0.3200535  0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.68181914 0.86912185 0.24474046 0.32151613 0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.67670625 0.81469756 0.19358414 0.33426693 0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.6488579  0.7949962  0.20313749 0.2679551  0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.6903124  0.91838926 0.23431633 0.3219305  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.722941   0.9224804  0.17389609 0.324375   0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.7605266  0.93980545 0.16866064 0.34602368 0.54      ], Reward: 7.0\n",
            "Processed Log → State: [0.8496012  1.         0.18157643 0.38483718 0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.8659466  0.9736336  0.22988582 0.3775002  0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.8837769  1.         0.20284563 0.42080778 0.63      ], Reward: -1.0\n",
            "Processed Log → State: [0.83923084 1.         0.21866514 0.43321705 0.66      ], Reward: 5.0\n",
            "Processed Log → State: [0.79767543 1.         0.25180835 0.38962796 0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.7724133  1.         0.25211093 0.4104387  0.72      ], Reward: 7.0\n",
            "Processed Log → State: [0.7843775  0.9796927  0.28391925 0.42414716 0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.8136359  0.9608226  0.27504656 0.37072742 0.78      ], Reward: 0.0\n",
            "Processed Log → State: [0.82257086 0.9832595  0.3117819  0.31567112 0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.8389888  1.         0.33273402 0.3132589  0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.79801536 1.         0.3936889  0.35625604 0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.77388275 1.         0.42858857 0.39222097 0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.23857804 0.38590774 0.03725    0.5476934  0.03      ], Reward: 5.0\n",
            "Processed Log → State: [0.34633294 0.43487686 0.05610705 0.5196332  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.31601554 0.536665   0.04412385 0.59133387 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.37000936 0.58056486 0.02274364 0.52525055 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.34689113 0.6072221  0.07977958 0.555301   0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.36076045 0.66252923 0.1315703  0.5127427  0.18      ], Reward: -1.0\n",
            "Processed Log → State: [0.24445315 0.7082901  0.16972488 0.5462999  0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.26138934 0.7090024  0.18036485 0.57858694 0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.31823418 0.7500069  0.19349952 0.5271564  0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.38419893 0.74083865 0.19049382 0.57054615 0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.38663936 0.8118055  0.22668357 0.56304085 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.43880773 0.824495   0.24181804 0.63401383 0.36      ], Reward: 7.0\n",
            "Processed Log → State: [0.42149264 0.84761035 0.25996614 0.60264516 0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.37415445 0.8852013  0.21936038 0.4647365  0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.3318163  0.91908836 0.21667755 0.46520683 0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.31700432 0.96482414 0.23575047 0.45322102 0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.3236353  0.9501712  0.2190473  0.46963298 0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.4055129  1.         0.23755926 0.47924128 0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.4232076  0.9657562  0.21848777 0.43572053 0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.41222867 0.9837043  0.25997373 0.45588514 0.6       ], Reward: 7.0\n",
            "Processed Log → State: [0.40916649 0.99386007 0.27386642 0.49191982 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.42456475 1.         0.24107327 0.57304716 0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.45461044 1.         0.25112814 0.5132306  0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.52964985 1.         0.23218118 0.494777   0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.5663104  1.         0.22043179 0.58688605 0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.6293627  1.         0.23238249 0.58803916 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.6658531  0.99130434 0.25245062 0.5415885  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.65310955 0.97227323 0.31551573 0.5921704  0.84      ], Reward: 0.0\n",
            "Processed Log → State: [0.60348356 0.9553683  0.30954924 0.540423   0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.6521299  0.92294663 0.31812546 0.5978824  0.9       ], Reward: 6.0\n",
            "Processed Log → State: [0.18475784 0.2779689  0.09387034 0.5536743  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.2593796  0.36225608 0.10104094 0.51787347 0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.3476117  0.3963158  0.19316533 0.44651788 0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.451143   0.45942146 0.20017809 0.37394956 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.53425056 0.4870688  0.3206196  0.3092645  0.15      ], Reward: 7.0\n",
            "Processed Log → State: [0.55656457 0.52080137 0.32713938 0.21698172 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.63621795 0.53167516 0.4046298  0.25095132 0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.5696014  0.56590044 0.50143164 0.1681813  0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.5690891  0.61402285 0.52298695 0.18479916 0.27      ], Reward: 7.0\n",
            "Processed Log → State: [0.546104   0.62865454 0.487826   0.13814352 0.3       ], Reward: 0.0\n",
            "Processed Log → State: [0.5696039  0.7135688  0.55333394 0.18265754 0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.63787156 0.722213   0.58404464 0.17508842 0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.6706245  0.7514015  0.5793655  0.19278972 0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.6870808  0.79886454 0.6056251  0.15537496 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.6595514  0.8227867  0.6555594  0.16850536 0.45      ], Reward: 7.0\n",
            "Processed Log → State: [0.6678329  0.8437016  0.67696947 0.16508959 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.76511633 0.8579925  0.73836577 0.12454168 0.51      ], Reward: 5.0\n",
            "Processed Log → State: [0.7589102  0.85856545 0.69488186 0.15654413 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.8357713  0.8488049  0.6624506  0.20279224 0.57      ], Reward: 2.0\n",
            "Processed Log → State: [0.8209983  0.868423   0.64286715 0.19998416 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.7681718  0.84717697 0.6645079  0.30327448 0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.79043823 0.8584498  0.6635867  0.30888328 0.66      ], Reward: 4.0\n",
            "Processed Log → State: [0.80262    0.88006836 0.65914327 0.3163398  0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.83212584 0.86951625 0.6318146  0.32307222 0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.773363  0.8431623 0.6478256 0.348002  0.75     ], Reward: 6.0\n",
            "Processed Log → State: [0.74792403 0.821927   0.6378474  0.2994955  0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.81837904 0.826428   0.6775232  0.34083638 0.81      ], Reward: -1.0\n",
            "Processed Log → State: [0.8406988  0.82147807 0.66385865 0.29904667 0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.9546034  0.85954773 0.6917698  0.2814416  0.87      ], Reward: 2.0\n",
            "Processed Log → State: [0.833762   0.75974673 0.67730695 0.2772638  0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.19510597 0.32282233 0.05366028 0.7083631  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.1838052  0.35477126 0.04819276 0.69824475 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.20192136 0.3815322  0.06000115 0.72928166 0.09      ], Reward: 7.0\n",
            "Processed Log → State: [0.2473662  0.3967749  0.05448698 0.80154103 0.12      ], Reward: 0.0\n",
            "Processed Log → State: [0.26371503 0.3949468  0.06539433 0.73770267 0.15      ], Reward: 0.0\n",
            "Processed Log → State: [0.26194105 0.41065902 0.0526284  0.7598856  0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.24466835 0.41958678 0.05242364 0.78712237 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.26178065 0.42928222 0.0572248  0.811087   0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.24746965 0.44040596 0.05923788 0.76876134 0.27      ], Reward: 0.0\n",
            "Processed Log → State: [0.28456736 0.4600674  0.06106651 0.7262979  0.3       ], Reward: -1.0\n",
            "Processed Log → State: [0.27643007 0.4834921  0.0598541  0.7309584  0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.30031294 0.4719557  0.05240308 0.7906634  0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.31425858 0.48943794 0.05252787 0.86099786 0.39      ], Reward: -1.0\n",
            "Processed Log → State: [0.31947044 0.49984062 0.05249737 0.7798699  0.42      ], Reward: -1.0\n",
            "Processed Log → State: [0.3152445  0.5288002  0.05835446 0.8363368  0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.3277955  0.54888624 0.06031907 0.8445443  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.31642917 0.55724376 0.05608813 0.84021723 0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.32951856 0.547281   0.06354167 0.8197462  0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.36720535 0.53834796 0.06575675 0.9307782  0.57      ], Reward: 2.0\n",
            "Processed Log → State: [0.3821292  0.5191575  0.06758864 0.95008224 0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.36382556 0.5227741  0.077073   1.         0.63      ], Reward: 6.0\n",
            "Processed Log → State: [0.3693488  0.5219375  0.0789902  0.99887097 0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.36509073 0.52103645 0.08973772 0.97776735 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.37659314 0.51687074 0.09325103 1.         0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.41066584 0.5259242  0.09521587 1.         0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.41750556 0.552579   0.11461127 0.9533395  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.43611118 0.5208554  0.12815733 0.9346235  0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.4091552  0.49859846 0.14142795 0.9413277  0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.41429833 0.53781193 0.15885277 0.94958574 0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.40290612 0.5420955  0.18190041 1.         0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.19996561 0.3038959  0.06085386 0.6336112  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.20706648 0.3138004  0.05898481 0.6391694  0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.18253094 0.3208315  0.07268924 0.72010225 0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.16383839 0.34264243 0.07142764 0.7272027  0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.1789926  0.37763226 0.05996687 0.772335   0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.20078442 0.39782417 0.08373564 0.83305424 0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.27355722 0.37633932 0.07786717 0.9387089  0.21      ], Reward: 0.0\n",
            "Processed Log → State: [0.28689438 0.390688   0.08230059 0.9016513  0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.31122074 0.4022446  0.09463437 0.924502   0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.3163546  0.42511594 0.10763381 0.93966144 0.3       ], Reward: 2.0\n",
            "Processed Log → State: [0.33021903 0.44499406 0.12457436 1.         0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.3276757  0.46474233 0.12626764 1.         0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.3432472  0.47919175 0.12426136 1.         0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.3283354  0.4831888  0.11292133 1.         0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.3077498  0.49654883 0.11381716 1.         0.45      ], Reward: 7.0\n",
            "Processed Log → State: [0.3096197  0.5395261  0.10055654 0.9350097  0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.32581407 0.5494295  0.09951915 0.8825393  0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.3602176  0.57446486 0.09225334 0.92744404 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.34607095 0.5557828  0.09106082 0.95462835 0.57      ], Reward: -1.0\n",
            "Processed Log → State: [0.37306577 0.5780588  0.0687189  0.98030025 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.41030636 0.5673156  0.0897221  0.9952395  0.63      ], Reward: 2.0\n",
            "Processed Log → State: [0.42804724 0.57069844 0.08156541 1.         0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.44446605 0.5802387  0.09629622 1.         0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.46284157 0.5738344  0.08300646 0.95711863 0.72      ], Reward: -1.0\n",
            "Processed Log → State: [0.45414045 0.6100411  0.08785399 0.8981473  0.75      ], Reward: 7.0\n",
            "Processed Log → State: [0.43559912 0.6428509  0.08707507 0.8291647  0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.45595232 0.6373248  0.10036478 0.7620111  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.4846815  0.6034818  0.11958515 0.72497356 0.84      ], Reward: 5.0\n",
            "Processed Log → State: [0.46775252 0.60245216 0.14136417 0.7087385  0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.47494438 0.5884705  0.16489856 0.7184447  0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.21066454 0.31045195 0.08487573 0.6152729  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.21691665 0.32242864 0.08010093 0.54928726 0.06      ], Reward: 7.0\n",
            "Processed Log → State: [0.21978232 0.33222756 0.08699631 0.5861278  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.24097313 0.29854402 0.08394028 0.6084112  0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.251098   0.31529555 0.10020098 0.6062953  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.29127854 0.324191   0.11677629 0.62566066 0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.2887807  0.35537362 0.12185339 0.64857024 0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.337633   0.3604168  0.13152821 0.6070207  0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.37591335 0.41555727 0.16038439 0.6990173  0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.38721347 0.4489203  0.14673597 0.8514421  0.3       ], Reward: 2.0\n",
            "Processed Log → State: [0.43272293 0.44435108 0.16706616 0.95867294 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.46777692 0.47112232 0.16687112 0.94473946 0.36      ], Reward: -1.0\n",
            "Processed Log → State: [0.4969165  0.45313042 0.17071559 0.9276719  0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.5353869  0.47070283 0.17879312 0.9565947  0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.53440225 0.49490818 0.18966208 0.9362077  0.45      ], Reward: 7.0\n",
            "Processed Log → State: [0.5584878  0.5215762  0.18227369 0.98890233 0.48      ], Reward: 7.0\n",
            "Processed Log → State: [0.5770993  0.57407576 0.18406476 1.         0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.6082958  0.59721214 0.18891074 0.96753633 0.54      ], Reward: 0.0\n",
            "Processed Log → State: [0.60345477 0.6040836  0.19518974 0.9947423  0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.61927265 0.6388126  0.18251188 1.         0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.62081134 0.6656729  0.17739384 1.         0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.62376624 0.6277649  0.18734749 1.         0.66      ], Reward: 0.0\n",
            "Processed Log → State: [0.613456   0.622431   0.17521176 0.98119617 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.6455121  0.64139944 0.17202501 1.         0.72      ], Reward: 5.0\n",
            "Processed Log → State: [0.6504796  0.63301075 0.17062883 1.         0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.67343    0.6207135  0.16367246 0.9693013  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.6294434  0.634371   0.19970769 0.9866679  0.81      ], Reward: 2.0\n",
            "Processed Log → State: [0.61908704 0.6362018  0.21235242 0.9792454  0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.6292424  0.6346424  0.22885859 0.99617475 0.87      ], Reward: 0.0\n",
            "Processed Log → State: [0.6114263  0.62049973 0.25160787 1.         0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.2948586  0.3792881  0.05929071 0.6264776  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.249094   0.5022402  0.05416583 0.6436163  0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.32488552 0.5484568  0.05428743 0.55922747 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.36703518 0.52382576 0.06363606 0.52294284 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.43769687 0.64271355 0.0503681  0.46340027 0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.41966164 0.67569375 0.1014734  0.43719205 0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.4433933  0.6570903  0.16664974 0.28318736 0.21      ], Reward: 0.0\n",
            "Processed Log → State: [0.34197512 0.74531245 0.1370019  0.2525331  0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.35769424 0.83045626 0.15895352 0.20300448 0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.4139814  0.81813776 0.19858502 0.08356196 0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.42376393 0.82812744 0.1961147  0.00826972 0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.4215526  0.81735295 0.22302833 0.         0.36      ], Reward: -1.0\n",
            "Processed Log → State: [0.3726251  0.8468082  0.20510387 0.05751568 0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.3851434  0.8517551  0.1966629  0.07298234 0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.4007426  0.8350463  0.19295374 0.02774922 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.47407812 0.74602807 0.19397394 0.01651482 0.48      ], Reward: 6.0\n",
            "Processed Log → State: [0.52647406 0.76890457 0.21127608 0.04649307 0.51      ], Reward: 2.0\n",
            "Processed Log → State: [0.5376756  0.806241   0.25071105 0.08990429 0.54      ], Reward: 0.0\n",
            "Processed Log → State: [0.49441466 0.8745268  0.2012965  0.12414616 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.51166785 0.9228442  0.24037936 0.2107126  0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.49000737 0.95662576 0.19873363 0.18947004 0.63      ], Reward: 5.0\n",
            "Processed Log → State: [0.47009504 1.         0.1923014  0.15603219 0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.40586004 0.9955553  0.22473738 0.17574602 0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.44186026 1.         0.20455647 0.20671143 0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.46865454 0.9130803  0.23701221 0.22857848 0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.47218817 0.9993537  0.23035286 0.30997768 0.78      ], Reward: 0.0\n",
            "Processed Log → State: [0.49751157 0.9304886  0.22658361 0.33424944 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.5109126  0.9151889  0.29571205 0.39134148 0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.60804087 0.96609277 0.3150984  0.44458097 0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.7125628  0.97342765 0.34241733 0.50052446 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.23035333 0.31459567 0.04918787 0.58242923 0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.31187442 0.30126482 0.13580911 0.6434163  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.4180309  0.39165172 0.20664003 0.632994   0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.501547   0.44637775 0.24753055 0.58023083 0.12      ], Reward: -1.0\n",
            "Processed Log → State: [0.46443057 0.50186867 0.31437144 0.5923925  0.15      ], Reward: 6.0\n",
            "Processed Log → State: [0.47335213 0.575494   0.36094132 0.62944925 0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.5747935  0.6589794  0.4394493  0.57188153 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.56105316 0.6496205  0.5586846  0.59573203 0.24      ], Reward: 5.0\n",
            "Processed Log → State: [0.64662826 0.6586477  0.5489851  0.5968159  0.27      ], Reward: 7.0\n",
            "Processed Log → State: [0.64334476 0.6407001  0.49248597 0.5200889  0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.67162025 0.69719994 0.4764042  0.528182   0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.6636579  0.740454   0.573414   0.62421715 0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.738794   0.7366242  0.59295255 0.56147903 0.39      ], Reward: 6.0\n",
            "Processed Log → State: [0.8033975  0.7598146  0.73847324 0.5969631  0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.8473636  0.7711651  0.7477233  0.56524414 0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.9064438  0.8233646  0.69376725 0.5596377  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.85017467 0.6699941  0.58772117 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.97683024 0.90310425 0.6039558  0.55499256 0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.96426195 0.94573265 0.72923607 0.5597552  0.57      ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.95499086 0.7122742  0.5406498  0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.90680915 0.9661971  0.6899628  0.51359487 0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.9071628  0.96315706 0.623102   0.6004691  0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.89434    0.94320476 0.50883234 0.55526227 0.69      ], Reward: 7.0\n",
            "Processed Log → State: [0.90118414 0.9148008  0.5346996  0.4547135  0.72      ], Reward: 5.0\n",
            "Processed Log → State: [0.93657464 0.92458487 0.504022   0.44412637 0.75      ], Reward: 7.0\n",
            "Processed Log → State: [0.9271809  0.91352004 0.5735116  0.49558187 0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.9392346  0.91276747 0.50780725 0.5086871  0.81      ], Reward: 6.0\n",
            "Processed Log → State: [0.93556654 0.9294387  0.561757   0.44652832 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.9629527  0.88750035 0.5795467  0.4510348  0.87      ], Reward: 4.0\n",
            "Processed Log → State: [0.9908764  0.8740097  0.5955675  0.50779593 0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.24174382 0.3382503  0.04235971 0.53382266 0.03      ], Reward: 2.0\n",
            "Processed Log → State: [0.39913166 0.35407177 0.11718868 0.5139926  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.3988886  0.39470518 0.18428995 0.45733476 0.09      ], Reward: 2.0\n",
            "Processed Log → State: [0.55038375 0.46418238 0.16073495 0.4058203  0.12      ], Reward: 0.0\n",
            "Processed Log → State: [0.63614994 0.4395118  0.16448657 0.42577872 0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.7135564  0.45863724 0.20020755 0.33026645 0.18      ], Reward: 5.0\n",
            "Processed Log → State: [0.79050374 0.5068705  0.25571612 0.25377977 0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.8253734  0.49963206 0.2794133  0.1817833  0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.9597473  0.51236993 0.29225108 0.159587   0.27      ], Reward: 2.0\n",
            "Processed Log → State: [1.         0.57111096 0.304205   0.10389297 0.3       ], Reward: 2.0\n",
            "Processed Log → State: [1.         0.6197419  0.27761334 0.06432591 0.33      ], Reward: 2.0\n",
            "Processed Log → State: [1.         0.6544439  0.3112448  0.09931745 0.36      ], Reward: 5.0\n",
            "Processed Log → State: [1.         0.6424384  0.3042459  0.12437863 0.39      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.7000637  0.24942306 0.17679162 0.42      ], Reward: 4.0\n",
            "Processed Log → State: [1.         0.6419541  0.24735463 0.19941312 0.45      ], Reward: 0.0\n",
            "Processed Log → State: [0.9424816  0.6695238  0.2839702  0.18597852 0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.9479051  0.660698   0.29579404 0.15135147 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.9562868  0.6952091  0.30078822 0.18413861 0.54      ], Reward: 0.0\n",
            "Processed Log → State: [0.93727434 0.72524047 0.3154076  0.19729055 0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.92955935 0.7326425  0.39985943 0.21873514 0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.93579376 0.7637386  0.48064253 0.28037673 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.9433283  0.7642601  0.4818061  0.25198653 0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.9406548  0.7703879  0.43125746 0.3267172  0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.9448476  0.78650635 0.50223273 0.37005508 0.72      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.7936552  0.589649   0.26990798 0.75      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.7926947  0.63797194 0.22397874 0.78      ], Reward: 6.0\n",
            "Processed Log → State: [0.99556154 0.77729696 0.6499649  0.21808545 0.81      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.7817075  0.69187605 0.29902202 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.98372114 0.7390847  0.77572197 0.26592895 0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.9767061  0.7431376  0.74828094 0.21550258 0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.16655456 0.33655202 0.03233236 0.5993215  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.21638872 0.501898   0.09677196 0.4645542  0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.24712092 0.59046715 0.08844218 0.4606976  0.09      ], Reward: 5.0\n",
            "Processed Log → State: [0.28658524 0.65879756 0.08861046 0.42521515 0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.28048488 0.61702514 0.0705246  0.4580745  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.33065543 0.52695394 0.09340765 0.45318896 0.18      ], Reward: 0.0\n",
            "Processed Log → State: [0.33224246 0.500299   0.11226311 0.44440654 0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.43913627 0.4645483  0.12686457 0.47852656 0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.40335637 0.47007024 0.15053618 0.4263784  0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.37472475 0.52230144 0.19762862 0.42110205 0.3       ], Reward: 5.0\n",
            "Processed Log → State: [0.42165896 0.5146156  0.26404727 0.4550161  0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.45366722 0.59769255 0.31750914 0.42461634 0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.48801723 0.6603472  0.270105   0.3793407  0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.62840563 0.7149323  0.3246837  0.28712466 0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.69882494 0.8266686  0.34156975 0.28624812 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.76334894 0.8541747  0.36352426 0.23723175 0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.6760215  0.9733885  0.36360466 0.21587355 0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.69401324 1.         0.35135466 0.24427487 0.54      ], Reward: 7.0\n",
            "Processed Log → State: [0.71789557 0.9380188  0.38578764 0.268266   0.57      ], Reward: 0.0\n",
            "Processed Log → State: [0.7619285  0.9126128  0.36529016 0.25256166 0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.7887569  0.93156755 0.3765726  0.2494027  0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.7769029  0.8941163  0.36936873 0.2973425  0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.760652   0.91577333 0.38402984 0.32561514 0.69      ], Reward: 0.0\n",
            "Processed Log → State: [0.75978583 0.8901964  0.38019377 0.33339262 0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.74025416 0.8496262  0.40374863 0.45093265 0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.755993   0.9042045  0.42985526 0.41411984 0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.78472537 0.91371465 0.43058386 0.52530396 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.7769103 0.9120652 0.4533092 0.5124489 0.84     ], Reward: 0.0\n",
            "Processed Log → State: [0.79022527 0.90765536 0.44563454 0.59332544 0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.765369   0.89534104 0.45396566 0.65890855 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.20728533 0.2756781  0.04609138 0.63101286 0.03      ], Reward: 2.0\n",
            "Processed Log → State: [0.23403147 0.30758384 0.03342925 0.6734588  0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.27177277 0.325588   0.03177371 0.7177239  0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.28620833 0.36267358 0.03083603 0.7819771  0.12      ], Reward: 0.0\n",
            "Processed Log → State: [0.31292596 0.36215493 0.05376572 0.7756116  0.15      ], Reward: 5.0\n",
            "Processed Log → State: [0.31772447 0.36271957 0.05985345 0.825771   0.18      ], Reward: 5.0\n",
            "Processed Log → State: [0.337988   0.3788657  0.06350294 0.8419214  0.21      ], Reward: 7.0\n",
            "Processed Log → State: [0.33691874 0.42443308 0.06367547 0.91263694 0.24      ], Reward: 7.0\n",
            "Processed Log → State: [0.32603285 0.38231733 0.06304591 0.9016257  0.27      ], Reward: -2.0\n",
            "Processed Log → State: [0.34451878 0.39003465 0.07324743 0.9410598  0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.34013003 0.39470744 0.06437612 0.9551986  0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.33900365 0.41507953 0.06826244 0.9648924  0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.3495787  0.47404012 0.07222815 0.96621454 0.39      ], Reward: 7.0\n",
            "Processed Log → State: [0.34809527 0.49991825 0.07246234 0.99454594 0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.34336796 0.5104282  0.08617887 1.         0.45      ], Reward: 0.0\n",
            "Processed Log → State: [0.34356365 0.49184197 0.09862485 0.9837403  0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.3241024 0.5021271 0.1031535 0.9979693 0.51     ], Reward: 3.0\n",
            "Processed Log → State: [0.3311753  0.5266919  0.10093696 1.         0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.3639834  0.5510208  0.10085794 1.         0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.3832189  0.5756794  0.10207025 0.94294727 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.3576451  0.5737323  0.09777949 1.         0.63      ], Reward: 6.0\n",
            "Processed Log → State: [0.33157513 0.6158936  0.09905253 0.98911923 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.34398562 0.597447   0.10548537 0.96220344 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.33312657 0.59392625 0.08438166 0.9638161  0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.36660543 0.6183957  0.07764371 1.         0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.3567346  0.61501855 0.08631227 0.99310833 0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.38206083 0.6155147  0.11652466 0.94173837 0.81      ], Reward: 7.0\n",
            "Processed Log → State: [0.40273845 0.63491637 0.12645742 0.8915756  0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.4102002  0.6248054  0.15601806 0.9213502  0.87      ], Reward: 2.0\n",
            "Processed Log → State: [0.41407084 0.6423282  0.17350891 1.         0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.21484928 0.3379798  0.05870651 0.612685   0.03      ], Reward: 7.0\n",
            "Processed Log → State: [0.24260879 0.3467257  0.05326096 0.66860926 0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.23931007 0.3611288  0.04747736 0.71412015 0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.24572003 0.37431297 0.04404286 0.69766265 0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.29557624 0.37469178 0.03740627 0.78148544 0.15      ], Reward: 6.0\n",
            "Processed Log → State: [0.31515703 0.39435387 0.0392551  0.80923885 0.18      ], Reward: -1.0\n",
            "Processed Log → State: [0.31722885 0.41762042 0.03107888 0.8132692  0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.32797962 0.40128168 0.04548852 0.8529401  0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.34034932 0.38807252 0.03939707 0.8023527  0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.29161137 0.37251097 0.04541864 0.8458489  0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.29577634 0.3853442  0.03958666 0.884324   0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.2923032  0.39033282 0.05280393 0.9340135  0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.28872052 0.37896776 0.05719663 0.9379417  0.39      ], Reward: 6.0\n",
            "Processed Log → State: [0.335404   0.37418213 0.0657564  0.9592718  0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.34157786 0.36892262 0.05395155 1.         0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.3521321  0.4041877  0.06251577 0.9802455  0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.38662738 0.39069325 0.0687432  0.97405857 0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.3595409  0.38182473 0.06737548 1.         0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.40131307 0.38320786 0.06790833 1.         0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.37706825 0.39705575 0.08824009 1.         0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.407779   0.41712072 0.10282599 1.         0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.44123206 0.4675751  0.11381943 1.         0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.4237199  0.5192355  0.11226145 0.934603   0.69      ], Reward: 7.0\n",
            "Processed Log → State: [0.42162377 0.53725916 0.11318519 0.9676389  0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.44289374 0.5075479  0.10582736 1.         0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.44602534 0.50819886 0.1166325  0.9332427  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.4563634  0.52913153 0.13188465 0.98507255 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.43921846 0.53967804 0.15308526 0.97589785 0.84      ], Reward: 5.0\n",
            "Processed Log → State: [0.43375435 0.5393383  0.16294044 0.90270615 0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.43127447 0.55453515 0.16260041 0.90177673 0.9       ], Reward: 0.0\n",
            "Processed Log → State: [0.24017532 0.3650148  0.04289241 0.5476379  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.28022188 0.32739982 0.06770227 0.5367539  0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.35461712 0.33874586 0.12617712 0.48867628 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.38006622 0.3802771  0.11304583 0.39774802 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.39129275 0.4687965  0.12255383 0.39127967 0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.52017814 0.51347834 0.08965372 0.44999167 0.18      ], Reward: 2.0\n",
            "Processed Log → State: [0.6290814  0.6023385  0.11321889 0.40631047 0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.63502055 0.65347654 0.1572808  0.40329975 0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.6243548  0.71610916 0.18160328 0.40137514 0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.58030975 0.7132041  0.11378878 0.38158682 0.3       ], Reward: 0.0\n",
            "Processed Log → State: [0.7009865  0.75749785 0.15480714 0.35750094 0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.76771843 0.8396212  0.14439432 0.3802702  0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.8365671  0.81670165 0.16168626 0.4398771  0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.8147493  0.7799618  0.1866587  0.39407215 0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.8441207  0.8721127  0.24626753 0.4147165  0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.79915357 0.9652902  0.2730931  0.33088493 0.48      ], Reward: -1.0\n",
            "Processed Log → State: [0.8058282  0.96421987 0.26160923 0.28470337 0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.85852414 1.         0.2506632  0.31996384 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.8414989  1.         0.22153598 0.39208928 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.8693739  1.         0.21165518 0.46832243 0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.8319666  0.981885   0.18730898 0.41995764 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.86010104 1.         0.17429787 0.34312984 0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.90830076 0.96853274 0.18608294 0.33227864 0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.8922019  1.         0.18316372 0.33621842 0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.8631595  1.         0.2293908  0.32276264 0.75      ], Reward: 0.0\n",
            "Processed Log → State: [0.81629604 0.9688133  0.2304574  0.37097746 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.80410767 1.         0.23456159 0.27875286 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.8511637  1.         0.2584531  0.33554223 0.84      ], Reward: 4.0\n",
            "Processed Log → State: [0.87978023 1.         0.30064765 0.360988   0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.87571436 1.         0.33926687 0.3609883  0.9       ], Reward: 6.0\n",
            "Processed Log → State: [0.31272623 0.36544403 0.05906376 0.5092884  0.03      ], Reward: 2.0\n",
            "Processed Log → State: [0.4232653  0.41716295 0.03303335 0.5142362  0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.46753207 0.4466897  0.01736531 0.47697192 0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.41080105 0.54601187 0.04122006 0.39031383 0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.44500113 0.5543973  0.07968761 0.4095025  0.15      ], Reward: -1.0\n",
            "Processed Log → State: [0.42840856 0.5793944  0.13492821 0.3123141  0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.5828957 0.6075377 0.14284   0.3016905 0.21     ], Reward: 6.0\n",
            "Processed Log → State: [0.6050483  0.6357843  0.18447486 0.31114277 0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.69830436 0.6219205  0.20504463 0.3049784  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.7182052  0.62792134 0.27942207 0.3253919  0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.70697755 0.63450575 0.31579328 0.31942463 0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.69654435 0.71718895 0.43570447 0.27651    0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.627016   0.75582993 0.46404296 0.2968126  0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.77944493 0.81798804 0.4112601  0.266326   0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.8180002  0.78407544 0.46390814 0.25949037 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.8460264  0.80696297 0.55573696 0.27528432 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.8769788  0.7989622  0.5755144  0.33656746 0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.8716321  0.82799864 0.6640618  0.29217786 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.8661323  0.8468118  0.7295147  0.27028194 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.90156287 0.8303671  0.819709   0.18676583 0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.8993439  0.8274391  0.8322248  0.21276669 0.63      ], Reward: 5.0\n",
            "Processed Log → State: [0.8862313  0.8067759  0.81963986 0.28329676 0.66      ], Reward: -2.0\n",
            "Processed Log → State: [0.83758277 0.8067483  0.8283502  0.28803095 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.78212774 0.81212497 0.8266295  0.3456814  0.72      ], Reward: 6.0\n",
            "Processed Log → State: [0.76057094 0.85527956 0.8665172  0.4087214  0.75      ], Reward: 0.0\n",
            "Processed Log → State: [0.79897493 0.8635461  0.84552455 0.43971297 0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.8272181  0.86458635 0.82038397 0.42924374 0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.86659783 0.8484509  0.8197911  0.391887   0.84      ], Reward: 5.0\n",
            "Processed Log → State: [0.8574929  0.81327325 0.85491824 0.36494622 0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.87042624 0.82911044 0.8234633  0.34574875 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.2051459  0.31035134 0.04974931 0.5738687  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.2342832  0.34544227 0.04442744 0.5802833  0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.2585694  0.3611527  0.0616855  0.65532786 0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.25555924 0.35288063 0.06053391 0.67915565 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.27012444 0.3635742  0.06605756 0.7401174  0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.25632182 0.41523317 0.06024444 0.7794908  0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.24379174 0.4250693  0.07186279 0.7969932  0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.27842107 0.39477098 0.06550444 0.8734372  0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.27651364 0.40401968 0.0663217  0.9211719  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.30872422 0.43300194 0.07434428 0.9458816  0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.3275197  0.44174826 0.0651925  0.97647184 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.30565965 0.4323951  0.07131485 0.9607243  0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.34736678 0.4581853  0.07112589 0.9022044  0.39      ], Reward: 5.0\n",
            "Processed Log → State: [0.36340192 0.47117585 0.08281509 0.8721097  0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.36918697 0.49275133 0.09677355 0.8865203  0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.42598787 0.49734783 0.08434624 0.8642953  0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.4522992  0.48934782 0.08009108 0.9183778  0.51      ], Reward: 5.0\n",
            "Processed Log → State: [0.4762508  0.47707975 0.08125477 1.         0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.5242849  0.50262684 0.09530506 1.         0.57      ], Reward: 5.0\n",
            "Processed Log → State: [0.55257016 0.50205064 0.09148765 0.99881285 0.6       ], Reward: 5.0\n",
            "Processed Log → State: [0.59568584 0.5303895  0.08828552 1.         0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.5736289  0.5214699  0.10443409 1.         0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.5905921  0.524563   0.08296095 0.90465015 0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.5766701  0.5338191  0.08029453 0.87869906 0.72      ], Reward: 6.0\n",
            "Processed Log → State: [0.60572064 0.556306   0.0642814  0.8485501  0.75      ], Reward: 5.0\n",
            "Processed Log → State: [0.60869426 0.56572306 0.07485919 0.9702314  0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.5665898  0.5574077  0.11098847 0.97145534 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.58001405 0.54002696 0.13271217 0.9928235  0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.5960327  0.5712253  0.15163253 1.         0.87      ], Reward: 7.0\n",
            "Processed Log → State: [0.62749845 0.57015544 0.15834543 1.         0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.20718408 0.32605356 0.07083167 0.6144451  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.21870081 0.3460914  0.08275061 0.6190222  0.06      ], Reward: 0.0\n",
            "Processed Log → State: [0.2246898  0.36039603 0.0927041  0.6822085  0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.2758885  0.40575638 0.0938838  0.76146376 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.2985053  0.4319374  0.09007523 0.8057155  0.15      ], Reward: 7.0\n",
            "Processed Log → State: [0.31761423 0.427195   0.10295375 0.84690624 0.18      ], Reward: 7.0\n",
            "Processed Log → State: [0.34085122 0.44115666 0.11465348 0.8894779  0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.35413906 0.46652502 0.13748364 0.8738222  0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.37266633 0.488235   0.13474187 0.89573437 0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.37389207 0.51600444 0.15717062 0.93253994 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.41856074 0.5206131  0.17423384 0.9179788  0.33      ], Reward: 2.0\n",
            "Processed Log → State: [0.44499126 0.55176055 0.1721238  0.9067896  0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.41135085 0.55412483 0.18719965 0.9618316  0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.41277826 0.5484778  0.18599302 0.9419857  0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.40729162 0.58872104 0.18532093 1.         0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.4016428  0.5890576  0.18799828 1.         0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.41692072 0.58782274 0.19222093 1.         0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.4383355  0.586069   0.19024843 1.         0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.45508823 0.6097368  0.18927367 1.         0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.48529822 0.6376025  0.19409952 1.         0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.5004237  0.6204868  0.19769935 0.9989244  0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.5048081  0.6283753  0.19370002 1.         0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.50968367 0.65093213 0.22136214 1.         0.69      ], Reward: 8.0\n",
            "Processed Log → State: [0.52372086 0.6766083  0.22342081 1.         0.72      ], Reward: 0.0\n",
            "Processed Log → State: [0.50284654 0.6761876  0.20939675 0.97233355 0.75      ], Reward: 7.0\n",
            "Processed Log → State: [0.5140839 0.7196924 0.1918475 0.9861262 0.78     ], Reward: 0.0\n",
            "Processed Log → State: [0.5144292  0.7338088  0.20921437 1.         0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.49859345 0.71992356 0.23836973 0.98275596 0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.5048325 0.7080985 0.2574598 0.9921057 0.87     ], Reward: 8.0\n",
            "Processed Log → State: [0.51220566 0.69730484 0.2813303  0.97598    0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.29208168 0.33802968 0.05696484 0.5915148  0.03      ], Reward: 2.0\n",
            "Processed Log → State: [0.29856303 0.3729296  0.12395865 0.5447504  0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.3900548  0.37920237 0.19261503 0.5589314  0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.40126973 0.3983038  0.23635079 0.66901594 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.474705   0.46661347 0.31871933 0.6190692  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.51974833 0.5344113  0.373213   0.65923285 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.62664896 0.54064363 0.53010917 0.6755135  0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.6301771  0.58609027 0.47969005 0.53596145 0.24      ], Reward: 5.0\n",
            "Processed Log → State: [0.6786822  0.5876024  0.57262015 0.4339626  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.7715232  0.61551535 0.6224022  0.44361773 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.79404604 0.6619614  0.62317145 0.40913582 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.94856566 0.6738936  0.6603474  0.39532477 0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.9211659 0.6695345 0.7094121 0.42329   0.39     ], Reward: 3.0\n",
            "Processed Log → State: [0.98565143 0.7113202  0.8041811  0.3761951  0.42      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.7000006  0.83488756 0.33829528 0.45      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.7302916  0.77984935 0.305575   0.48      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.7709695  0.75376445 0.32832336 0.51      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.772018   0.75551283 0.2754268  0.54      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.7736269  0.8495082  0.22392917 0.57      ], Reward: 6.0\n",
            "Processed Log → State: [1.        0.8281878 0.8958432 0.1586203 0.6      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.8180773  0.8973881  0.16803198 0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.9852221  0.79127604 0.80189955 0.23008674 0.66      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.8005796  0.65798044 0.19144504 0.69      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.81730586 0.65051943 0.12700552 0.72      ], Reward: 5.0\n",
            "Processed Log → State: [1.         0.81426966 0.62650263 0.11948234 0.75      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.78177255 0.5932484  0.12865628 0.78      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.7652301  0.598095   0.17328864 0.81      ], Reward: 6.0\n",
            "Processed Log → State: [0.91950816 0.776576   0.5381529  0.20096983 0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.90975416 0.8281931  0.5908883  0.26770782 0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.94003683 0.7965619  0.5559444  0.2540304  0.9       ], Reward: 0.0\n",
            "Processed Log → State: [0.24809198 0.3506521  0.15482675 0.6416476  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.32638392 0.430054   0.21589972 0.66433495 0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.38036668 0.4645543  0.33957452 0.6540215  0.09      ], Reward: 7.0\n",
            "Processed Log → State: [0.44284025 0.4390026  0.48215082 0.6042822  0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.5013882  0.4858727  0.55063534 0.5850335  0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.52180064 0.53266394 0.6307214  0.6616521  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.6097462  0.5742235  0.68528783 0.6086337  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.5654014  0.59638965 0.73584044 0.5449975  0.24      ], Reward: -1.0\n",
            "Processed Log → State: [0.64143276 0.57309765 0.7324788  0.55324256 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.60922873 0.58112264 0.86820173 0.5755796  0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.60066646 0.5656304  0.90871346 0.511297   0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.6019984  0.5664829  0.8887797  0.39693525 0.36      ], Reward: 4.0\n",
            "Processed Log → State: [0.640338   0.6215412  0.90473855 0.38596702 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.69739586 0.5952493  0.931287   0.36459556 0.42      ], Reward: 7.0\n",
            "Processed Log → State: [0.7366478  0.63417584 0.91770077 0.31664652 0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.7837097  0.65898496 0.8657053  0.38877827 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.8056322  0.7099684  0.84702766 0.4220678  0.51      ], Reward: -1.0\n",
            "Processed Log → State: [0.8554705  0.76324946 0.9142204  0.41477415 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.8925637  0.8149597  0.93225497 0.46638048 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.82446223 0.92841804 0.5507378  0.6       ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.8109246  0.91519016 0.57724315 0.63      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.81037337 0.8247087  0.6315931  0.66      ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.823688   0.83405066 0.6414403  0.69      ], Reward: 3.0\n",
            "Processed Log → State: [1.        0.8355957 0.7961672 0.5932882 0.72     ], Reward: 7.0\n",
            "Processed Log → State: [0.9475402  0.8426987  0.80569667 0.5678838  0.75      ], Reward: 0.0\n",
            "Processed Log → State: [0.9175243  0.7817622  0.6715554  0.45312417 0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.93803203 0.7828904  0.7356865  0.4542196  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.98872584 0.7946616  0.7496647  0.4132974  0.84      ], Reward: 5.0\n",
            "Processed Log → State: [1.         0.81112194 0.7669351  0.44073611 0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.99763775 0.85110134 0.8060144  0.37547144 0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.20711286 0.3283117  0.09761418 0.6402165  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.25579858 0.381646   0.08390423 0.5786403  0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.3342237  0.43812948 0.08472869 0.5987621  0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.3243899  0.49725527 0.10040193 0.56692964 0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.32503653 0.63698345 0.11293815 0.49003413 0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.37015873 0.73062855 0.11964212 0.47796398 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.37366745 0.7286298  0.2183187  0.3878608  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.35417262 0.7673282  0.2852065  0.39989784 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.29447442 0.78901577 0.3973483  0.44654602 0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.34254065 0.8096875  0.47025976 0.38985154 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.35373122 0.84192425 0.44994733 0.417139   0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.37585178 0.8468989  0.501827   0.39020345 0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.38748875 0.82142764 0.51834327 0.41753381 0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.42000264 0.8139617  0.5663993  0.46933696 0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.48326012 0.8327163  0.6310106  0.4066746  0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.5514937  0.8509175  0.6907526  0.37706476 0.48      ], Reward: 6.0\n",
            "Processed Log → State: [0.5483876  0.8494327  0.6785155  0.33364454 0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.5113633  0.82576275 0.7657423  0.31012312 0.54      ], Reward: 7.0\n",
            "Processed Log → State: [0.5418388  0.8717827  0.67160016 0.32886788 0.57      ], Reward: 2.0\n",
            "Processed Log → State: [0.55320615 0.88953364 0.69694054 0.29941404 0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.5636449  0.9232414  0.67475235 0.35340896 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.5785678  0.9279634  0.7011854  0.36360452 0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.64308894 0.9832385  0.714413   0.37867033 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.6582243  0.97610515 0.7237498  0.34945828 0.72      ], Reward: 6.0\n",
            "Processed Log → State: [0.7546871  0.9809519  0.7456246  0.39376813 0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.80523694 0.9512692  0.76272213 0.3976495  0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.81650096 0.88537514 0.7872517  0.44728667 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.84575295 0.84013885 0.8557297  0.4171323  0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.78588474 0.81923795 0.9824027  0.4538042  0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.80707526 0.77665037 1.         0.5198254  0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.21211429 0.3288735  0.05176083 0.59460545 0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.27271038 0.35100853 0.05671924 0.68205965 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.32576796 0.33936888 0.07188856 0.75753057 0.09      ], Reward: 7.0\n",
            "Processed Log → State: [0.34851807 0.33263636 0.08626015 0.8225945  0.12      ], Reward: 5.0\n",
            "Processed Log → State: [0.35169658 0.37316877 0.09416457 0.87724555 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.37188563 0.40161765 0.10018226 0.8298139  0.18      ], Reward: 7.0\n",
            "Processed Log → State: [0.40731427 0.42100435 0.1210253  0.84020245 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.3777639  0.45524314 0.1098235  0.9850649  0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.39103493 0.46853527 0.11897209 0.9919402  0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.4251174  0.49095166 0.12303542 1.         0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.447443   0.50398374 0.14272045 1.         0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.4994853  0.5216481  0.13579609 1.         0.36      ], Reward: 7.0\n",
            "Processed Log → State: [0.5213206  0.53403956 0.14227544 1.         0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.51643616 0.56247026 0.16182666 0.982315   0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.5263564  0.6063452  0.15882978 1.         0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.5244298  0.64027125 0.17370832 1.         0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.5203035  0.65047055 0.1771061  1.         0.51      ], Reward: 2.0\n",
            "Processed Log → State: [0.5053309  0.6329932  0.18474261 1.         0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.49972942 0.639258   0.18724681 1.         0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.5094674  0.62502265 0.20042169 0.9993989  0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.530514   0.63717914 0.2177648  1.         0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.54787236 0.66177845 0.21277662 0.94509995 0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.5679706  0.6513129  0.1951947  0.93762594 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.59753203 0.6516836  0.19338618 0.980576   0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.60707724 0.6506735  0.19573605 0.99865115 0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.63799804 0.6381874  0.19602454 0.9824189  0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.6766433  0.6633837  0.21483967 0.98091096 0.81      ], Reward: 7.0\n",
            "Processed Log → State: [0.6819575  0.65343064 0.24610028 0.9142449  0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.7066366  0.61840713 0.25325695 0.83928835 0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.74822634 0.60462314 0.24857804 0.8549379  0.9       ], Reward: -1.0\n",
            "Processed Log → State: [0.23049071 0.3383861  0.05606087 0.5716221  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.26070723 0.35510403 0.05232742 0.5192522  0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.2630053  0.3549848  0.03275012 0.61409444 0.09      ], Reward: 2.0\n",
            "Processed Log → State: [0.26551777 0.3882691  0.04139964 0.64877695 0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.29616365 0.41436195 0.06839169 0.68650883 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.29390845 0.44953486 0.07325199 0.7458457  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.34955367 0.50455475 0.07840873 0.7966042  0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.3671494  0.5583798  0.10494819 0.8532898  0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.3778817  0.57186246 0.13070285 0.84927696 0.27      ], Reward: 0.0\n",
            "Processed Log → State: [0.3888049  0.5835544  0.13904954 0.898419   0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.41711402 0.61470866 0.15017724 0.9573361  0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.42990452 0.65183866 0.1676469  0.97707605 0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.4493837  0.66814613 0.16941205 1.         0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.48837164 0.72169584 0.16283995 1.         0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.4912565  0.71852785 0.17009844 1.         0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.5259851  0.73687047 0.17416151 1.         0.48      ], Reward: 4.0\n",
            "Processed Log → State: [0.524902   0.7447985  0.17915215 1.         0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.533455   0.74772644 0.18194917 1.         0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.53375065 0.7866637  0.17731006 0.9900044  0.57      ], Reward: -2.0\n",
            "Processed Log → State: [0.5099235  0.78812635 0.19215275 1.         0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.53980803 0.79346776 0.18551697 1.         0.63      ], Reward: 2.0\n",
            "Processed Log → State: [0.549634   0.7934577  0.18933392 1.         0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.5696591  0.7972106  0.20010345 1.         0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.5870087  0.78189623 0.20922863 0.9719907  0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.5725263  0.76458913 0.21595769 0.9409006  0.75      ], Reward: 0.0\n",
            "Processed Log → State: [0.59544003 0.7599906  0.20625521 0.95778924 0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.6122554  0.75942725 0.21673995 0.95449275 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.6289461 0.7678878 0.238103  0.9306594 0.84     ], Reward: 2.0\n",
            "Processed Log → State: [0.6594757  0.7444886  0.24907419 0.9843229  0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.63374263 0.743637   0.26386365 0.98942274 0.9       ], Reward: 4.0\n",
            "Processed Log → State: [0.18841793 0.30620366 0.06679371 0.63427556 0.03      ], Reward: 7.0\n",
            "Processed Log → State: [0.1799163  0.3332712  0.04361605 0.71567845 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.19848813 0.35111475 0.05610305 0.764947   0.09      ], Reward: 2.0\n",
            "Processed Log → State: [0.16788054 0.3627854  0.04896647 0.739609   0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.176727   0.38506544 0.05034956 0.7826037  0.15      ], Reward: 5.0\n",
            "Processed Log → State: [0.16928774 0.37829792 0.06747626 0.826662   0.18      ], Reward: 6.0\n",
            "Processed Log → State: [0.15719931 0.4169119  0.06960586 0.91417944 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.20659505 0.43423578 0.06616722 0.9261518  0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.20790067 0.49885353 0.09119531 1.         0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.22413096 0.53038496 0.08216903 1.         0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.24483125 0.5196868  0.07249407 0.93833864 0.33      ], Reward: 5.0\n",
            "Processed Log → State: [0.24597159 0.54983103 0.08024495 0.99488765 0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.25170243 0.5571744  0.07533041 0.97799563 0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.2560549  0.5526599  0.07174709 1.         0.42      ], Reward: 0.0\n",
            "Processed Log → State: [0.25140405 0.5334636  0.08283295 0.99307996 0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.2464462  0.5679041  0.08692546 0.9947449  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.2527857  0.5672337  0.07729799 0.93009484 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.22232229 0.5817907  0.07717641 0.98341644 0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.2176797  0.6073272  0.08410428 0.9693334  0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.22612277 0.61299247 0.09597107 0.9882897  0.6       ], Reward: 7.0\n",
            "Processed Log → State: [0.23954698 0.6255059  0.10640925 0.9957302  0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.26598158 0.6500758  0.10514613 1.         0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.28225973 0.66706115 0.10955605 1.         0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.3231065  0.68183935 0.12020659 0.9846774  0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.36380357 0.68112534 0.11369615 1.         0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.37166733 0.68342215 0.11970453 1.         0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.34247285 0.6647836  0.14414884 1.         0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.34778902 0.6291433  0.15670587 1.         0.84      ], Reward: 5.0\n",
            "Processed Log → State: [0.3278426  0.62120056 0.17388968 1.         0.87      ], Reward: 7.0\n",
            "Processed Log → State: [0.3576454  0.62956476 0.17714946 1.         0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.22256307 0.32387295 0.04769183 0.6030139  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.21191823 0.29962176 0.03908756 0.6065909  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.21562226 0.3006979  0.04599866 0.63360924 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.21505697 0.33408973 0.05905642 0.7174828  0.12      ], Reward: -1.0\n",
            "Processed Log → State: [0.22562295 0.37596807 0.06824031 0.74037004 0.15      ], Reward: 5.0\n",
            "Processed Log → State: [0.21469034 0.4243334  0.08333191 0.7389858  0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.24550001 0.46423462 0.09523776 0.7692258  0.21      ], Reward: 5.0\n",
            "Processed Log → State: [0.28477907 0.4916525  0.1042207  0.78134423 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.29389328 0.49788347 0.11460191 0.8152674  0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.34143624 0.5082527  0.13611652 0.79725444 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.37485412 0.51614445 0.1426694  0.7896353  0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.39220282 0.52664244 0.14842428 0.87318915 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.4328821  0.52461284 0.15887153 0.88194513 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.43244666 0.51025504 0.17570366 0.87319744 0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.42518708 0.54089254 0.19013947 0.9491697  0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.425912   0.54745996 0.1861839  0.9523834  0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.45476982 0.5432756  0.21330422 0.927591   0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.45117924 0.53032005 0.23686075 0.92862    0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.4867137  0.5708383  0.22048455 1.         0.57      ], Reward: 0.0\n",
            "Processed Log → State: [0.49142843 0.5874592  0.21788621 1.         0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.50370425 0.57936615 0.21095483 1.         0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.5246053  0.5853153  0.22017491 1.         0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.52068967 0.5688482  0.22428009 0.9912584  0.69      ], Reward: 7.0\n",
            "Processed Log → State: [0.5123627  0.57516074 0.21626168 1.         0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.5240969  0.6019249  0.22318791 0.9929137  0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.4924725 0.5781176 0.207076  1.        0.78     ], Reward: 3.0\n",
            "Processed Log → State: [0.52314156 0.55766034 0.22343908 0.95958394 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.5451779  0.55133694 0.2331144  0.97606957 0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.5032613  0.57809263 0.242937   0.96626157 0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.5144757  0.597489   0.25788915 0.99981654 0.9       ], Reward: -1.0\n",
            "Processed Log → State: [0.14582562 0.32075077 0.08320969 0.61671674 0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.1849629  0.35897547 0.10819699 0.5324433  0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.2363285  0.365374   0.18410923 0.5917299  0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.29116222 0.363643   0.29847336 0.5678054  0.12      ], Reward: 5.0\n",
            "Processed Log → State: [0.29380667 0.39875197 0.3959851  0.5945019  0.15      ], Reward: 5.0\n",
            "Processed Log → State: [0.25809562 0.4485334  0.4145537  0.5371866  0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.3283986  0.48419556 0.5200834  0.53752923 0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.302218   0.57979524 0.54565936 0.53605556 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.25917643 0.612125   0.61517584 0.496215   0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.30496058 0.6914539  0.6591172  0.4680702  0.3       ], Reward: 4.0\n",
            "Processed Log → State: [0.3935992  0.67072874 0.61241215 0.3679421  0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.43982038 0.65211695 0.67790306 0.37263572 0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.50123453 0.7047549  0.63808197 0.3488752  0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.58953476 0.669617   0.70524436 0.4258033  0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.6107359 0.6689956 0.8156165 0.4053714 0.45     ], Reward: 6.0\n",
            "Processed Log → State: [0.6276018  0.710509   0.9024553  0.38535488 0.48      ], Reward: 6.0\n",
            "Processed Log → State: [0.6664219  0.72269297 0.89488816 0.35350257 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.67721486 0.71931285 0.8545801  0.19846599 0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.8149881  0.69581205 0.8398736  0.22924803 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.84992605 0.72856313 0.7913583  0.19742899 0.6       ], Reward: 7.0\n",
            "Processed Log → State: [0.8654843  0.72397    0.7658213  0.17726733 0.63      ], Reward: -1.0\n",
            "Processed Log → State: [0.86489284 0.71276945 0.7962273  0.20417894 0.66      ], Reward: 7.0\n",
            "Processed Log → State: [0.8973108  0.72839785 0.8044294  0.31301975 0.69      ], Reward: 5.0\n",
            "Processed Log → State: [0.9249419 0.7532852 0.7743051 0.2897778 0.72     ], Reward: 1.0\n",
            "Processed Log → State: [0.89023554 0.77415997 0.77408963 0.24636051 0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.91453034 0.7780604  0.73464984 0.22014272 0.78      ], Reward: 4.0\n",
            "Processed Log → State: [1.         0.8131831  0.745902   0.23060693 0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.98354524 0.8235262  0.74146193 0.24718958 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.9888824  0.81871974 0.6930116  0.35220873 0.87      ], Reward: 2.0\n",
            "Processed Log → State: [1.         0.84561795 0.7272993  0.32728237 0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.29273343 0.32727978 0.11585258 0.5352009  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.37063497 0.34530404 0.13922557 0.5521008  0.06      ], Reward: 4.0\n",
            "Processed Log → State: [0.5135752  0.33949763 0.20941515 0.45499173 0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.5268682  0.3320425  0.20263815 0.29515538 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.58713484 0.3904862  0.27947488 0.24456082 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.61540353 0.48545653 0.21683511 0.23583712 0.18      ], Reward: 0.0\n",
            "Processed Log → State: [0.68491393 0.52227765 0.1936684  0.2934276  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.7290848  0.5344518  0.22808352 0.27011347 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.86001116 0.55968684 0.32525307 0.2718248  0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.9083089  0.6328633  0.3602192  0.19784373 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.9457181  0.65458184 0.44228643 0.18388234 0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.98718315 0.6596856  0.45448166 0.1495482  0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.9875834  0.71406406 0.46802896 0.17921819 0.39      ], Reward: 0.0\n",
            "Processed Log → State: [0.9282272  0.7393983  0.5445246  0.27814955 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [1.        0.7842795 0.5305975 0.3331533 0.45     ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.8188621  0.53224856 0.35408053 0.48      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.8583799  0.5786132  0.30164763 0.51      ], Reward: 5.0\n",
            "Processed Log → State: [0.95237875 0.87251157 0.64186305 0.26159698 0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.9493227  0.8823734  0.7089049  0.26897678 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.9393079  0.9230957  0.7567305  0.29762793 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.9644649  0.80997956 0.32025272 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.98963195 0.93616515 0.8432807  0.40373838 0.66      ], Reward: 7.0\n",
            "Processed Log → State: [1.         0.9319873  0.81202227 0.40941912 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.9085851  0.8888922  0.35047656 0.72      ], Reward: 2.0\n",
            "Processed Log → State: [1.        0.8519065 0.8678553 0.3741598 0.75     ], Reward: 8.0\n",
            "Processed Log → State: [1.        0.8729226 0.8429375 0.3716966 0.78     ], Reward: 3.0\n",
            "Processed Log → State: [0.9527269  0.8665582  0.74164134 0.45770147 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.84911007 0.818704   0.4593477  0.84      ], Reward: 7.0\n",
            "Processed Log → State: [1.         0.7841202  0.82002854 0.45074755 0.87      ], Reward: 0.0\n",
            "Processed Log → State: [0.9093148  0.802599   0.87597924 0.5545724  0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.34905672 0.41523844 0.05718382 0.58446276 0.03      ], Reward: 2.0\n",
            "Processed Log → State: [0.29332024 0.39764956 0.10970142 0.6847945  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.25599772 0.4813617  0.09238356 0.6340323  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.355987   0.5764947  0.12577315 0.6853701  0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.41838658 0.664158   0.09574577 0.59195125 0.15      ], Reward: -1.0\n",
            "Processed Log → State: [0.40876704 0.6770183  0.14797573 0.55136675 0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.4746837  0.70500827 0.1498211  0.65769887 0.21      ], Reward: 4.0\n",
            "Processed Log → State: [0.5408554  0.6925319  0.21059185 0.65767443 0.24      ], Reward: 0.0\n",
            "Processed Log → State: [0.5559515  0.75189304 0.23065135 0.63716644 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.59344524 0.75249976 0.2967331  0.5790233  0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.5814391  0.8399176  0.29759538 0.5506684  0.33      ], Reward: 2.0\n",
            "Processed Log → State: [0.66352093 0.8314863  0.3370883  0.5518165  0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.70371133 0.8979487  0.35720694 0.62998146 0.39      ], Reward: 6.0\n",
            "Processed Log → State: [0.7701321 0.8935074 0.3724169 0.584708  0.42     ], Reward: 1.0\n",
            "Processed Log → State: [0.8188415  0.87270457 0.3237682  0.59827334 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.8023828  0.84475726 0.32577458 0.66491205 0.48      ], Reward: 4.0\n",
            "Processed Log → State: [0.8403372  0.84131616 0.35361752 0.69060534 0.51      ], Reward: 2.0\n",
            "Processed Log → State: [0.890631   0.85609245 0.43931854 0.62428206 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.84117377 0.9707995  0.41823184 0.53993005 0.57      ], Reward: 2.0\n",
            "Processed Log → State: [0.7392675 1.        0.4240524 0.5717053 0.6      ], Reward: 7.0\n",
            "Processed Log → State: [0.76949376 0.9612116  0.41763756 0.60051906 0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.82287294 0.9183652  0.4066106  0.50500476 0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.83145875 0.9425977  0.4273806  0.56302977 0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.9148244  0.90449846 0.44732127 0.53784114 0.72      ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.9146337  0.46862814 0.5364249  0.75      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.9369326  0.52121687 0.55744463 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.9859394  0.90591455 0.5308107  0.5447881  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.82545567 0.5408894  0.5353546  0.84      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.8372833  0.5030075  0.57309353 0.87      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.83861893 0.5083975  0.57169795 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.23474842 0.32798672 0.04826095 0.645224   0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.28445587 0.36627614 0.06185036 0.6700791  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.29205936 0.37992778 0.07893378 0.7759074  0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.32296902 0.4108113  0.07945082 0.8334153  0.12      ], Reward: 0.0\n",
            "Processed Log → State: [0.30344954 0.4439028  0.08451881 0.8655356  0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.32657126 0.4650341  0.10248113 0.9437901  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.36703566 0.47267276 0.11016335 0.9542131  0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.36384925 0.51868373 0.1312828  0.9547733  0.24      ], Reward: 0.0\n",
            "Processed Log → State: [0.4025053  0.55160207 0.13256945 0.98844606 0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.41855487 0.54385155 0.1690435  1.         0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.43438417 0.5548653  0.17731364 1.         0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.4464703  0.5263589  0.18646172 1.         0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.48504046 0.5662105  0.19514163 1.         0.39      ], Reward: -1.0\n",
            "Processed Log → State: [0.53505564 0.58346236 0.19990674 1.         0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.542708   0.5777767  0.18615653 0.935143   0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.5383851  0.58145696 0.18095128 0.9351653  0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.5549714  0.61178046 0.17810266 0.9049995  0.51      ], Reward: -1.0\n",
            "Processed Log → State: [0.5497919  0.64849067 0.18350261 0.9353638  0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.52949697 0.63542825 0.1900799  0.9267021  0.57      ], Reward: 7.0\n",
            "Processed Log → State: [0.5496289  0.6303658  0.19830282 0.9792536  0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.56563205 0.6537406  0.20680705 1.         0.63      ], Reward: 5.0\n",
            "Processed Log → State: [0.57354474 0.64498615 0.2174831  1.         0.66      ], Reward: 0.0\n",
            "Processed Log → State: [0.59719527 0.6575359  0.20794037 1.         0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.60861766 0.67842865 0.22093463 1.         0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.6474209  0.6599965  0.21602443 1.         0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.67783237 0.689111   0.2129326  0.948856   0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.7169189  0.6612479  0.21366306 1.         0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.73493934 0.65910447 0.23823003 0.98639625 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.74779695 0.6926952  0.2435669  1.         0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.7662888 0.6649131 0.2643748 0.9749565 0.9      ], Reward: 2.0\n",
            "Processed Log → State: [0.30260321 0.32192555 0.09991362 0.5302979  0.03      ], Reward: 5.0\n",
            "Processed Log → State: [0.20028369 0.3464878  0.0651628  0.55887437 0.06      ], Reward: 5.0\n",
            "Processed Log → State: [0.29703787 0.43473297 0.14765055 0.47764876 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.2884744  0.47827068 0.16853651 0.41586363 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.35176542 0.48885903 0.15426199 0.42553058 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.3496024  0.52458596 0.2334514  0.2960102  0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.46270102 0.571658   0.31764847 0.3169947  0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.44294095 0.61651045 0.37780148 0.28259584 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.50964946 0.6062313  0.3833013  0.3371404  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.56241214 0.5858331  0.4363287  0.2782787  0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.65401924 0.58622617 0.44455636 0.1920472  0.33      ], Reward: 2.0\n",
            "Processed Log → State: [0.7452737  0.5836646  0.46595088 0.23415522 0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.8818832  0.5890238  0.51876146 0.2837444  0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.9471398 0.5870297 0.5582177 0.1716514 0.42     ], Reward: 5.0\n",
            "Processed Log → State: [1.         0.65244794 0.499857   0.23234583 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.9428786  0.6855536  0.51470834 0.18945047 0.48      ], Reward: 5.0\n",
            "Processed Log → State: [1.         0.72055304 0.5296426  0.13962615 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.7106049  0.5686088  0.15272158 0.54      ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.7324613  0.6051079  0.22274959 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.74677587 0.61376244 0.18112083 0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.9566947  0.7661241  0.69925106 0.2028275  0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.9582054  0.7418331  0.72414917 0.21621871 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.9262583 0.7259686 0.7353277 0.2833977 0.69     ], Reward: 2.0\n",
            "Processed Log → State: [0.97707677 0.74471307 0.7837282  0.29319373 0.72      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.7808958  0.82276714 0.2722639  0.75      ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.80564946 0.7896208  0.30745596 0.78      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.82607627 0.85357326 0.3203506  0.81      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.826188   0.8670293  0.36186656 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.83120674 0.87935185 0.39667964 0.87      ], Reward: 2.0\n",
            "Processed Log → State: [1.         0.8417863  0.8974492  0.36209187 0.9       ], Reward: 0.0\n",
            "Processed Log → State: [0.20890643 0.30475196 0.05990509 0.6490894  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.22710931 0.34362888 0.08296546 0.5968602  0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.27052793 0.3397149  0.10329138 0.7186111  0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.31357434 0.37794632 0.10224042 0.704913   0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.34341377 0.37388715 0.1208351  0.77221054 0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.34660602 0.40995967 0.14239387 0.81288445 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.334476   0.41791844 0.15020369 0.82387924 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.34813476 0.43989065 0.17027405 0.83254266 0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.36994442 0.4222801  0.18329415 0.92749375 0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.3943968  0.44551328 0.16827708 0.9619212  0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.41877642 0.4901208  0.1910803  1.         0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.43972337 0.52110994 0.18188986 1.         0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.42427894 0.55500674 0.17343709 0.99055654 0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.46578783 0.5847336  0.18383676 1.         0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.49450925 0.62973136 0.18570553 1.         0.45      ], Reward: 7.0\n",
            "Processed Log → State: [0.5256084  0.638337   0.19328815 0.97290385 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.526668   0.64583236 0.1979183  1.         0.51      ], Reward: -2.0\n",
            "Processed Log → State: [0.5679404  0.69514614 0.20490217 1.         0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.5661981  0.694018   0.21827665 1.         0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.57403827 0.69841474 0.22487117 1.         0.6       ], Reward: 7.0\n",
            "Processed Log → State: [0.57461154 0.71914786 0.2226716  0.9915956  0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.5833492  0.7432147  0.21688749 0.9717582  0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.5857894  0.76135486 0.22310804 0.9757628  0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.5571729  0.7501635  0.22104578 0.9530701  0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.54209805 0.7702167  0.21989273 0.9850228  0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.555484   0.7653276  0.23569527 1.         0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.56945795 0.7616098  0.26118147 1.         0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.5396285  0.76618165 0.27677992 1.         0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.54888856 0.7627832  0.29659995 1.         0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.51289475 0.7136539  0.31106746 1.         0.9       ], Reward: 0.0\n",
            "Processed Log → State: [0.15939748 0.39528933 0.04185469 0.6239453  0.03      ], Reward: 0.0\n",
            "Processed Log → State: [0.14427586 0.3912122  0.06463574 0.5676233  0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.22764875 0.33372366 0.08206695 0.5045089  0.09      ], Reward: 4.0\n",
            "Processed Log → State: [0.3140144  0.42867446 0.06424119 0.44390565 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.30335507 0.50555617 0.10405423 0.43322653 0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.31165165 0.5662373  0.11741308 0.42504698 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.3002029  0.53484726 0.15873833 0.44774738 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.30047578 0.54296565 0.15457116 0.45282933 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.35386342 0.56288993 0.1726574  0.4824584  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.44789347 0.680228   0.20972641 0.47435585 0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.45360625 0.73380953 0.21175088 0.37445158 0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.43439358 0.8282951  0.21410266 0.35391885 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.36490032 0.7536389  0.24520284 0.26996014 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.43763882 0.75054586 0.24811831 0.33169642 0.42      ], Reward: 0.0\n",
            "Processed Log → State: [0.44236737 0.7992568  0.25192997 0.44307524 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.42727658 0.8885137  0.2587838  0.49368215 0.48      ], Reward: -1.0\n",
            "Processed Log → State: [0.529227   0.87470573 0.31316018 0.50744796 0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.5733354  0.8514944  0.30408123 0.42817336 0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.52038234 0.899079   0.30286282 0.38960657 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.5823862  0.96929383 0.29074568 0.4606624  0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.605265   0.96115637 0.27967724 0.55408764 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.59590065 0.9470581  0.25909367 0.5278311  0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.6219841  0.9266211  0.26680657 0.4886968  0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.7423071  0.87204313 0.24207167 0.50660235 0.72      ], Reward: 7.0\n",
            "Processed Log → State: [0.7499277  0.9376922  0.21271318 0.5502318  0.75      ], Reward: 0.0\n",
            "Processed Log → State: [0.75644344 1.         0.24272041 0.54895496 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.7551435  1.         0.28982437 0.50369483 0.81      ], Reward: 6.0\n",
            "Processed Log → State: [0.72543657 1.         0.2782515  0.4023679  0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.7909569  1.         0.3323138  0.42661765 0.87      ], Reward: -1.0\n",
            "Processed Log → State: [0.7759152  1.         0.33274463 0.43422553 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.18152325 0.27449122 0.06351778 0.6410544  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.17430906 0.28399262 0.04869441 0.6620674  0.06      ], Reward: 0.0\n",
            "Processed Log → State: [0.16038162 0.289209   0.06178696 0.6992488  0.09      ], Reward: 0.0\n",
            "Processed Log → State: [0.15201013 0.28887355 0.05416526 0.6752412  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.17457272 0.2518819  0.05292275 0.6732547  0.15      ], Reward: 0.0\n",
            "Processed Log → State: [0.18327548 0.28996503 0.06045727 0.6333259  0.18      ], Reward: 0.0\n",
            "Processed Log → State: [0.19807847 0.30933923 0.0700945  0.63804054 0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.21861967 0.3186901  0.07784616 0.68511075 0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.23916234 0.33286643 0.07751938 0.7393197  0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.2177004  0.36171588 0.0780355  0.74966264 0.3       ], Reward: 2.0\n",
            "Processed Log → State: [0.25281867 0.39789516 0.0903089  0.84829724 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.25702554 0.39440545 0.07030032 0.86778784 0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.3014844  0.44260076 0.06407323 0.81080693 0.39      ], Reward: 7.0\n",
            "Processed Log → State: [0.29045096 0.42681825 0.06703673 0.7947278  0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.29785547 0.42827594 0.05182111 0.8570678  0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.2595894  0.42866528 0.06889453 0.84677726 0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.2821916  0.44125533 0.06332254 0.9221299  0.51      ], Reward: 2.0\n",
            "Processed Log → State: [0.29122022 0.4576026  0.06500825 0.9266139  0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.31914753 0.48076904 0.06478213 0.95982605 0.57      ], Reward: 0.0\n",
            "Processed Log → State: [0.29521537 0.5125068  0.07580372 0.9047436  0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.2996951  0.5447992  0.08055344 0.92269635 0.63      ], Reward: 5.0\n",
            "Processed Log → State: [0.31754914 0.52048314 0.07946652 0.971955   0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.3313359  0.54050416 0.09398516 0.9899517  0.69      ], Reward: 4.0\n",
            "Processed Log → State: [0.30662468 0.5341953  0.09979739 0.9346044  0.72      ], Reward: 6.0\n",
            "Processed Log → State: [0.3289455  0.54696697 0.09373149 0.95829034 0.75      ], Reward: -1.0\n",
            "Processed Log → State: [0.36321148 0.5370756  0.0939602  0.95626897 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.37912574 0.5891204  0.1030376  0.99073786 0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.40999287 0.5893878  0.11226881 0.9352131  0.84      ], Reward: -1.0\n",
            "Processed Log → State: [0.423192   0.5773899  0.12487966 0.9097468  0.87      ], Reward: 5.0\n",
            "Processed Log → State: [0.42716473 0.579304   0.13694581 0.94048476 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.22009245 0.3070714  0.05673922 0.583178   0.03      ], Reward: 0.0\n",
            "Processed Log → State: [0.19511469 0.3227044  0.0552951  0.59566355 0.06      ], Reward: 7.0\n",
            "Processed Log → State: [0.16013123 0.33061382 0.06647597 0.57500917 0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.14537011 0.36855122 0.07157195 0.6125259  0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.14852285 0.40419823 0.06565755 0.5948939  0.15      ], Reward: -1.0\n",
            "Processed Log → State: [0.13407968 0.40735328 0.05878536 0.68088454 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.13552031 0.40258023 0.04650247 0.75852    0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.1411181  0.4134832  0.0474879  0.86442494 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.1576881  0.4564193  0.04465776 0.8716413  0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.14024706 0.42359075 0.0521394  0.8296699  0.3       ], Reward: 5.0\n",
            "Processed Log → State: [0.10503497 0.44444373 0.06008456 0.79705757 0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.12051351 0.44458133 0.05185882 0.80649966 0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.16585661 0.42632523 0.04533955 0.84121037 0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.16344742 0.45204195 0.06177352 0.86298096 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.19519195 0.46793404 0.07261448 0.8458934  0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.2096848  0.49216515 0.07479356 0.8681896  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.22319967 0.47328305 0.07441591 0.8942041  0.51      ], Reward: 2.0\n",
            "Processed Log → State: [0.2308184  0.45828047 0.07207308 0.92965066 0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.2432397  0.47252044 0.0689148  0.98376316 0.57      ], Reward: 0.0\n",
            "Processed Log → State: [0.26417944 0.46841723 0.07001974 0.97975343 0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.2957118  0.48559886 0.08756343 1.         0.63      ], Reward: 6.0\n",
            "Processed Log → State: [0.32812512 0.47720298 0.0872064  1.         0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.33839816 0.48859307 0.10294496 1.         0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.3557471  0.5269224  0.09701619 1.         0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.33717927 0.54325753 0.09485625 1.         0.75      ], Reward: -1.0\n",
            "Processed Log → State: [0.35035324 0.5535042  0.09687547 1.         0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.34376642 0.5549097  0.12742653 0.9728744  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.32061842 0.54639214 0.1475523  1.         0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.3504334  0.61373204 0.14381288 0.9659312  0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.35664165 0.6019972  0.14416763 0.9218393  0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.24557512 0.40034238 0.07607418 0.54153126 0.03      ], Reward: 7.0\n",
            "Processed Log → State: [0.32091185 0.51265866 0.1337925  0.53620136 0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.3799947 0.6345178 0.1738213 0.5695544 0.09     ], Reward: 8.0\n",
            "Processed Log → State: [0.347773   0.653033   0.20379603 0.63754225 0.12      ], Reward: 7.0\n",
            "Processed Log → State: [0.39375168 0.7008082  0.23341964 0.56714255 0.15      ], Reward: 5.0\n",
            "Processed Log → State: [0.40712142 0.79437655 0.2729946  0.5705391  0.18      ], Reward: 4.0\n",
            "Processed Log → State: [0.453298   0.78621024 0.29460356 0.4903333  0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.4816348  0.73572206 0.34725964 0.49560955 0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.51881623 0.7925586  0.34839907 0.5094427  0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.5054199  0.80381274 0.3896006  0.5809404  0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.5015024  0.8307017  0.43457088 0.6388463  0.33      ], Reward: 2.0\n",
            "Processed Log → State: [0.5570601 0.8982928 0.4734804 0.5724899 0.36     ], Reward: 1.0\n",
            "Processed Log → State: [0.5270632  0.95866877 0.52684236 0.51345277 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.64480925 0.980265   0.52414423 0.5363632  0.42      ], Reward: 5.0\n",
            "Processed Log → State: [0.6700584 1.        0.516531  0.5254253 0.45     ], Reward: 8.0\n",
            "Processed Log → State: [0.64485663 1.         0.5366491  0.5019683  0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.5857535  1.         0.50153047 0.49669528 0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.63488394 1.         0.461715   0.5524765  0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.64311635 0.97416854 0.49880782 0.5723363  0.57      ], Reward: 7.0\n",
            "Processed Log → State: [0.5414613  1.         0.49017555 0.5931223  0.6       ], Reward: 0.0\n",
            "Processed Log → State: [0.53384966 0.9938179  0.45567575 0.5702627  0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.4475814  1.         0.47555542 0.5824664  0.66      ], Reward: 0.0\n",
            "Processed Log → State: [0.5199355 1.        0.4587431 0.6154726 0.69     ], Reward: 3.0\n",
            "Processed Log → State: [0.5129878  1.         0.43418244 0.5938083  0.72      ], Reward: 0.0\n",
            "Processed Log → State: [0.55350775 1.         0.4504004  0.63483    0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.6296586  0.9617372  0.46780974 0.5407803  0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.660748   0.9894338  0.50902224 0.55025434 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.70417863 0.98016477 0.50859404 0.5075486  0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.6908298  0.96045434 0.46229857 0.52625483 0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.76961684 0.9478793  0.45019332 0.51296425 0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.21845967 0.34199718 0.04331308 0.5238644  0.03      ], Reward: 0.0\n",
            "Processed Log → State: [0.28812584 0.4444052  0.07741216 0.46725476 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.32088703 0.54257625 0.11584889 0.35401714 0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.31373677 0.5852185  0.16800494 0.3473785  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.3123224  0.57755685 0.18338358 0.3324766  0.15      ], Reward: 6.0\n",
            "Processed Log → State: [0.3074657  0.5768678  0.18749751 0.3123374  0.18      ], Reward: 0.0\n",
            "Processed Log → State: [0.2733589  0.55734074 0.19113404 0.28283018 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.28399915 0.63410306 0.21761581 0.32738066 0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.24621224 0.68025    0.30210215 0.31734282 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.20168509 0.73837215 0.27384496 0.37535468 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.21959251 0.6959316  0.24825606 0.4079492  0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.20020367 0.7785754  0.31863347 0.35783607 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.27344498 0.8109555  0.33757672 0.3305155  0.39      ], Reward: 0.0\n",
            "Processed Log → State: [0.23387688 0.76115704 0.36141726 0.26115978 0.42      ], Reward: 7.0\n",
            "Processed Log → State: [0.2830291  0.80890644 0.3212962  0.26476374 0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.35178182 0.8697979  0.33016768 0.2903564  0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.3703896  0.9225179  0.37118617 0.24764669 0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.3920608  0.91944    0.4078588  0.27763134 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.4364316  0.97876567 0.35919988 0.29115444 0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.3629858  1.         0.35003078 0.1941203  0.6       ], Reward: 7.0\n",
            "Processed Log → State: [0.4444311  1.         0.38449308 0.227033   0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.5190235  0.9993619  0.40254053 0.27977103 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.5662906  1.         0.36445102 0.2915013  0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.57753444 0.9896621  0.3531221  0.3081048  0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.55006796 0.96183205 0.3990327  0.25582764 0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.5649447  0.969205   0.3567622  0.30181783 0.78      ], Reward: 0.0\n",
            "Processed Log → State: [0.5004626 1.        0.3225498 0.364844  0.81     ], Reward: 8.0\n",
            "Processed Log → State: [0.5813917  1.         0.34412137 0.31238452 0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.51049745 0.97405404 0.37691498 0.37275013 0.87      ], Reward: 2.0\n",
            "Processed Log → State: [0.464113   0.9747109  0.38750827 0.42294988 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.23579307 0.32724547 0.06088432 0.5851126  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.2698777  0.37884882 0.08647734 0.67028064 0.06      ], Reward: 5.0\n",
            "Processed Log → State: [0.3081036  0.3814533  0.07284624 0.62530166 0.09      ], Reward: 2.0\n",
            "Processed Log → State: [0.33848444 0.39303032 0.08489457 0.6788235  0.12      ], Reward: 0.0\n",
            "Processed Log → State: [0.34495187 0.41426593 0.09662823 0.70172286 0.15      ], Reward: 5.0\n",
            "Processed Log → State: [0.36621    0.41297704 0.1151344  0.7569503  0.18      ], Reward: 0.0\n",
            "Processed Log → State: [0.36921155 0.39918387 0.1279097  0.7874483  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.39940786 0.40513712 0.13754566 0.82918584 0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.40171632 0.42630213 0.15741833 0.8485034  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.4281199  0.42177927 0.15641154 0.8765749  0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.43569988 0.44266906 0.15704025 0.9131477  0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.47699022 0.49943492 0.15173352 0.97508323 0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.49210122 0.4916639  0.17161068 0.98408955 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.50815296 0.53045464 0.16995355 0.96182793 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.5457545  0.56545275 0.19542518 0.9554218  0.45      ], Reward: 0.0\n",
            "Processed Log → State: [0.5392573  0.5782338  0.20874889 0.9655438  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.5784867  0.592667   0.20433693 0.9292113  0.51      ], Reward: -1.0\n",
            "Processed Log → State: [0.57174283 0.60744834 0.21227223 0.9813121  0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.5697617  0.6086379  0.23243344 1.         0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.56760204 0.6107519  0.22549033 1.         0.6       ], Reward: 0.0\n",
            "Processed Log → State: [0.6111174  0.621583   0.22928995 0.9766527  0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.6046803  0.642443   0.24221559 1.         0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.61501956 0.6258769  0.22796619 1.         0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.654299   0.6096617  0.21914224 1.         0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.693205   0.61554545 0.2170536  1.         0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.6832201  0.6132994  0.20437814 1.         0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.6449519  0.647264   0.22758956 0.9747093  0.81      ], Reward: 2.0\n",
            "Processed Log → State: [0.6469665  0.67093796 0.24703753 1.         0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.68581223 0.69677377 0.25808346 0.9777438  0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.69433373 0.7009693  0.2917488  0.94714254 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.15957922 0.35096088 0.05194673 0.5249694  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.26420063 0.39288396 0.04153268 0.37644082 0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.23325412 0.47940102 0.10605004 0.2998356  0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.29974777 0.46334094 0.09372444 0.29441527 0.12      ], Reward: 0.0\n",
            "Processed Log → State: [0.25532502 0.5267771  0.11887617 0.26260173 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.3070476  0.6321073  0.11501376 0.28019956 0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.32723182 0.6503147  0.13013501 0.24273643 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.3848498  0.59737915 0.1644576  0.27225152 0.24      ], Reward: -1.0\n",
            "Processed Log → State: [0.4684271  0.6070937  0.19493486 0.29484248 0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.53032726 0.629281   0.2160163  0.20948192 0.3       ], Reward: 0.0\n",
            "Processed Log → State: [0.50997293 0.6416447  0.19564189 0.14713626 0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.52250385 0.70652544 0.2540449  0.20436463 0.36      ], Reward: 5.0\n",
            "Processed Log → State: [0.63403994 0.73097163 0.30320764 0.1752493  0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.72303563 0.8788383  0.3203709  0.17305113 0.42      ], Reward: 7.0\n",
            "Processed Log → State: [0.7024404  1.         0.3451846  0.20628421 0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.6446342  0.9738945  0.36739835 0.1692762  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.69120103 0.9910372  0.4156732  0.13250315 0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.75018084 1.         0.41602543 0.1649553  0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.8311119  1.         0.3740948  0.22742431 0.57      ], Reward: 7.0\n",
            "Processed Log → State: [0.7581193  0.95187855 0.40545925 0.26528823 0.6       ], Reward: 0.0\n",
            "Processed Log → State: [0.70763737 0.9757726  0.4623101  0.25156403 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.75607777 0.9804874  0.4310957  0.26534045 0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.83009833 0.9909513  0.3872316  0.29548383 0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.78064656 0.9769787  0.39786813 0.28452933 0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.80741733 1.         0.40473217 0.3349238  0.75      ], Reward: 7.0\n",
            "Processed Log → State: [0.83471495 0.9964117  0.34704095 0.37649658 0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.79664874 0.9538693  0.3845099  0.42216185 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.8857641  1.         0.4347672  0.40373144 0.84      ], Reward: 7.0\n",
            "Processed Log → State: [0.8713944  0.9780906  0.39412257 0.44314006 0.87      ], Reward: 2.0\n",
            "Processed Log → State: [0.833883   1.         0.40611213 0.46138915 0.9       ], Reward: 7.0\n",
            "Processed Log → State: [0.1958419  0.26795825 0.06259919 0.6411809  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.19895634 0.27049062 0.05804723 0.68939096 0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.1902104  0.28413272 0.07891144 0.6901424  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.19609727 0.29568455 0.0581041  0.6333297  0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.2370657  0.3049667  0.05733305 0.67646027 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.23939182 0.3351761  0.04989351 0.6243829  0.18      ], Reward: 2.0\n",
            "Processed Log → State: [0.2578276  0.3926218  0.06412231 0.70469326 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.23816088 0.42958677 0.05189774 0.6691061  0.24      ], Reward: 0.0\n",
            "Processed Log → State: [0.27552047 0.4064691  0.03824517 0.645041   0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.29237112 0.4331843  0.03089545 0.66998327 0.3       ], Reward: 2.0\n",
            "Processed Log → State: [0.31034094 0.41733286 0.0099563  0.7045794  0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.35567242 0.41263008 0.02481227 0.7016224  0.36      ], Reward: 5.0\n",
            "Processed Log → State: [0.3862612  0.41525567 0.01396879 0.6983919  0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.3936254  0.42242917 0.02888008 0.8326604  0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.37102634 0.44535777 0.02594593 0.90111536 0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.3774536  0.4192254  0.02425897 0.9416702  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.38968185 0.44865313 0.0152267  0.900287   0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.37267002 0.48424056 0.0025066  1.         0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.38580313 0.45700318 0.         1.         0.57      ], Reward: 5.0\n",
            "Processed Log → State: [0.39591587 0.4825607  0.02244132 1.         0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.40413305 0.50758344 0.04131974 0.98857903 0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.41282973 0.5229538  0.0521952  0.9585216  0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.42014438 0.50060856 0.0683722  1.         0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.40811044 0.5255697  0.07323838 1.         0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.4619444  0.5398171  0.08351289 1.         0.75      ], Reward: 7.0\n",
            "Processed Log → State: [0.45525992 0.53616995 0.06850366 0.98981804 0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.4686662  0.52127004 0.10246234 0.9506069  0.81      ], Reward: 6.0\n",
            "Processed Log → State: [0.4840331  0.55275124 0.12043353 0.9981355  0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.4941578  0.5571215  0.13405539 0.96238065 0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.5058088  0.55837035 0.13838181 0.9041326  0.9       ], Reward: 6.0\n",
            "Processed Log → State: [0.26675168 0.34835836 0.07133095 0.6898353  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.31720623 0.4646691  0.11782592 0.6515431  0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.40498784 0.51611793 0.16768828 0.6779969  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.4985588  0.5527408  0.15769471 0.7602458  0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.5567082  0.5479903  0.15368064 0.7849097  0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.6249952  0.66881937 0.17297526 0.64782166 0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.67290264 0.7897331  0.15861383 0.6315063  0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.7072159  0.7979708  0.09846635 0.6850507  0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.70710224 0.8517571  0.10106486 0.67069393 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.72701216 0.9360167  0.12486442 0.68427706 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.7816172  0.9429518  0.13796555 0.70389885 0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.8613463  1.         0.17974855 0.7054606  0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.867542   1.         0.2112406  0.63360316 0.39      ], Reward: 4.0\n",
            "Processed Log → State: [0.8429489  1.         0.19288969 0.63997895 0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.9262138  0.95366615 0.22197616 0.6165352  0.45      ], Reward: 2.0\n",
            "Processed Log → State: [1.         0.96516865 0.20512153 0.62278783 0.48      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.9773308  0.25684002 0.62182903 0.51      ], Reward: 4.0\n",
            "Processed Log → State: [0.9472957  0.96843636 0.29787886 0.5983074  0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.955751  1.        0.2834865 0.5938803 0.57     ], Reward: 8.0\n",
            "Processed Log → State: [0.9730365 0.9861027 0.3110266 0.5831506 0.6      ], Reward: 2.0\n",
            "Processed Log → State: [0.9020568  1.         0.32737672 0.5296289  0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.90680283 0.9694529  0.3514153  0.46752894 0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.91895163 0.90775985 0.3429519  0.46484205 0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.995295   0.95258164 0.3356708  0.5685528  0.72      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.92900836 0.27942538 0.47956    0.75      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.87798035 0.30653334 0.42746046 0.78      ], Reward: 6.0\n",
            "Processed Log → State: [0.9566893  0.9420874  0.33003667 0.3966217  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.94573104 0.9884661  0.38049582 0.4003396  0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.8888774  0.97063357 0.35926977 0.5101667  0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.89220166 0.84751606 0.3709374  0.46362904 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.28002232 0.37392017 0.10862092 0.5763889  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.30294123 0.4873181  0.10815589 0.5594346  0.06      ], Reward: 7.0\n",
            "Processed Log → State: [0.3297024  0.4681654  0.09551702 0.5011826  0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.33047608 0.5394293  0.10519942 0.53013104 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.31636962 0.61140007 0.17882812 0.5294079  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.2831679  0.60504097 0.18165797 0.4239966  0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.31023505 0.7217621  0.23561524 0.40446943 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.33159283 0.74130535 0.26137292 0.40214822 0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.33052403 0.8133744  0.2592929  0.31716764 0.27      ], Reward: 7.0\n",
            "Processed Log → State: [0.35642797 0.87493455 0.29538995 0.34164914 0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.4928199  0.93434584 0.3084198  0.3553841  0.33      ], Reward: 7.0\n",
            "Processed Log → State: [0.52964675 1.         0.36202395 0.39467418 0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.5688312  1.         0.40723908 0.32864797 0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.6095563  1.         0.47314024 0.36768496 0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.677338   1.         0.40621734 0.37280753 0.45      ], Reward: 6.0\n",
            "Processed Log → State: [0.61606055 0.9928332  0.44155365 0.4093535  0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.59333515 1.         0.4193886  0.38122627 0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.582085   0.9729145  0.39450184 0.44526702 0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.6426832  0.9652154  0.41839138 0.44659364 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.6496321  1.         0.44539782 0.4297176  0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.6484293  0.97578853 0.44059223 0.4827834  0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.6649282  1.         0.42747298 0.46846098 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.6921071 1.        0.4003946 0.4511846 0.69     ], Reward: 6.0\n",
            "Processed Log → State: [0.6968209  0.984513   0.3942534  0.45174944 0.72      ], Reward: 7.0\n",
            "Processed Log → State: [0.75462985 1.         0.38598835 0.48787826 0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.8000929  1.         0.395819   0.52568185 0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.8063735  0.9830517  0.37578842 0.48217648 0.81      ], Reward: 7.0\n",
            "Processed Log → State: [0.82039374 0.97905046 0.35453323 0.49410832 0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.7550986  0.9100644  0.39692032 0.4986399  0.87      ], Reward: 0.0\n",
            "Processed Log → State: [0.6844968  0.9701233  0.3997311  0.45035583 0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.17585775 0.3580076  0.13162926 0.5811776  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.17665777 0.39321265 0.21765018 0.6128265  0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.23038228 0.46246138 0.25666934 0.58396745 0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.22021748 0.5118742  0.33533415 0.50287133 0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.33650503 0.5429325  0.31309733 0.46614146 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.31859252 0.61423707 0.2924548  0.5191492  0.18      ], Reward: -2.0\n",
            "Processed Log → State: [0.35569605 0.67822224 0.3191829  0.6052079  0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.2873201  0.7301495  0.29047155 0.50556827 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.29568508 0.7643678  0.34294045 0.47945914 0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.3744827  0.79993397 0.39403582 0.5361931  0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.43293846 0.8273815  0.45002583 0.5910964  0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.47844607 0.8916777  0.55335623 0.57739574 0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.55600864 0.88902795 0.6663912  0.53475064 0.39      ], Reward: 6.0\n",
            "Processed Log → State: [0.57224786 0.93219006 0.66543454 0.51421016 0.42      ], Reward: 0.0\n",
            "Processed Log → State: [0.5250971  0.9102222  0.65878415 0.40206763 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.5759861  0.9164995  0.67953664 0.45256227 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.584123   0.92616814 0.6260743  0.47315216 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.5604034  0.91369504 0.61369556 0.53074926 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.6112907  0.9764     0.63761723 0.54293233 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.6321666  0.95241445 0.65164804 0.517466   0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.6661785  0.94645727 0.630098   0.40182146 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.7007753  0.9305383  0.616067   0.34279618 0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.78298175 0.9208858  0.66110045 0.3555369  0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.84854805 0.9235853  0.7114484  0.36207166 0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.8625394  0.8796265  0.7196944  0.31511116 0.75      ], Reward: -1.0\n",
            "Processed Log → State: [0.8648955  0.919994   0.70356643 0.31434065 0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.8743759  0.96231014 0.8000477  0.30209973 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.91664135 0.9531848  0.8676375  0.37427577 0.84      ], Reward: 0.0\n",
            "Processed Log → State: [0.9337752  0.9537233  0.8335592  0.39440832 0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.956319   0.9019573  0.773357   0.37495285 0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.23034717 0.28571448 0.05497802 0.5865793  0.03      ], Reward: 0.0\n",
            "Processed Log → State: [0.20767719 0.30649477 0.05118537 0.62080896 0.06      ], Reward: 0.0\n",
            "Processed Log → State: [0.27647012 0.3284584  0.04876955 0.6741303  0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.3307813  0.3208815  0.05925082 0.6606114  0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.36753926 0.35492155 0.06065727 0.6996537  0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.37513825 0.39832523 0.06326713 0.7261565  0.18      ], Reward: 2.0\n",
            "Processed Log → State: [0.39001974 0.43339816 0.06720307 0.73011214 0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.4019715  0.45191157 0.0598045  0.739447   0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.3668653  0.44535023 0.06596544 0.805664   0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.37364298 0.46660596 0.06710958 0.879641   0.3       ], Reward: 7.0\n",
            "Processed Log → State: [0.35778856 0.48570496 0.06032128 0.89219546 0.33      ], Reward: 5.0\n",
            "Processed Log → State: [0.36414623 0.52167106 0.05899039 0.86430985 0.36      ], Reward: 7.0\n",
            "Processed Log → State: [0.37670022 0.53829557 0.05919186 0.7980593  0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.41023055 0.51711994 0.05888011 0.7602517  0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.3806781  0.5111469  0.04948009 0.7295678  0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.3722704  0.5291163  0.06346252 0.74850154 0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.33687562 0.5371821  0.06037602 0.7548094  0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.36270902 0.5509888  0.05492772 0.8008661  0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.39002958 0.5256492  0.06787875 0.7923944  0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.4064124  0.5107321  0.07387484 0.8989583  0.6       ], Reward: 5.0\n",
            "Processed Log → State: [0.4242745  0.52576476 0.06753608 0.85774827 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.40745622 0.5082293  0.07899734 0.95283604 0.66      ], Reward: 5.0\n",
            "Processed Log → State: [0.4277487  0.5153929  0.08353864 0.9388536  0.69      ], Reward: 0.0\n",
            "Processed Log → State: [0.44892344 0.549181   0.08078784 1.         0.72      ], Reward: 5.0\n",
            "Processed Log → State: [0.48926398 0.54382575 0.07491268 0.9737494  0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.49386424 0.5411196  0.05784464 0.99334764 0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.4882767  0.5395258  0.08663908 1.         0.81      ], Reward: 2.0\n",
            "Processed Log → State: [0.4687652  0.58745307 0.09020057 1.         0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.47331488 0.6062426  0.1078863  1.         0.87      ], Reward: 7.0\n",
            "Processed Log → State: [0.45448926 0.63944674 0.13024624 0.9531457  0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.20509936 0.38796842 0.10406668 0.58154505 0.03      ], Reward: 2.0\n",
            "Processed Log → State: [0.29067922 0.40188563 0.2493403  0.55764544 0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.2578525  0.49601093 0.37395406 0.48614573 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.23606618 0.5490441  0.4431172  0.50407994 0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.26806107 0.5318966  0.41648772 0.4032697  0.15      ], Reward: 7.0\n",
            "Processed Log → State: [0.3349904  0.56764835 0.44516847 0.43293595 0.18      ], Reward: 7.0\n",
            "Processed Log → State: [0.35725993 0.59560513 0.40094668 0.38286838 0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.408258   0.6279714  0.4478368  0.34439272 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.4756992  0.61788    0.5470887  0.37645215 0.27      ], Reward: 4.0\n",
            "Processed Log → State: [0.43379083 0.64224756 0.56152207 0.42632198 0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.52670544 0.64951324 0.52898324 0.32518825 0.33      ], Reward: 5.0\n",
            "Processed Log → State: [0.5977046  0.70656365 0.5539678  0.26411873 0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.66365236 0.76640654 0.6887366  0.24034064 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.77742094 0.7870562  0.75725776 0.27647775 0.42      ], Reward: 4.0\n",
            "Processed Log → State: [0.7828759  0.79994667 0.760874   0.24876902 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.81744546 0.83089334 0.74447966 0.20653646 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.8851758  0.85273015 0.7720734  0.21668279 0.51      ], Reward: 2.0\n",
            "Processed Log → State: [0.8035871  0.82744044 0.7937424  0.16608351 0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.95380354 0.90374637 0.7638158  0.20022447 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.9807783  0.88678366 0.74343836 0.18648674 0.6       ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.8979114  0.7329119  0.16019534 0.63      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.92042667 0.6840386  0.18590587 0.66      ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.928133   0.73113674 0.20193331 0.69      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.9188522  0.7302984  0.04754261 0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.9224045  0.90224934 0.7076185  0.11436423 0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.98450243 0.9024228  0.70348394 0.10823547 0.78      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.92036915 0.73227406 0.12822409 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.9732336  0.92564267 0.7549068  0.15202354 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.90412736 0.7654678  0.18790382 0.87      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.89966327 0.79840696 0.28177613 0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.21610092 0.32776877 0.05236137 0.6182531  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.26484278 0.35523784 0.07911441 0.71838856 0.06      ], Reward: 7.0\n",
            "Processed Log → State: [0.2755403  0.35916814 0.08844328 0.68323624 0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.2755527  0.39749718 0.06740389 0.7263581  0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.2973892  0.4557585  0.07058692 0.7282803  0.15      ], Reward: 7.0\n",
            "Processed Log → State: [0.292858   0.4955476  0.07792807 0.7299655  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.28979546 0.49863365 0.08262898 0.7838371  0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.2870713  0.5633197  0.11120671 0.8401786  0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.33982554 0.594439   0.11609352 0.89336735 0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.38617688 0.6090245  0.13403706 0.93927413 0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.40435362 0.6452966  0.14991283 0.8985945  0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.40457347 0.66830355 0.15831271 0.93765    0.36      ], Reward: 5.0\n",
            "Processed Log → State: [0.425726   0.6742361  0.16637222 0.9104372  0.39      ], Reward: 5.0\n",
            "Processed Log → State: [0.41983545 0.6342205  0.17616671 0.894687   0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.43842825 0.64871675 0.18272707 0.8944573  0.45      ], Reward: 0.0\n",
            "Processed Log → State: [0.43386087 0.6572911  0.20206594 0.98582566 0.48      ], Reward: 7.0\n",
            "Processed Log → State: [0.49047387 0.6821595  0.20516656 1.         0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.46029013 0.68803805 0.21254611 1.         0.54      ], Reward: 4.0\n",
            "Processed Log → State: [0.49434644 0.702614   0.19825274 1.         0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.52268267 0.73122656 0.19360326 0.96202385 0.6       ], Reward: 7.0\n",
            "Processed Log → State: [0.51135683 0.71891075 0.19908975 0.9052039  0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.5051027  0.7387378  0.22118227 0.86089414 0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.51162183 0.7525333  0.23249294 0.98518246 0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.504682   0.7201606  0.20541568 0.9906982  0.72      ], Reward: 0.0\n",
            "Processed Log → State: [0.48920175 0.71904606 0.19862755 0.9970973  0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.48848927 0.7200599  0.20611924 1.         0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.52515    0.7240853  0.21355027 0.9749178  0.81      ], Reward: 6.0\n",
            "Processed Log → State: [0.5419302  0.73205984 0.23935802 0.9525744  0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.5812871  0.72930294 0.25679278 0.9615135  0.87      ], Reward: 7.0\n",
            "Processed Log → State: [0.5796788 0.7095118 0.2787027 0.9429415 0.9      ], Reward: 6.0\n",
            "Processed Log → State: [0.1921277  0.38122004 0.05074731 0.49476424 0.03      ], Reward: 7.0\n",
            "Processed Log → State: [0.2396841  0.3719508  0.05101788 0.4231167  0.06      ], Reward: 5.0\n",
            "Processed Log → State: [0.29641983 0.43823043 0.05720155 0.3377021  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.3551586  0.43054646 0.08245002 0.369301   0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.4442299  0.468846   0.12725143 0.31919545 0.15      ], Reward: 0.0\n",
            "Processed Log → State: [0.4791185  0.40967697 0.15872452 0.3415597  0.18      ], Reward: 0.0\n",
            "Processed Log → State: [0.52030885 0.504162   0.18187225 0.43801922 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.53709644 0.49624786 0.18223946 0.44386905 0.24      ], Reward: 5.0\n",
            "Processed Log → State: [0.55902433 0.46253616 0.18628705 0.49072155 0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.6569358  0.46154788 0.17810938 0.51564014 0.3       ], Reward: 5.0\n",
            "Processed Log → State: [0.68738914 0.5339106  0.22800648 0.47913018 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.7218115  0.68651253 0.24781121 0.47426066 0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.8085265  0.70295763 0.2619706  0.5000824  0.39      ], Reward: 0.0\n",
            "Processed Log → State: [0.84095395 0.76868224 0.24007002 0.5174529  0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.86290914 0.77393705 0.25586313 0.57223904 0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.94370747 0.80334276 0.27551025 0.576511   0.48      ], Reward: 2.0\n",
            "Processed Log → State: [0.9433246  0.77341264 0.3442211  0.65746874 0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.9547212 0.7900573 0.3593081 0.6266523 0.54     ], Reward: 8.0\n",
            "Processed Log → State: [0.9820997  0.8062676  0.32009155 0.6471891  0.57      ], Reward: 3.0\n",
            "Processed Log → State: [1.        0.8261183 0.3358633 0.6441217 0.6      ], Reward: 2.0\n",
            "Processed Log → State: [1.         0.8597641  0.34748277 0.6554829  0.63      ], Reward: 2.0\n",
            "Processed Log → State: [1.         0.8337908  0.34235084 0.63939327 0.66      ], Reward: 0.0\n",
            "Processed Log → State: [0.94939435 0.92844224 0.3538838  0.5708505  0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.93440807 0.95476043 0.35807145 0.60527307 0.72      ], Reward: 7.0\n",
            "Processed Log → State: [0.9971606  0.96255744 0.3827737  0.59408516 0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.9749974  0.9814987  0.38128692 0.5539076  0.78      ], Reward: 7.0\n",
            "Processed Log → State: [1.         1.         0.45457193 0.6585682  0.81      ], Reward: 5.0\n",
            "Processed Log → State: [0.9309583  1.         0.5102049  0.62578624 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.93579453 0.9852013  0.5308646  0.66782206 0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.8317544 1.        0.5441636 0.681864  0.9      ], Reward: 0.0\n",
            "Processed Log → State: [0.21502028 0.31124532 0.04165023 0.608246   0.03      ], Reward: -1.0\n",
            "Processed Log → State: [0.26039162 0.3325113  0.05371212 0.6273534  0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.2948508  0.3463975  0.05566221 0.53547937 0.09      ], Reward: 0.0\n",
            "Processed Log → State: [0.29211515 0.34617072 0.06131786 0.5381683  0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.31528527 0.33006257 0.05419422 0.4893198  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.3424857  0.3300538  0.05416135 0.5231756  0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.32956362 0.33928397 0.04752929 0.5527474  0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.33003128 0.37669444 0.02347264 0.6074856  0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.32317197 0.39064842 0.0120971  0.60792834 0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.32864517 0.41934073 0.0140963  0.5906226  0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.31934986 0.47848415 0.01902546 0.64833647 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.3347282  0.47830778 0.02412567 0.7745123  0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.34096023 0.4729552  0.01773126 0.7670203  0.39      ], Reward: 0.0\n",
            "Processed Log → State: [0.3492841  0.51780397 0.02540464 0.8877913  0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.3804087  0.5855866  0.03042188 0.90081173 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.3843241  0.62251234 0.02855947 0.9192029  0.48      ], Reward: 6.0\n",
            "Processed Log → State: [0.4406032  0.6107622  0.04241477 0.9438153  0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.43935773 0.59417385 0.05174689 0.9468791  0.54      ], Reward: 7.0\n",
            "Processed Log → State: [0.4710974  0.5892145  0.04083816 0.97119087 0.57      ], Reward: 2.0\n",
            "Processed Log → State: [0.4666284  0.6130104  0.05374733 1.         0.6       ], Reward: 0.0\n",
            "Processed Log → State: [0.49040705 0.625479   0.07545205 0.9739463  0.63      ], Reward: 7.0\n",
            "Processed Log → State: [0.5280493  0.6401218  0.0798322  0.87681305 0.66      ], Reward: 5.0\n",
            "Processed Log → State: [0.49710193 0.6310289  0.08513566 0.84593284 0.69      ], Reward: 0.0\n",
            "Processed Log → State: [0.5142498  0.6330582  0.07978595 0.8604069  0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.51685    0.61605483 0.08645618 0.90408486 0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.49426037 0.61222583 0.08489487 0.95343655 0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.5069429  0.5451944  0.09826468 0.91418606 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.5156169  0.5321565  0.1079011  0.85528296 0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.55039483 0.5550738  0.13405481 0.86359686 0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.5373393  0.5408967  0.15319636 0.85174966 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.23777501 0.27408522 0.08376689 0.5554333  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.20094508 0.3039007  0.10989036 0.48353803 0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.2181189  0.37105772 0.1438636  0.43395945 0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.2959425  0.45118397 0.1995302  0.44858265 0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.41078946 0.39725706 0.21306649 0.3861863  0.15      ], Reward: 7.0\n",
            "Processed Log → State: [0.51806945 0.46335953 0.2462609  0.3794655  0.18      ], Reward: 5.0\n",
            "Processed Log → State: [0.5806063  0.48886025 0.21304637 0.41413265 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.6800431  0.52857465 0.23232654 0.45643902 0.24      ], Reward: 7.0\n",
            "Processed Log → State: [0.75686604 0.62496895 0.26557925 0.4882273  0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.76392615 0.6440747  0.28580832 0.5391641  0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.78561956 0.64103156 0.32433733 0.55631554 0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.7674444  0.68972206 0.3375948  0.61609304 0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.7838448  0.7481321  0.34914127 0.5531621  0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.7693993  0.8185791  0.356677   0.60692316 0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.84079105 0.7618134  0.36497235 0.60435945 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.8292695  0.78601646 0.35134587 0.6111558  0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.8587873  0.8687645  0.3792958  0.68096143 0.51      ], Reward: 5.0\n",
            "Processed Log → State: [0.9474656  0.966584   0.38734618 0.61389077 0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.9967942  0.9840379  0.38090602 0.61131614 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.9957093  0.9976487  0.35767284 0.5483917  0.6       ], Reward: 1.0\n",
            "Processed Log → State: [1.         1.         0.38330382 0.45757705 0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.9759941  0.9775872  0.39972556 0.48856223 0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.9559276  1.         0.39871237 0.43004638 0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.909604   0.97700924 0.37090912 0.43329567 0.72      ], Reward: 6.0\n",
            "Processed Log → State: [0.83511513 0.93808776 0.3487031  0.41598704 0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.8643551  0.9512686  0.38488185 0.43892208 0.78      ], Reward: 7.0\n",
            "Processed Log → State: [0.83666706 0.9216807  0.4342992  0.49405456 0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.8985803  0.9276071  0.4335073  0.50381404 0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.94591236 0.9281673  0.45638186 0.43198055 0.87      ], Reward: 5.0\n",
            "Processed Log → State: [0.984386   0.93014115 0.40305558 0.403102   0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.26413164 0.29399875 0.07351603 0.6021372  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.27079472 0.39805564 0.14272925 0.6748516  0.06      ], Reward: 0.0\n",
            "Processed Log → State: [0.30097678 0.48917747 0.1679194  0.67290103 0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.27785495 0.5765115  0.20524397 0.6649061  0.12      ], Reward: -1.0\n",
            "Processed Log → State: [0.37028033 0.5248321  0.25776356 0.6326555  0.15      ], Reward: 0.0\n",
            "Processed Log → State: [0.42113745 0.47309643 0.3044014  0.6654835  0.18      ], Reward: 6.0\n",
            "Processed Log → State: [0.45663708 0.5590033  0.31595197 0.6803602  0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.49805892 0.5335607  0.3486028  0.65632313 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.6514326  0.6265236  0.35847571 0.65405625 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.6812422 0.6905057 0.3276113 0.5781739 0.3      ], Reward: 3.0\n",
            "Processed Log → State: [0.8344274  0.6082417  0.35165226 0.523749   0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.8154431  0.6547327  0.40570447 0.5228326  0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.8554797  0.59940064 0.44206348 0.4633987  0.39      ], Reward: 7.0\n",
            "Processed Log → State: [0.9370333  0.6758008  0.4607692  0.46771702 0.42      ], Reward: 0.0\n",
            "Processed Log → State: [0.96491504 0.6236703  0.47861877 0.48910016 0.45      ], Reward: 5.0\n",
            "Processed Log → State: [1.        0.7260952 0.5037526 0.5214032 0.48     ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.6868371  0.49820492 0.53604925 0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.99157596 0.74974257 0.47356173 0.56581235 0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.9284318 0.7735519 0.4496267 0.555877  0.57     ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.80429804 0.40951395 0.5161574  0.6       ], Reward: 5.0\n",
            "Processed Log → State: [1.         0.852491   0.39375564 0.60461086 0.63      ], Reward: 2.0\n",
            "Processed Log → State: [0.9639741  0.9106945  0.39536703 0.5718357  0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.91671246 1.         0.37542933 0.70371073 0.69      ], Reward: 7.0\n",
            "Processed Log → State: [0.79614085 1.         0.43506065 0.7978137  0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.8597954  1.         0.40828797 0.76792413 0.75      ], Reward: 6.0\n",
            "Processed Log → State: [0.8401438  1.         0.38038224 0.76127493 0.78      ], Reward: 0.0\n",
            "Processed Log → State: [0.8608907 0.9770017 0.4106011 0.7884742 0.81     ], Reward: 8.0\n",
            "Processed Log → State: [0.91750515 0.9623046  0.42083773 0.77199197 0.84      ], Reward: -2.0\n",
            "Processed Log → State: [0.8332174  0.97087497 0.3997063  0.7732766  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.8042     0.92413896 0.41615573 0.76091975 0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.2958512  0.3668156  0.03249015 0.5436583  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.30853364 0.42032972 0.06352179 0.50753963 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.3321371  0.4350498  0.12393115 0.5123237  0.09      ], Reward: 2.0\n",
            "Processed Log → State: [0.3701482  0.45145878 0.16441634 0.41783965 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.41414142 0.48840785 0.20226628 0.45865536 0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.51127285 0.5248026  0.20610684 0.43082216 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.5859558  0.54739517 0.2293818  0.46988648 0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.6372214  0.5514792  0.27120167 0.52387583 0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.7626192  0.5822616  0.35652617 0.40099895 0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.8199753  0.59304523 0.35374466 0.3447671  0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.8662636  0.62043005 0.3490825  0.35155362 0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.87910587 0.6465145  0.3911925  0.3311487  0.36      ], Reward: 7.0\n",
            "Processed Log → State: [0.99219996 0.6760306  0.39205086 0.34600496 0.39      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.6858365  0.48406819 0.2703313  0.42      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.69589907 0.5498363  0.23268792 0.45      ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.6715159  0.47779256 0.17932925 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.99995244 0.69986516 0.51825124 0.17364542 0.51      ], Reward: 7.0\n",
            "Processed Log → State: [0.9947089  0.70519316 0.5407816  0.1582558  0.54      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.77372634 0.53683853 0.15993147 0.57      ], Reward: 7.0\n",
            "Processed Log → State: [0.982167   0.836012   0.56148255 0.19288261 0.6       ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.8104664  0.6068953  0.19822215 0.63      ], Reward: -2.0\n",
            "Processed Log → State: [0.91653883 0.8018726  0.5842129  0.2716021  0.66      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.78122944 0.6056874  0.29645914 0.69      ], Reward: 0.0\n",
            "Processed Log → State: [0.94743943 0.7778024  0.66060704 0.3034666  0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.9891755  0.8000517  0.639209   0.21183154 0.75      ], Reward: -1.0\n",
            "Processed Log → State: [1.         0.7826318  0.6080723  0.18840241 0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.9143494  0.73591185 0.6780954  0.2626881  0.81      ], Reward: 7.0\n",
            "Processed Log → State: [0.90983605 0.73766357 0.7077493  0.3008082  0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.86481917 0.7688213  0.7214305  0.3696522  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.86149335 0.7924788  0.78004867 0.41051155 0.9       ], Reward: 0.0\n",
            "Processed Log → State: [0.20214622 0.2898164  0.05517679 0.71220917 0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.2285781  0.27250028 0.06714148 0.7569153  0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.25025186 0.28186285 0.07310901 0.77588695 0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.23520683 0.27152398 0.07930923 0.69834405 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.22871773 0.27632368 0.07639929 0.78846264 0.15      ], Reward: 6.0\n",
            "Processed Log → State: [0.24569479 0.28734544 0.07362989 0.909346   0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.30158693 0.2817937  0.06660395 0.9829757  0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.31966454 0.28402087 0.06495734 1.         0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.31158906 0.29184502 0.06799231 1.         0.27      ], Reward: 0.0\n",
            "Processed Log → State: [0.28236663 0.2853562  0.06688002 1.         0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.29297218 0.35608473 0.07995038 0.9909967  0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.31192628 0.38378838 0.06770623 0.93520564 0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.32262373 0.37733325 0.07460069 0.91345626 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.3362314  0.37612164 0.0793616  0.9995959  0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.34986082 0.4032528  0.08102088 0.99495906 0.45      ], Reward: 7.0\n",
            "Processed Log → State: [0.37111825 0.41286466 0.07966354 1.         0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.39323816 0.408167   0.07762749 0.95829475 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.39496097 0.4116587  0.07938628 0.8949699  0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.40283644 0.38621908 0.07616536 0.94783235 0.57      ], Reward: 0.0\n",
            "Processed Log → State: [0.39175737 0.4091734  0.07899529 1.         0.6       ], Reward: 0.0\n",
            "Processed Log → State: [0.4324478  0.44847336 0.07879861 0.9887667  0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.42038044 0.4719189  0.07861944 1.         0.66      ], Reward: 4.0\n",
            "Processed Log → State: [0.40491614 0.4626087  0.09307268 0.9826644  0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.38618127 0.49297923 0.08344133 1.         0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.38852325 0.5230363  0.08353551 0.94847846 0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.44095853 0.5227077  0.08275636 1.         0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.47712952 0.5236032  0.10498868 1.         0.81      ], Reward: 5.0\n",
            "Processed Log → State: [0.4728088  0.53720576 0.1292211  1.         0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.44499665 0.53952473 0.13550147 1.         0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.45225498 0.5595722  0.1579837  1.         0.9       ], Reward: 6.0\n",
            "Processed Log → State: [0.22120635 0.3720357  0.12209    0.55989116 0.03      ], Reward: 7.0\n",
            "Processed Log → State: [0.33055773 0.43109038 0.20606406 0.57521504 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.41765258 0.5050782  0.20661391 0.52757573 0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.5149427  0.52988297 0.27452776 0.53051764 0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.5944614  0.56496316 0.3399675  0.44555396 0.15      ], Reward: 2.0\n",
            "Processed Log → State: [0.58291185 0.603792   0.34020042 0.3865397  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.59567654 0.67413497 0.34877458 0.44095758 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.72543293 0.6987454  0.36781996 0.49636668 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.7662613  0.7307138  0.4648495  0.43520105 0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.7829682  0.73519343 0.49890476 0.35309142 0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.8937514  0.7123272  0.4886744  0.34198996 0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.9109124  0.7345542  0.4681532  0.35102543 0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.98615754 0.7447575  0.5354229  0.25780126 0.39      ], Reward: 7.0\n",
            "Processed Log → State: [0.94490856 0.7758494  0.5795384  0.32868    0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.9620703  0.7582502  0.5837624  0.32028562 0.45      ], Reward: 7.0\n",
            "Processed Log → State: [0.9724962  0.786489   0.6737267  0.23414007 0.48      ], Reward: 2.0\n",
            "Processed Log → State: [0.9638022  0.79201514 0.73246366 0.13875958 0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.94647694 0.8081698  0.7130026  0.23377174 0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.98529565 0.8395569  0.624979   0.3490882  0.57      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.824807   0.58418036 0.3834036  0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.955685   0.76421463 0.5336688  0.42482877 0.63      ], Reward: 6.0\n",
            "Processed Log → State: [0.99846923 0.8011808  0.6083055  0.48570964 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.89994806 0.83260345 0.6783657  0.54388946 0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.8806658  0.83484334 0.71407    0.5196667  0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.9171234  0.8302707  0.73150736 0.48719397 0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.9700593 0.826254  0.756002  0.5655924 0.78     ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.8018137  0.7547736  0.49745217 0.81      ], Reward: 1.0\n",
            "Processed Log → State: [1.        0.7894968 0.7134967 0.471909  0.84     ], Reward: 8.0\n",
            "Processed Log → State: [0.9767348  0.7969183  0.7626701  0.44997215 0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.9743022  0.7866467  0.75414157 0.42830226 0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.22356133 0.30773938 0.05362264 0.54838926 0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.2879871  0.3473715  0.07207591 0.5356291  0.06      ], Reward: 5.0\n",
            "Processed Log → State: [0.25903782 0.39098212 0.09390225 0.5882395  0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.23942803 0.40204844 0.11528115 0.6008038  0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.26183602 0.45075953 0.11395559 0.5905665  0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.26946783 0.4719554  0.12264877 0.56937516 0.18      ], Reward: 6.0\n",
            "Processed Log → State: [0.29935804 0.5267424  0.12868582 0.60264534 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.29319638 0.5429099  0.12825981 0.63434154 0.24      ], Reward: 0.0\n",
            "Processed Log → State: [0.30192706 0.54894733 0.15654549 0.7327417  0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.29853302 0.55133283 0.17207146 0.7211417  0.3       ], Reward: 7.0\n",
            "Processed Log → State: [0.29874778 0.5776487  0.17174137 0.75350523 0.33      ], Reward: 2.0\n",
            "Processed Log → State: [0.35647097 0.58881944 0.18243098 0.76955146 0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.3845242  0.6363009  0.19743937 0.7619816  0.39      ], Reward: -1.0\n",
            "Processed Log → State: [0.40211734 0.64199644 0.19666892 0.78445107 0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.43696317 0.63607347 0.21055464 0.7392926  0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.44803858 0.67831403 0.22853439 0.7410343  0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.49032843 0.68004006 0.22837746 0.7382722  0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.4889316  0.694452   0.2461897  0.70180327 0.54      ], Reward: 0.0\n",
            "Processed Log → State: [0.46347928 0.6991808  0.23867238 0.79264855 0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.4767967  0.73222977 0.22673117 0.7925068  0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.481362   0.711474   0.22529492 0.84174293 0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.49983963 0.7326376  0.24344693 0.89084655 0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.5139164  0.76496536 0.2503581  0.8713869  0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.5494099  0.74696714 0.24668954 0.8661455  0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.5331892 0.7646714 0.2173296 0.8816991 0.75     ], Reward: 7.0\n",
            "Processed Log → State: [0.50147873 0.73584855 0.22724463 1.         0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.51496613 0.75345266 0.25308734 1.         0.81      ], Reward: 5.0\n",
            "Processed Log → State: [0.5387473  0.73487234 0.24800035 0.97433317 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.55973107 0.7257967  0.24018584 1.         0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.5436835  0.7369983  0.25642645 1.         0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.19785002 0.33789077 0.05367528 0.7242227  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.21010233 0.3419778  0.06774797 0.7972267  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.22291468 0.38003853 0.06619476 0.9014709  0.09      ], Reward: 0.0\n",
            "Processed Log → State: [0.22845437 0.355158   0.05986499 0.9150313  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.24281216 0.37225306 0.05234846 0.90988535 0.15      ], Reward: 5.0\n",
            "Processed Log → State: [0.23927526 0.41605255 0.06223591 0.9737835  0.18      ], Reward: -1.0\n",
            "Processed Log → State: [0.26340833 0.42845416 0.05321718 0.93175215 0.21      ], Reward: 4.0\n",
            "Processed Log → State: [0.27643204 0.46483254 0.05743203 0.9224437  0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.28612694 0.4700802  0.07055155 0.9764872  0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.27984506 0.504483   0.08414313 1.         0.3       ], Reward: 0.0\n",
            "Processed Log → State: [0.30854613 0.5591765  0.06695259 0.97481436 0.33      ], Reward: 5.0\n",
            "Processed Log → State: [0.28615895 0.54204494 0.06090156 0.98651415 0.36      ], Reward: 7.0\n",
            "Processed Log → State: [0.2671641  0.5746703  0.06019061 0.9619104  0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.2643861  0.57883114 0.05271034 1.         0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.26920637 0.5755041  0.06354547 1.         0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.27826878 0.5812322  0.05936045 0.92265487 0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.29427764 0.5799424  0.06994908 0.8831928  0.51      ], Reward: 2.0\n",
            "Processed Log → State: [0.31596166 0.55358034 0.06026422 0.9333026  0.54      ], Reward: 7.0\n",
            "Processed Log → State: [0.3502496  0.5458459  0.07319666 0.9325009  0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.34963247 0.5507543  0.07206552 0.9655496  0.6       ], Reward: 6.0\n",
            "Processed Log → State: [0.33715007 0.52616066 0.06520195 0.98830324 0.63      ], Reward: 7.0\n",
            "Processed Log → State: [0.33952406 0.5126784  0.06994049 0.96381617 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.34393355 0.52827215 0.07463478 1.         0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.39087853 0.5574557  0.06176322 0.96109504 0.72      ], Reward: 0.0\n",
            "Processed Log → State: [0.41745758 0.57463455 0.06672023 0.9598073  0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.41112855 0.6052018  0.07119472 0.83130497 0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.4263668  0.5998681  0.09197738 0.87977445 0.81      ], Reward: 0.0\n",
            "Processed Log → State: [0.4420684  0.6237565  0.11536485 0.90243506 0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.44812134 0.6085027  0.12735756 0.9201984  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.41806418 0.6227288  0.15999429 0.87850183 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.2086149  0.38912857 0.04966178 0.61818904 0.03      ], Reward: 7.0\n",
            "Processed Log → State: [0.21907525 0.49893728 0.05133963 0.5586136  0.06      ], Reward: 7.0\n",
            "Processed Log → State: [0.18521604 0.5492575  0.11122699 0.5402469  0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.18562765 0.59830666 0.11359178 0.44399577 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.30853903 0.69083214 0.15674476 0.48504037 0.15      ], Reward: 6.0\n",
            "Processed Log → State: [0.34547582 0.68005466 0.21805401 0.4624105  0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.26725668 0.7905973  0.21561277 0.45699328 0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.35995343 0.9201258  0.20200202 0.46071774 0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.33896953 1.         0.20152926 0.3957895  0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.39849502 1.         0.14528637 0.38512668 0.3       ], Reward: 2.0\n",
            "Processed Log → State: [0.37933576 0.9751495  0.14200857 0.36199966 0.33      ], Reward: 7.0\n",
            "Processed Log → State: [0.3422609  0.97779095 0.15240821 0.31568825 0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.40392703 1.         0.14121374 0.38922358 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.40580925 0.9957254  0.12288892 0.39758304 0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.467463   1.         0.113268   0.38215983 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.4941127  1.         0.12651283 0.34244758 0.48      ], Reward: 2.0\n",
            "Processed Log → State: [0.5381629  1.         0.18171775 0.33108136 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.53528064 0.96737254 0.14586835 0.3752459  0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.5485822  0.9767891  0.13863516 0.41285768 0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.5840704  1.         0.11055566 0.40610343 0.6       ], Reward: 6.0\n",
            "Processed Log → State: [0.58681726 0.94105226 0.10520077 0.36281636 0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.6483575  0.9344666  0.09138164 0.41670284 0.66      ], Reward: 0.0\n",
            "Processed Log → State: [0.7152843 0.9808028 0.130671  0.3610162 0.69     ], Reward: 2.0\n",
            "Processed Log → State: [0.7488065  1.         0.11690196 0.35403022 0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.8061967  1.         0.1524552  0.39840522 0.75      ], Reward: 0.0\n",
            "Processed Log → State: [0.85104203 0.98027503 0.12024878 0.40555486 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.8249243 1.        0.1686211 0.3902798 0.81     ], Reward: 3.0\n",
            "Processed Log → State: [0.8584254  1.         0.17666927 0.418009   0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.8998062  0.96513706 0.17898113 0.44170228 0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.96823    0.9896796  0.18439175 0.40789884 0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.21552198 0.32373345 0.06434424 0.50122887 0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.21753033 0.3472222  0.06913438 0.53081465 0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.2199242  0.34328747 0.09951033 0.5992042  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.24419537 0.352682   0.10503154 0.61124825 0.12      ], Reward: 0.0\n",
            "Processed Log → State: [0.28519696 0.40278763 0.12977156 0.59632295 0.15      ], Reward: 0.0\n",
            "Processed Log → State: [0.30144516 0.41710958 0.1395142  0.5934533  0.18      ], Reward: 7.0\n",
            "Processed Log → State: [0.32515645 0.44938877 0.17401178 0.5548249  0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.35690516 0.46292052 0.17775783 0.63882685 0.24      ], Reward: 5.0\n",
            "Processed Log → State: [0.3936193  0.47535178 0.17883132 0.6464569  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.4123358  0.5039681  0.19118023 0.6739442  0.3       ], Reward: 0.0\n",
            "Processed Log → State: [0.42794192 0.51466024 0.20136568 0.64549214 0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.47253573 0.53070843 0.20184755 0.6700562  0.36      ], Reward: 5.0\n",
            "Processed Log → State: [0.49317685 0.53021276 0.2237954  0.7009782  0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.5122346  0.5403774  0.22527371 0.70283866 0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.5413581  0.53972703 0.22581959 0.70131487 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.5669122 0.5601214 0.2315241 0.6877184 0.48     ], Reward: 8.0\n",
            "Processed Log → State: [0.59219897 0.5730628  0.24080963 0.7464309  0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.58806527 0.55819774 0.23805586 0.76226133 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.61866796 0.54719627 0.22833626 0.86484003 0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.6431513 0.5442602 0.2317415 0.8137964 0.6      ], Reward: 3.0\n",
            "Processed Log → State: [0.662325  0.5847913 0.2262462 0.7737919 0.63     ], Reward: 6.0\n",
            "Processed Log → State: [0.6589578  0.6090974  0.21389149 0.78346366 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.66127634 0.6107328  0.22990438 0.7938513  0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.66484994 0.6445235  0.23047449 0.78647894 0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.6319154  0.6717765  0.22714794 0.795199   0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.654291   0.6608223  0.22760092 0.8534806  0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.6714026  0.67348975 0.2334793  0.85293144 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.6694124  0.71357805 0.24490987 0.8515521  0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.68105906 0.7073691  0.25028384 0.8053881  0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.684254  0.72601   0.2583316 0.7569942 0.9      ], Reward: 7.0\n",
            "Processed Log → State: [0.1780079  0.33192003 0.05924025 0.6401479  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.199327   0.38456807 0.07201489 0.65320325 0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.2580246  0.36950985 0.07918081 0.76731807 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.2701578  0.41309783 0.09143746 0.749852   0.12      ], Reward: 0.0\n",
            "Processed Log → State: [0.29107562 0.46594715 0.10009162 0.7630166  0.15      ], Reward: 2.0\n",
            "Processed Log → State: [0.26132092 0.5137319  0.12195028 0.76401633 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.28323323 0.5438726  0.12741932 0.7191754  0.21      ], Reward: 0.0\n",
            "Processed Log → State: [0.30122164 0.5534972  0.15754096 0.7580918  0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.2991912  0.5533991  0.16850737 0.7850146  0.27      ], Reward: 5.0\n",
            "Processed Log → State: [0.33514455 0.56809795 0.17566128 0.8564708  0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.34674487 0.54054135 0.17399272 0.83801347 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.33303565 0.56321853 0.17023546 0.8893707  0.36      ], Reward: 5.0\n",
            "Processed Log → State: [0.37012324 0.53740853 0.16182746 0.9032543  0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.4131499  0.55665153 0.16415739 0.8927674  0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.4116765  0.56683743 0.17369795 0.84210557 0.45      ], Reward: 4.0\n",
            "Processed Log → State: [0.42916602 0.5583311  0.17334832 0.81361103 0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.4942323  0.5781656  0.19677828 0.83941793 0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.5226751 0.6201293 0.1993532 0.8462215 0.54     ], Reward: 6.0\n",
            "Processed Log → State: [0.55042297 0.6315365  0.20290023 0.84188443 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.556364   0.6746291  0.19220975 0.93518627 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.5662533 0.7196162 0.2113729 0.918604  0.63     ], Reward: 1.0\n",
            "Processed Log → State: [0.5637643  0.700521   0.23090625 0.91080934 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.5757556  0.69981956 0.23827474 0.9670685  0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.58088917 0.7210726  0.24176516 1.         0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.59058696 0.73595285 0.24875596 0.9884917  0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.59687614 0.7377646  0.23999223 0.98097897 0.78      ], Reward: 6.0\n",
            "Processed Log → State: [0.5898612  0.7250358  0.24465965 1.         0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.61890167 0.70486695 0.27334273 1.         0.84      ], Reward: 7.0\n",
            "Processed Log → State: [0.6221077  0.72858334 0.28240034 1.         0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.6086966  0.7298784  0.29379594 0.9709175  0.9       ], Reward: 0.0\n",
            "Processed Log → State: [0.21430111 0.36580715 0.06740954 0.5740392  0.03      ], Reward: 7.0\n",
            "Processed Log → State: [0.2286754  0.39080015 0.07638506 0.5925961  0.06      ], Reward: 7.0\n",
            "Processed Log → State: [0.2560828  0.3595317  0.07945377 0.6358705  0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.2759411  0.35339668 0.09087005 0.67348284 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.2833318  0.37213656 0.09674757 0.73859656 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.2811014  0.40198326 0.09044803 0.80604184 0.18      ], Reward: 6.0\n",
            "Processed Log → State: [0.2827076  0.42090324 0.08866473 0.8411397  0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.30365604 0.46426994 0.10613426 0.89496267 0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.31111813 0.49457607 0.11627012 0.94255316 0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.35543042 0.51591516 0.12902905 0.98516    0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.36701208 0.55039716 0.1314114  1.         0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.38534242 0.573699   0.1354786  1.         0.36      ], Reward: -1.0\n",
            "Processed Log → State: [0.43014428 0.5790928  0.13072851 1.         0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.394058   0.62230706 0.14959142 1.         0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.43137696 0.6648074  0.15686803 1.         0.45      ], Reward: 5.0\n",
            "Processed Log → State: [0.44906598 0.6888433  0.1822359  1.         0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.41592014 0.6990121  0.19886363 0.9642193  0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.41991976 0.69426006 0.20064074 1.         0.54      ], Reward: 5.0\n",
            "Processed Log → State: [0.43067944 0.6746225  0.19832699 0.97947556 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.43564105 0.6609118  0.1937355  1.         0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.46318114 0.67009735 0.18729065 1.         0.63      ], Reward: 6.0\n",
            "Processed Log → State: [0.46969423 0.6811952  0.20105538 1.         0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.4802208  0.6889296  0.19503462 0.9288468  0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.5037139  0.67385095 0.19678845 1.         0.72      ], Reward: -2.0\n",
            "Processed Log → State: [0.51450723 0.69388217 0.20816208 0.97711736 0.75      ], Reward: 4.0\n",
            "Processed Log → State: [0.5271154  0.68809295 0.20731822 0.961021   0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.51325524 0.66073596 0.21624045 0.9887923  0.81      ], Reward: 0.0\n",
            "Processed Log → State: [0.5019196  0.68039536 0.22698069 0.93979126 0.84      ], Reward: 0.0\n",
            "Processed Log → State: [0.50348437 0.69285566 0.24375601 0.95549196 0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.51440114 0.69440395 0.2504103  1.         0.9       ], Reward: 6.0\n",
            "Processed Log → State: [0.29727343 0.34925818 0.07202701 0.5330464  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.2990065  0.39332637 0.12476425 0.45282972 0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.26698366 0.48811075 0.12081383 0.4105039  0.09      ], Reward: 2.0\n",
            "Processed Log → State: [0.2783113  0.5833701  0.15339625 0.3320607  0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.32391915 0.61323017 0.25274938 0.35117835 0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.36252388 0.60839456 0.25696018 0.39018106 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.5091168  0.60066295 0.26990148 0.3891501  0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.5929697  0.6655035  0.32828265 0.34712148 0.24      ], Reward: 7.0\n",
            "Processed Log → State: [0.54149    0.7680972  0.35801303 0.3286835  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.5361357  0.7242977  0.32766584 0.33660164 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.557247   0.7905164  0.32226554 0.3284286  0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.64943445 0.811098   0.2940081  0.32172513 0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.69547427 0.8787065  0.30668995 0.3520849  0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.7710371  0.95426047 0.2948235  0.36624312 0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.76135373 0.9769595  0.28199646 0.38119823 0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.80302185 1.         0.31655768 0.32314315 0.48      ], Reward: -2.0\n",
            "Processed Log → State: [0.8370824  0.98609066 0.3758648  0.3004653  0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.88619775 1.         0.45346844 0.35725152 0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.80516607 1.         0.41194257 0.49722064 0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.9226021  1.         0.44610468 0.54532206 0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.9355582  1.         0.45103982 0.47881013 0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.9611375  1.         0.43451592 0.45766255 0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.9459495  1.         0.42275965 0.44326985 0.69      ], Reward: 7.0\n",
            "Processed Log → State: [0.9476217  0.9229467  0.37886998 0.43670106 0.72      ], Reward: 0.0\n",
            "Processed Log → State: [0.9506807  1.         0.37599257 0.4578457  0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.849537   1.         0.38852477 0.49742454 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.89431614 0.9027921  0.37680674 0.44245213 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.8400885  0.99257964 0.39163527 0.50440145 0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.88441706 1.         0.3927753  0.52603143 0.87      ], Reward: 5.0\n",
            "Processed Log → State: [0.87246734 1.         0.41572723 0.62111217 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.30160636 0.39037827 0.19606198 0.62279654 0.03      ], Reward: 0.0\n",
            "Processed Log → State: [0.38824078 0.4633204  0.24937885 0.6567982  0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.33418182 0.47140867 0.2513963  0.6855649  0.09      ], Reward: 0.0\n",
            "Processed Log → State: [0.32678863 0.51533705 0.272966   0.6495276  0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.39299753 0.5080517  0.2591212  0.63853586 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.39490932 0.5544362  0.34250793 0.58989084 0.18      ], Reward: 5.0\n",
            "Processed Log → State: [0.35783836 0.6273402  0.31028694 0.4623845  0.21      ], Reward: 7.0\n",
            "Processed Log → State: [0.37285942 0.6766907  0.362592   0.41435573 0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.37186182 0.66846853 0.39232653 0.37911853 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.39022577 0.64419085 0.43019986 0.31213173 0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.42860055 0.6582652  0.48574147 0.19838485 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.42894217 0.6862551  0.5870921  0.24349983 0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.4336442 0.6983697 0.5742946 0.2631623 0.39     ], Reward: 3.0\n",
            "Processed Log → State: [0.3899295  0.7640198  0.6882472  0.29513943 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.45480955 0.7722417  0.668048   0.3628899  0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.43040857 0.8075911  0.6604377  0.39896408 0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.48259196 0.7688481  0.6344565  0.40636235 0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.4721823  0.78698534 0.65382123 0.37312573 0.54      ], Reward: 7.0\n",
            "Processed Log → State: [0.52172    0.8190989  0.7055902  0.37564352 0.57      ], Reward: 2.0\n",
            "Processed Log → State: [0.529479   0.8205857  0.71861696 0.39449543 0.6       ], Reward: 0.0\n",
            "Processed Log → State: [0.57292175 0.8659116  0.6977981  0.3636677  0.63      ], Reward: 7.0\n",
            "Processed Log → State: [0.5254067  0.8821579  0.6739418  0.29003277 0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.61658436 0.9091777  0.63795894 0.3293053  0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.6187396  0.91335493 0.67052484 0.23889443 0.72      ], Reward: 6.0\n",
            "Processed Log → State: [0.6539017  0.9275652  0.7438534  0.27925104 0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.6973249  0.9271262  0.77569747 0.3278251  0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.7434864  0.91039455 0.820997   0.31520116 0.81      ], Reward: 6.0\n",
            "Processed Log → State: [0.7833299  0.8893237  0.8462671  0.30659413 0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.82628036 0.90550476 0.8795696  0.333491   0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.90141594 0.87070364 0.93434364 0.39759585 0.9       ], Reward: 5.0\n",
            "Processed Log → State: [0.18647188 0.3627234  0.05770395 0.5430032  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.19735812 0.405399   0.14070296 0.4116809  0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.11298104 0.4617941  0.16963984 0.42923608 0.09      ], Reward: 5.0\n",
            "Processed Log → State: [0.15280603 0.5474316  0.18539535 0.41528636 0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.19871973 0.55324787 0.22829825 0.41930914 0.15      ], Reward: 7.0\n",
            "Processed Log → State: [0.28315318 0.57002413 0.17897929 0.33732975 0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.31915438 0.5706267  0.24679422 0.31717458 0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.30115914 0.57415146 0.2629409  0.35679093 0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.31744722 0.65130424 0.2776959  0.4027514  0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.27840573 0.6638046  0.2920848  0.35129872 0.3       ], Reward: 0.0\n",
            "Processed Log → State: [0.28549156 0.7699308  0.26391307 0.31904584 0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.3538143  0.8285976  0.25793716 0.25033706 0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.33055145 0.8757249  0.25256455 0.29887605 0.39      ], Reward: 6.0\n",
            "Processed Log → State: [0.37580815 0.94530815 0.21986471 0.36425933 0.42      ], Reward: 5.0\n",
            "Processed Log → State: [0.4593965 0.9671734 0.2602208 0.4304953 0.45     ], Reward: 6.0\n",
            "Processed Log → State: [0.47796407 1.         0.24743573 0.49579594 0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.41413033 1.         0.2504153  0.42791817 0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.38159692 1.         0.23412529 0.37409148 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.34165815 1.         0.231629   0.3623089  0.57      ], Reward: 0.0\n",
            "Processed Log → State: [0.34430802 1.         0.2690168  0.41626966 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.32648054 1.         0.20651798 0.38365296 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.32785794 1.         0.22091103 0.40062666 0.66      ], Reward: 7.0\n",
            "Processed Log → State: [0.28583288 0.99323684 0.22293639 0.3246506  0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.33433083 0.98790085 0.24866307 0.4186093  0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.33455995 0.96533555 0.23044474 0.4366571  0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.35702768 0.97699434 0.27651012 0.4289738  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.39531448 1.         0.28688997 0.4086779  0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.4430498  0.9805628  0.33498582 0.5356624  0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.50896806 0.97424954 0.33730945 0.6214976  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.56547135 0.94190687 0.35061547 0.57514846 0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.22906947 0.34886652 0.12313111 0.60230094 0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.20602705 0.40355676 0.11313743 0.53067446 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.2526078  0.53906983 0.12838064 0.55425286 0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.25947496 0.65596485 0.17002867 0.60207784 0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.30348235 0.70807457 0.19112189 0.6152281  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.2774135  0.78772765 0.22916389 0.5569112  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.32362035 0.8855235  0.1709886  0.55002874 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.31146935 0.9211584  0.16342478 0.52874243 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.32060772 0.97687596 0.21273638 0.5477513  0.27      ], Reward: -1.0\n",
            "Processed Log → State: [0.4047157  0.9696424  0.2668701  0.54957944 0.3       ], Reward: 7.0\n",
            "Processed Log → State: [0.36837173 1.         0.24964282 0.5189797  0.33      ], Reward: 7.0\n",
            "Processed Log → State: [0.41111672 0.9968351  0.24883857 0.48167267 0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.41986856 1.         0.21551314 0.4785543  0.39      ], Reward: 6.0\n",
            "Processed Log → State: [0.5009737  1.         0.21864071 0.46185213 0.42      ], Reward: 7.0\n",
            "Processed Log → State: [0.4309056  1.         0.24191882 0.44533193 0.45      ], Reward: 7.0\n",
            "Processed Log → State: [0.41610432 1.         0.25050464 0.39722893 0.48      ], Reward: 6.0\n",
            "Processed Log → State: [0.42974353 1.         0.31330863 0.45609352 0.51      ], Reward: 7.0\n",
            "Processed Log → State: [0.45926455 0.9938505  0.34789    0.517222   0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.5287652  1.         0.35694578 0.4881928  0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.49482626 1.         0.35740843 0.42140332 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.55169654 0.95644945 0.4120037  0.40658098 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.6113643  1.         0.40538764 0.37052408 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.6693266  0.97525656 0.39114213 0.44003597 0.69      ], Reward: 0.0\n",
            "Processed Log → State: [0.6628607  1.         0.3897611  0.51252276 0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.7341607  0.9534565  0.38946837 0.46208456 0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.75741994 0.9633547  0.40232402 0.47422048 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.7105614  0.9633403  0.49021485 0.3981734  0.81      ], Reward: 0.0\n",
            "Processed Log → State: [0.71791714 1.         0.5341889  0.39466742 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.7566655 0.9533113 0.5224841 0.4136265 0.87     ], Reward: 1.0\n",
            "Processed Log → State: [0.76277864 0.8770721  0.5294036  0.50107056 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.1986751  0.35887873 0.02244851 0.554326   0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.27581277 0.4388255  0.08561827 0.5710041  0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.38794914 0.48772183 0.02260371 0.5839355  0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.32675573 0.5608198  0.0631063  0.55064976 0.12      ], Reward: 7.0\n",
            "Processed Log → State: [0.38200787 0.6123695  0.11667813 0.48947796 0.15      ], Reward: 6.0\n",
            "Processed Log → State: [0.39895093 0.6934213  0.12928106 0.4747346  0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.4303958  0.78929526 0.18561755 0.4367706  0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.39983258 0.897556   0.21987817 0.43761805 0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.42122802 1.         0.23255171 0.3398638  0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.44455475 1.         0.21195754 0.34354275 0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.4215496  1.         0.23357148 0.34653363 0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.46628833 1.         0.28056824 0.37201712 0.36      ], Reward: 5.0\n",
            "Processed Log → State: [0.567114   1.         0.28710374 0.32866418 0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.570478   1.         0.29642227 0.3687677  0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.64126945 1.         0.34849077 0.36746097 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.66800475 1.         0.38410228 0.37795755 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.7256977  0.93749154 0.4481036  0.4587045  0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.7761486  0.9872005  0.41515923 0.4305143  0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.8517025  1.         0.41461822 0.49091196 0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.8387377  1.         0.42796585 0.43566456 0.6       ], Reward: 4.0\n",
            "Processed Log → State: [0.8696664  1.         0.45397672 0.42451754 0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.8839465 1.        0.4709632 0.4534234 0.66     ], Reward: 1.0\n",
            "Processed Log → State: [0.8507816  1.         0.47890115 0.43166864 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.89194274 1.         0.49409327 0.43955693 0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.9585631  0.9862863  0.49597895 0.48247096 0.75      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.9709712  0.4809101  0.49076265 0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.98286426 0.9370357  0.5117255  0.49576533 0.81      ], Reward: 5.0\n",
            "Processed Log → State: [0.93869954 0.9078008  0.46669808 0.5219577  0.84      ], Reward: 7.0\n",
            "Processed Log → State: [0.99190074 0.90324384 0.43956447 0.60434866 0.87      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.9678812  0.43811694 0.4947085  0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.23623024 0.3265525  0.04735209 0.65599996 0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.25270686 0.32145432 0.04430789 0.64661044 0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.26489738 0.35740533 0.04204112 0.6647186  0.09      ], Reward: 0.0\n",
            "Processed Log → State: [0.26775637 0.4005707  0.04143067 0.74703854 0.12      ], Reward: 5.0\n",
            "Processed Log → State: [0.24547379 0.4324125  0.05879087 0.7712104  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.26735324 0.4320431  0.06217152 0.7843459  0.18      ], Reward: 7.0\n",
            "Processed Log → State: [0.30762386 0.43362468 0.0860726  0.824134   0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.3389617  0.43376735 0.08191182 0.8473134  0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.3780411  0.4520436  0.10610627 0.82632726 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.3940883  0.47841498 0.11283199 0.8662478  0.3       ], Reward: 2.0\n",
            "Processed Log → State: [0.40799984 0.50171417 0.11959899 0.90038174 0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.42583865 0.48753646 0.13385256 0.93536866 0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.43690878 0.51957124 0.1412134  1.         0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.44113907 0.5734692  0.14994603 0.97245383 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.43871814 0.5832721  0.1761091  0.9636025  0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.44842538 0.57314205 0.1836364  1.         0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.4593607  0.5999836  0.19491339 1.         0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.50370353 0.61878437 0.19479099 0.99934715 0.54      ], Reward: 0.0\n",
            "Processed Log → State: [0.51584846 0.6022482  0.20137721 0.9934702  0.57      ], Reward: 0.0\n",
            "Processed Log → State: [0.51957244 0.64489555 0.22382548 1.         0.6       ], Reward: 5.0\n",
            "Processed Log → State: [0.5077207  0.6677092  0.23583156 1.         0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.5360753  0.67290074 0.22110902 1.         0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.5426543  0.6602759  0.20897676 0.9918242  0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.56885785 0.66269016 0.2277765  0.9841734  0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.53577447 0.68954766 0.22576274 1.         0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.5211001  0.6917339  0.22289892 1.         0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.5385076 0.6595467 0.2408867 1.        0.81     ], Reward: 1.0\n",
            "Processed Log → State: [0.5708738  0.63554156 0.25976962 0.9629636  0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.57078904 0.6612109  0.26073122 0.9801012  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.53126293 0.65652406 0.26159993 1.         0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.20207584 0.2705783  0.06240477 0.61355054 0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.18293604 0.27704018 0.06511499 0.6106855  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.1952079  0.24503733 0.0708968  0.54517347 0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.19642551 0.2706998  0.09065525 0.55400604 0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.20806207 0.2732686  0.09248434 0.585927   0.15      ], Reward: 7.0\n",
            "Processed Log → State: [0.2036353  0.2844019  0.09673951 0.5094266  0.18      ], Reward: 0.0\n",
            "Processed Log → State: [0.237688   0.3114849  0.11598231 0.5030541  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.23517267 0.3202988  0.11790861 0.4874541  0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.20324247 0.33934644 0.1148815  0.6124609  0.27      ], Reward: 4.0\n",
            "Processed Log → State: [0.20198959 0.33562168 0.11879341 0.64657    0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.21065103 0.3368429  0.10983109 0.6412248  0.33      ], Reward: 2.0\n",
            "Processed Log → State: [0.2142003  0.35300273 0.10864402 0.6108663  0.36      ], Reward: 5.0\n",
            "Processed Log → State: [0.21927524 0.3840761  0.10495808 0.62689704 0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.22880004 0.40813532 0.10888198 0.65489703 0.42      ], Reward: 7.0\n",
            "Processed Log → State: [0.25652674 0.42914078 0.12473233 0.58932185 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.24481498 0.42639104 0.14162838 0.68294555 0.48      ], Reward: 2.0\n",
            "Processed Log → State: [0.27380916 0.43057695 0.14747071 0.6975124  0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.25765258 0.45817098 0.13395043 0.72534865 0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.2917873 0.4693694 0.1392751 0.8155736 0.57     ], Reward: 3.0\n",
            "Processed Log → State: [0.29620734 0.46754643 0.1469615  0.83161134 0.6       ], Reward: 6.0\n",
            "Processed Log → State: [0.29693657 0.49583176 0.13958222 0.7867013  0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.33657506 0.50147295 0.13660721 0.7618887  0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.30711022 0.5010177  0.14154786 0.8086319  0.69      ], Reward: 0.0\n",
            "Processed Log → State: [0.2976685  0.506085   0.13683541 0.8852137  0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.30771944 0.4781955  0.13333026 0.8800718  0.75      ], Reward: 0.0\n",
            "Processed Log → State: [0.34483886 0.49139485 0.13157761 0.8844315  0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.33564085 0.51079303 0.13929254 0.87120473 0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.3557758  0.50911355 0.15691143 0.8801543  0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.3524385  0.5297942  0.17458503 0.91087157 0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.3449984  0.5490389  0.18772861 0.88644046 0.9       ], Reward: 6.0\n",
            "Processed Log → State: [0.19348763 0.30504808 0.05023798 0.63740176 0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.2119698  0.3144548  0.07054867 0.64687675 0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.20236802 0.29149386 0.07145122 0.6441736  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.23583707 0.3130393  0.05110417 0.595931   0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.20501232 0.30366188 0.06377374 0.59423465 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.22014023 0.32421774 0.08246966 0.621195   0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.2088034  0.33020023 0.07434753 0.5274704  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.19010957 0.35465434 0.066943   0.6231859  0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.23421364 0.37638605 0.08381674 0.6408589  0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.25445402 0.40246782 0.09404533 0.63870496 0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.28026244 0.41542232 0.09672861 0.7411577  0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.27240083 0.42195645 0.07765012 0.7948081  0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.2719328  0.42245707 0.08386075 0.8493708  0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.28855452 0.38472417 0.07795737 0.859942   0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.3152841  0.40105802 0.08454968 0.8634881  0.45      ], Reward: 5.0\n",
            "Processed Log → State: [0.33337972 0.41291    0.0773258  0.8402614  0.48      ], Reward: -1.0\n",
            "Processed Log → State: [0.29238832 0.40238208 0.0982854  0.8100583  0.51      ], Reward: 5.0\n",
            "Processed Log → State: [0.29163224 0.41921574 0.09594224 0.8974035  0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.290061   0.41854995 0.11243688 0.9573461  0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.27692467 0.43852586 0.11641762 0.98910105 0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.2834422  0.45641467 0.1121164  0.9892779  0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.30051816 0.46726546 0.12744452 1.         0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.32205373 0.46809855 0.1396106  0.87422854 0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.3432963  0.46623933 0.13525309 0.8601567  0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.33985096 0.48440525 0.1431344  0.75399935 0.75      ], Reward: 6.0\n",
            "Processed Log → State: [0.32575542 0.46278793 0.1293739  0.8117222  0.78      ], Reward: 6.0\n",
            "Processed Log → State: [0.32296506 0.45651844 0.1601157  0.78796345 0.81      ], Reward: -2.0\n",
            "Processed Log → State: [0.36478895 0.49213126 0.1723052  0.8076452  0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.3934258  0.48378313 0.18326376 0.8358762  0.87      ], Reward: 7.0\n",
            "Processed Log → State: [0.41597944 0.5135365  0.1816179  0.82471746 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.3531217  0.36936286 0.0652742  0.6066146  0.03      ], Reward: 0.0\n",
            "Processed Log → State: [0.41099194 0.45090523 0.03153364 0.59039927 0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.3959248  0.49668232 0.05805752 0.5886546  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.3506634  0.4431072  0.06123674 0.5096283  0.12      ], Reward: -1.0\n",
            "Processed Log → State: [0.3294005  0.48970821 0.08901479 0.43818536 0.15      ], Reward: 0.0\n",
            "Processed Log → State: [0.34085435 0.5783158  0.06813254 0.43191898 0.18      ], Reward: 2.0\n",
            "Processed Log → State: [0.41526794 0.6618453  0.12150034 0.44425106 0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.46479827 0.6160572  0.11672261 0.44244376 0.24      ], Reward: -2.0\n",
            "Processed Log → State: [0.47659898 0.58501273 0.1449007  0.4749502  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.5208427  0.57330155 0.1754909  0.45634037 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.56813085 0.590719   0.17173737 0.39533246 0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.57764053 0.55721575 0.15757762 0.3986785  0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.57693994 0.51292133 0.13289872 0.36556223 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.57589746 0.56212986 0.13856654 0.34809697 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.61568785 0.6070195  0.18556026 0.46642068 0.45      ], Reward: 7.0\n",
            "Processed Log → State: [0.58398706 0.6090116  0.24836223 0.4507103  0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.6492032  0.69732696 0.278688   0.519963   0.51      ], Reward: 2.0\n",
            "Processed Log → State: [0.65810424 0.86320984 0.29188916 0.4925352  0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.6306678  0.9597325  0.3377883  0.60017836 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.7066553 0.9408905 0.3395228 0.6004136 0.6      ], Reward: 1.0\n",
            "Processed Log → State: [0.6762188 0.9748061 0.3896534 0.6193682 0.63     ], Reward: 7.0\n",
            "Processed Log → State: [0.7068751  0.9799184  0.40089765 0.6606407  0.66      ], Reward: 0.0\n",
            "Processed Log → State: [0.6501351  0.9310191  0.37793013 0.64632493 0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.5501187  1.         0.35544887 0.67521596 0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.5153969  0.9681304  0.41031578 0.6615231  0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.46473303 0.93160623 0.37069613 0.54481465 0.78      ], Reward: 5.0\n",
            "Processed Log → State: [0.57918596 0.9154204  0.40550548 0.5688143  0.81      ], Reward: 6.0\n",
            "Processed Log → State: [0.6295047  0.9545585  0.3454869  0.58014905 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.6859241  0.9789435  0.35927364 0.6157179  0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.6615132  0.96156317 0.35042176 0.661723   0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.3006789  0.3558162  0.05041892 0.5679111  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.3582658  0.45001423 0.11814343 0.6048727  0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.3856832  0.44141635 0.15837057 0.5366425  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.44091383 0.46130943 0.1745576  0.58092296 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.61393636 0.43187374 0.2037805  0.5788609  0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.589557   0.4816099  0.16298707 0.50243896 0.18      ], Reward: 2.0\n",
            "Processed Log → State: [0.6006141  0.5430441  0.19628473 0.52341807 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.6246545  0.621132   0.20653197 0.4875203  0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.6844162  0.6678564  0.24696572 0.52690494 0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.6613235  0.79862916 0.2263623  0.45147946 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.69303066 0.88612914 0.2493727  0.46918082 0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.66860896 0.941333   0.20987162 0.47209355 0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.73003525 1.         0.21084738 0.47326624 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.7645858  1.         0.25127256 0.4749161  0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.84103465 1.         0.29634318 0.43695214 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.8869145  0.97179717 0.28311595 0.36594355 0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.9321457 0.9901002 0.2657513 0.2847085 0.51     ], Reward: 5.0\n",
            "Processed Log → State: [0.97356844 1.         0.3051554  0.30204165 0.54      ], Reward: 8.0\n",
            "Processed Log → State: [1.        1.        0.3234865 0.2133427 0.57     ], Reward: 1.0\n",
            "Processed Log → State: [1.         1.         0.28274077 0.15791875 0.6       ], Reward: 8.0\n",
            "Processed Log → State: [1.         1.         0.25545576 0.13697685 0.63      ], Reward: 5.0\n",
            "Processed Log → State: [0.9596689  0.9975349  0.2647169  0.18330418 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.92235965 0.9966325  0.22733767 0.20158967 0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.93276995 0.9611159  0.21045138 0.20525733 0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.98757756 0.9977313  0.23635465 0.25263694 0.75      ], Reward: 0.0\n",
            "Processed Log → State: [1.         1.         0.24570455 0.26023582 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [1.         1.         0.2363898  0.37802705 0.81      ], Reward: 7.0\n",
            "Processed Log → State: [0.8977366  1.         0.24842876 0.3581382  0.84      ], Reward: 7.0\n",
            "Processed Log → State: [0.84948075 1.         0.23794073 0.35918447 0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.79861563 1.         0.25105417 0.27787817 0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.19426322 0.3880726  0.05991802 0.5968767  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.21922491 0.39245364 0.05187522 0.57856727 0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.15028004 0.4528305  0.09228133 0.5345991  0.09      ], Reward: -2.0\n",
            "Processed Log → State: [0.18430194 0.5126155  0.08338439 0.59999543 0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.2376502  0.57461005 0.03626896 0.5707065  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.29094717 0.65011066 0.01687083 0.58605266 0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.39184278 0.680736   0.0103105  0.5809047  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.38646272 0.6265652  0.03955135 0.53532374 0.24      ], Reward: 0.0\n",
            "Processed Log → State: [0.40959302 0.62133795 0.10433394 0.48103353 0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.44495863 0.69135857 0.09828372 0.42747474 0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.52503073 0.7626935  0.04447637 0.44028118 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.5286317  0.8218876  0.09156326 0.39044437 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.52351207 0.85351187 0.08629132 0.36368856 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.5605178  0.7862572  0.09829841 0.3740481  0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.56370693 0.8334214  0.16530198 0.24954908 0.45      ], Reward: -1.0\n",
            "Processed Log → State: [0.5417779  0.9311432  0.16999453 0.229924   0.48      ], Reward: 2.0\n",
            "Processed Log → State: [0.62013125 0.9471121  0.17530501 0.27177325 0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.6103601  0.9799861  0.20521688 0.25003913 0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.5650879  0.9646585  0.25958157 0.18516581 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.5395044  1.         0.30087522 0.20405194 0.6       ], Reward: 7.0\n",
            "Processed Log → State: [0.55529034 0.9866588  0.3078519  0.1548293  0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.49324763 0.9149002  0.29703978 0.18637037 0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.5082703  1.         0.31175354 0.24288927 0.69      ], Reward: 5.0\n",
            "Processed Log → State: [0.531572   0.94898194 0.34916937 0.250685   0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.5492768  0.9198213  0.36475518 0.26685575 0.75      ], Reward: 4.0\n",
            "Processed Log → State: [0.60157615 0.92164755 0.3527563  0.30988428 0.78      ], Reward: 0.0\n",
            "Processed Log → State: [0.6112553  0.9880193  0.40444288 0.41082704 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.6782181  1.         0.4251274  0.44860026 0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.73214036 1.         0.4572705  0.4110669  0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.8466007  1.         0.44556245 0.4376288  0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.2728763  0.33847195 0.12604733 0.53558826 0.03      ], Reward: -1.0\n",
            "Processed Log → State: [0.3572565  0.360601   0.13241315 0.39938095 0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.4375359  0.40978608 0.1490944  0.2913161  0.09      ], Reward: 5.0\n",
            "Processed Log → State: [0.45737627 0.47338238 0.23092231 0.25399846 0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.4585006  0.5165877  0.22116546 0.24871387 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.40048838 0.48985314 0.22214594 0.18387134 0.18      ], Reward: 7.0\n",
            "Processed Log → State: [0.52792025 0.4767105  0.35783353 0.05375164 0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.5060004  0.52999765 0.4462028  0.11587504 0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.60821056 0.5741028  0.44970453 0.15594782 0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.7031308  0.58354646 0.4976108  0.08348078 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.7320513  0.67443585 0.530393   0.02408483 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.90968055 0.6703586  0.59135604 0.1029277  0.36      ], Reward: 7.0\n",
            "Processed Log → State: [0.9659791  0.70231706 0.5521147  0.06490437 0.39      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.7120049  0.5946111  0.02720713 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.9481646  0.71928376 0.6210138  0.04789971 0.45      ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.7331512  0.61754805 0.07161506 0.48      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.70507777 0.7370451  0.04837912 0.51      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.7120402  0.78664017 0.03494295 0.54      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.73485464 0.7136535  0.11705141 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.7374578  0.7493355  0.14150488 0.6       ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.7060112  0.75558317 0.14912085 0.63      ], Reward: 6.0\n",
            "Processed Log → State: [0.9927911  0.7358913  0.7889035  0.18758975 0.66      ], Reward: 2.0\n",
            "Processed Log → State: [1.         0.6910962  0.7402934  0.11052486 0.69      ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.6340735  0.70319086 0.13338476 0.72      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.6476369  0.7266227  0.16045636 0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.9923381  0.7252222  0.7262161  0.23209919 0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.9649774  0.76530397 0.6230628  0.31017515 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.93734556 0.7884763  0.62334716 0.36547822 0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.89198405 0.8132749  0.6173303  0.36436507 0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.80541825 0.8195895  0.6809573  0.38683873 0.9       ], Reward: 0.0\n",
            "Processed Log → State: [0.22228654 0.3035142  0.03390671 0.6139731  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.24173301 0.32853836 0.03537359 0.56431973 0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.2384307  0.3607307  0.04609763 0.579981   0.09      ], Reward: 2.0\n",
            "Processed Log → State: [0.28314438 0.3850205  0.0467991  0.55001736 0.12      ], Reward: 0.0\n",
            "Processed Log → State: [0.31913254 0.3987993  0.05748488 0.611383   0.15      ], Reward: 0.0\n",
            "Processed Log → State: [0.32674795 0.40867245 0.04919392 0.6538516  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.3261223  0.41471443 0.04398873 0.6068898  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.34396073 0.4188331  0.04210692 0.64083844 0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.3383077  0.44939545 0.05135279 0.69279015 0.27      ], Reward: 7.0\n",
            "Processed Log → State: [0.38413042 0.45217445 0.05463863 0.69560087 0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.369065   0.45487055 0.05349016 0.73719233 0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.36379606 0.461727   0.04392418 0.7892272  0.36      ], Reward: -1.0\n",
            "Processed Log → State: [0.38646922 0.45051363 0.03145501 0.82119805 0.39      ], Reward: 6.0\n",
            "Processed Log → State: [0.39249647 0.4981959  0.03079562 0.88161594 0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.44079304 0.51810116 0.01580723 1.         0.45      ], Reward: 0.0\n",
            "Processed Log → State: [0.43645227 0.5198078  0.00948688 0.9681973  0.48      ], Reward: 2.0\n",
            "Processed Log → State: [0.4452423  0.5512437  0.00852672 0.9864677  0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.4109718  0.5614591  0.00178313 1.         0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.41772735 0.59393656 0.01108492 0.9923257  0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.4157213  0.5589962  0.00347107 1.         0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.4161368  0.5752374  0.01346642 0.9827973  0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.46787336 0.5636166  0.00913625 0.98925996 0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.4446188  0.58607185 0.01246706 0.962915   0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.4907902  0.5998203  0.02739064 1.         0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.50141555 0.58217925 0.04138935 0.9516984  0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.52115756 0.5623764  0.03112347 0.94495803 0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.5158283  0.58995825 0.05464479 0.93102384 0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.50542146 0.60839045 0.06537829 0.9229117  0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.49956036 0.6351836  0.08847681 0.9245758  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.5115412  0.644875   0.10573936 0.8739375  0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.29989025 0.29074383 0.02625899 0.57581747 0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.35295847 0.3372151  0.06863317 0.49214095 0.06      ], Reward: 0.0\n",
            "Processed Log → State: [0.38276836 0.33986542 0.05195497 0.46846133 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.4521066  0.38761586 0.01700471 0.48459744 0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.46303803 0.41863027 0.03707112 0.4481736  0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.46578005 0.4636235  0.09041975 0.44382322 0.18      ], Reward: 6.0\n",
            "Processed Log → State: [0.55265224 0.47586834 0.22314927 0.3559022  0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.6540116  0.501345   0.19432202 0.33599117 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.6040778  0.579028   0.22079837 0.34903523 0.27      ], Reward: 5.0\n",
            "Processed Log → State: [0.6009921  0.59392226 0.20350297 0.32923022 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.5950852  0.6451907  0.28220677 0.34018978 0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.6519772  0.7135003  0.36169758 0.41170725 0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.7272328  0.75864494 0.44390664 0.34062436 0.39      ], Reward: 6.0\n",
            "Processed Log → State: [0.8010361  0.80125    0.48335433 0.2894815  0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.8016861  0.78054583 0.51039577 0.3196706  0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.7851084  0.852007   0.5221429  0.33005795 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.70627624 0.8777109  0.51148635 0.36042613 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.8318026  0.87332475 0.5028365  0.3947672  0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.92845464 0.859607   0.537081   0.3951169  0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.9596988  0.81294006 0.5411269  0.41094124 0.6       ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.83435905 0.5620349  0.37652373 0.63      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.830288   0.6023095  0.37334818 0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.97971785 0.8326036  0.61632204 0.4223362  0.69      ], Reward: 8.0\n",
            "Processed Log → State: [0.9629761  0.86395544 0.6127641  0.34843048 0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.95477825 0.8693431  0.5780889  0.34218654 0.75      ], Reward: 5.0\n",
            "Processed Log → State: [0.9660768  0.8416378  0.62308156 0.32874352 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.93927807 0.849767   0.59363985 0.32297894 0.81      ], Reward: 2.0\n",
            "Processed Log → State: [0.9621185 0.7667518 0.7258171 0.364103  0.84     ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.74319315 0.7286538  0.3847856  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.9850546  0.7624693  0.70855594 0.44286028 0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.29525527 0.36610043 0.07989914 0.58369046 0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.30061164 0.42967162 0.1179238  0.62925243 0.06      ], Reward: 7.0\n",
            "Processed Log → State: [0.32976857 0.47168252 0.16311799 0.54832804 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.38373896 0.5904482  0.19308259 0.56691283 0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.4488089  0.48040417 0.22158661 0.5732756  0.15      ], Reward: 0.0\n",
            "Processed Log → State: [0.44958988 0.5054975  0.2546549  0.63198495 0.18      ], Reward: 2.0\n",
            "Processed Log → State: [0.47208712 0.52620846 0.2598998  0.58662754 0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.5335184  0.5321178  0.29807493 0.5328491  0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.54134065 0.5487889  0.31065568 0.5635396  0.27      ], Reward: 7.0\n",
            "Processed Log → State: [0.62543595 0.58979636 0.32304254 0.54637176 0.3       ], Reward: 0.0\n",
            "Processed Log → State: [0.64865786 0.5812046  0.32710996 0.5237852  0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.6970492  0.501147   0.30728528 0.5382423  0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.7887876  0.5756003  0.3371149  0.54308707 0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.8263817  0.7220535  0.37824124 0.5879814  0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.8321202 0.8144061 0.3595314 0.600419  0.45     ], Reward: 6.0\n",
            "Processed Log → State: [0.9359034  0.801396   0.37500942 0.5928018  0.48      ], Reward: 2.0\n",
            "Processed Log → State: [0.9586342  0.81071883 0.3615573  0.6338587  0.51      ], Reward: 2.0\n",
            "Processed Log → State: [0.8479752  0.8310796  0.3371528  0.63605946 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.8401971  0.8978549  0.33209524 0.54660845 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.9291651  0.8972066  0.35418513 0.56883126 0.6       ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.88213986 0.32396922 0.5526246  0.63      ], Reward: 4.0\n",
            "Processed Log → State: [1.         0.85685235 0.3012228  0.62325954 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.88497585 0.29305407 0.666475   0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.9823295  0.9144796  0.3111733  0.69721967 0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.9504736  0.8525473  0.29745492 0.6794072  0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.98851305 0.876457   0.33141792 0.68917924 0.78      ], Reward: 2.0\n",
            "Processed Log → State: [1.         0.8493278  0.38052112 0.6859234  0.81      ], Reward: -1.0\n",
            "Processed Log → State: [0.96140146 0.8482668  0.40007406 0.6024062  0.84      ], Reward: 0.0\n",
            "Processed Log → State: [0.9267907  0.8168487  0.39191532 0.59968054 0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.9690599  0.8329791  0.45402694 0.6091789  0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.24022791 0.32354775 0.06873489 0.65156746 0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.27309448 0.34490433 0.07000358 0.6591753  0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.28494978 0.33167285 0.10255308 0.7496307  0.09      ], Reward: 2.0\n",
            "Processed Log → State: [0.2849347  0.35217813 0.12452011 0.82031155 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.29764116 0.36750165 0.13465297 0.8593633  0.15      ], Reward: 5.0\n",
            "Processed Log → State: [0.33476388 0.37565205 0.12425816 0.91089857 0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.3736614  0.39630213 0.13823305 0.9544062  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.4336319  0.42015716 0.16522053 0.9324386  0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.4627151  0.43169835 0.17894791 0.9799261  0.27      ], Reward: -1.0\n",
            "Processed Log → State: [0.4818655  0.4319509  0.19146955 1.         0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.49092868 0.4743548  0.19447233 0.9738082  0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.4873028  0.5206192  0.20293774 1.         0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.5174665  0.54540396 0.19994465 0.9750156  0.39      ], Reward: 5.0\n",
            "Processed Log → State: [0.4851399  0.5682303  0.20444818 1.         0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.4964962  0.571367   0.21105218 1.         0.45      ], Reward: -2.0\n",
            "Processed Log → State: [0.515453   0.5853741  0.22291726 1.         0.48      ], Reward: -2.0\n",
            "Processed Log → State: [0.49303058 0.6197491  0.21470429 1.         0.51      ], Reward: 7.0\n",
            "Processed Log → State: [0.5351116  0.62680924 0.22915632 1.         0.54      ], Reward: 7.0\n",
            "Processed Log → State: [0.56985176 0.6761879  0.23848434 0.998233   0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.5981428  0.6597331  0.23831956 0.98286575 0.6       ], Reward: 6.0\n",
            "Processed Log → State: [0.6087025 0.6471446 0.223749  0.9958867 0.63     ], Reward: 8.0\n",
            "Processed Log → State: [0.6055341  0.645973   0.21472964 0.98780894 0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.62797    0.6540215  0.2221765  0.96315515 0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.6362087  0.70432127 0.21813934 1.         0.72      ], Reward: 7.0\n",
            "Processed Log → State: [0.63675874 0.7164554  0.2216987  1.         0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.635249   0.73523194 0.21233444 1.         0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.6221368  0.70963275 0.22564307 1.         0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.5830435  0.67415506 0.24983633 1.         0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.5919031  0.6668843  0.26902843 1.         0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.60422814 0.69021326 0.27536815 0.978661   0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.20251025 0.33307874 0.07904954 0.6148991  0.03      ], Reward: 7.0\n",
            "Processed Log → State: [0.18841642 0.36497265 0.07832015 0.63061666 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.20882058 0.34926695 0.08285352 0.6033977  0.09      ], Reward: 0.0\n",
            "Processed Log → State: [0.2216768  0.33198753 0.10728726 0.6135434  0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.25917903 0.36453983 0.12321653 0.7203899  0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.28282866 0.37335023 0.12726009 0.7385216  0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.32735842 0.39299962 0.14069332 0.8955014  0.21      ], Reward: 0.0\n",
            "Processed Log → State: [0.33703578 0.4047048  0.14294899 0.95660555 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.38347113 0.4365761  0.15942441 0.9517933  0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.41519827 0.419853   0.1581885  0.9793486  0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.42277876 0.45202523 0.16169542 0.9640679  0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.44699585 0.47894388 0.16449997 0.9539185  0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.46716943 0.48578492 0.16262881 0.9552305  0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.490669   0.49221283 0.15906915 1.         0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.48687214 0.50062907 0.1518166  0.96803224 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.49973723 0.53694636 0.1591241  0.93539023 0.48      ], Reward: 2.0\n",
            "Processed Log → State: [0.5163447  0.52662355 0.1622984  0.9733521  0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.5509581  0.54053676 0.16990986 0.9157983  0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.5677987  0.5445674  0.16107813 0.9509296  0.57      ], Reward: 5.0\n",
            "Processed Log → State: [0.5742988  0.5660015  0.16926895 0.9357579  0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.59547216 0.5835453  0.17516229 0.9849365  0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.5976474  0.60068333 0.17819253 0.92994016 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.5956606  0.6410988  0.18187967 0.90134704 0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.6177194  0.63853514 0.18859752 0.9593926  0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.63774055 0.61930966 0.19723836 0.9991917  0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.65184724 0.65195775 0.18973248 1.         0.78      ], Reward: 6.0\n",
            "Processed Log → State: [0.6768439 0.6513023 0.2226257 0.9598664 0.81     ], Reward: 0.0\n",
            "Processed Log → State: [0.6684252  0.6930635  0.25292978 0.9110538  0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.64575255 0.70231074 0.2792629  0.91939956 0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.65418446 0.6802633  0.28766975 0.99201286 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.19781284 0.3066294  0.05334285 0.62099266 0.03      ], Reward: 0.0\n",
            "Processed Log → State: [0.18366963 0.30561364 0.06429313 0.6363744  0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.21261232 0.39519355 0.04906594 0.69728804 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.23962545 0.403501   0.07008155 0.72590816 0.12      ], Reward: 7.0\n",
            "Processed Log → State: [0.23788342 0.40355152 0.08445713 0.78389114 0.15      ], Reward: 7.0\n",
            "Processed Log → State: [0.27986115 0.41728514 0.10425747 0.760521   0.18      ], Reward: 0.0\n",
            "Processed Log → State: [0.29322588 0.43646106 0.11646934 0.74364036 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.32267743 0.46307722 0.14181781 0.8055826  0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.3337522  0.46111596 0.13656346 0.80515057 0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.3830705  0.472013   0.12681486 0.75911975 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.43637952 0.48145634 0.1404423  0.8165611  0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.46506017 0.4793475  0.16020283 0.8171127  0.36      ], Reward: 5.0\n",
            "Processed Log → State: [0.47189113 0.49158075 0.17965667 0.8217815  0.39      ], Reward: 6.0\n",
            "Processed Log → State: [0.45502952 0.49952602 0.19055907 0.8753034  0.42      ], Reward: 7.0\n",
            "Processed Log → State: [0.47655597 0.539846   0.21103173 0.8655717  0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.48293635 0.5554591  0.21161942 0.92580783 0.48      ], Reward: 2.0\n",
            "Processed Log → State: [0.4882244  0.55394757 0.21849237 1.         0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.48725373 0.58860177 0.22057028 0.9929355  0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.5196474  0.5639888  0.2234832  0.99700505 0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.566352   0.60155344 0.22406887 1.         0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.5417658 0.5927496 0.2271768 1.        0.63     ], Reward: 6.0\n",
            "Processed Log → State: [0.5565942  0.6001265  0.21137913 0.9477237  0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.55771345 0.57588077 0.21810707 0.95920974 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.57424384 0.58606786 0.21932577 0.9777237  0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.5802132  0.5915339  0.20882389 1.         0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.58108324 0.5662421  0.2069248  1.         0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.5636828  0.5426977  0.21998252 1.         0.81      ], Reward: -1.0\n",
            "Processed Log → State: [0.5894799  0.54827136 0.24375403 1.         0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.6315723  0.54958194 0.25491434 1.         0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.6297034  0.55782735 0.27732936 1.         0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.21185695 0.3086636  0.06633354 0.6215524  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.27885053 0.30840847 0.06060634 0.70709383 0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.2976468  0.316982   0.05998421 0.7797095  0.09      ], Reward: 2.0\n",
            "Processed Log → State: [0.32346913 0.29526114 0.06455556 0.78626925 0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.2815385  0.3137678  0.05793069 0.7944448  0.15      ], Reward: 5.0\n",
            "Processed Log → State: [0.2775057  0.3195485  0.03506466 0.8362347  0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.31151134 0.32113007 0.05125809 0.8002064  0.21      ], Reward: 7.0\n",
            "Processed Log → State: [0.33537027 0.32736704 0.0519475  0.8006056  0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.36319077 0.36513534 0.06061463 0.803097   0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.3681326  0.3713159  0.06381515 0.7918969  0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.37605235 0.3843768  0.08264203 0.88149816 0.33      ], Reward: -1.0\n",
            "Processed Log → State: [0.35821506 0.39386138 0.0989095  0.9195494  0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.3871176  0.41161358 0.11089694 1.         0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.40129402 0.37865415 0.10872743 1.         0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.39070693 0.35464504 0.13105878 0.9160636  0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.3711784  0.36260045 0.1391207  0.92082316 0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.41192383 0.37073973 0.13603546 0.8319765  0.51      ], Reward: 7.0\n",
            "Processed Log → State: [0.41072044 0.39069852 0.11884886 0.8590992  0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.44058964 0.40802687 0.11409023 0.8740243  0.57      ], Reward: 0.0\n",
            "Processed Log → State: [0.42525122 0.39741606 0.11841764 0.9302876  0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.43871203 0.40195584 0.12583905 0.81812245 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.43479696 0.4557584  0.11790227 0.8214274  0.66      ], Reward: 0.0\n",
            "Processed Log → State: [0.43452206 0.46468267 0.11174728 0.7773852  0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.4173513  0.49569345 0.11272359 0.8197656  0.72      ], Reward: 6.0\n",
            "Processed Log → State: [0.44080263 0.48001888 0.08713526 0.87395    0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.4493663  0.48020574 0.11232163 0.8662935  0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.43129575 0.4990311  0.12448621 0.95720375 0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.4638266  0.51299036 0.1438368  0.972502   0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.47405607 0.5057286  0.16507901 1.         0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.47230777 0.5290774  0.16558528 0.9003658  0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.2231514  0.33185905 0.06270993 0.64818746 0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.26606327 0.3483584  0.06632358 0.6621282  0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.29767993 0.3781129  0.07024451 0.6695816  0.09      ], Reward: 2.0\n",
            "Processed Log → State: [0.33047274 0.41551408 0.08793063 0.7695574  0.12      ], Reward: 7.0\n",
            "Processed Log → State: [0.3416049  0.4647609  0.1097651  0.86250484 0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.37596437 0.49230105 0.10808143 0.8626036  0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.39851734 0.4900256  0.12884371 0.8962729  0.21      ], Reward: 0.0\n",
            "Processed Log → State: [0.4100175  0.5294933  0.14130268 0.8889595  0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.45432326 0.57077175 0.16903879 0.89231527 0.27      ], Reward: 7.0\n",
            "Processed Log → State: [0.48598227 0.57488805 0.16651715 0.9043751  0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.50250256 0.620084   0.17535934 1.         0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.5027147  0.6146657  0.20091346 1.         0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.52284896 0.6136052  0.19851711 1.         0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.5114617  0.6050095  0.21518521 0.999452   0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.5155944  0.624627   0.22590986 1.         0.45      ], Reward: 5.0\n",
            "Processed Log → State: [0.56817985 0.6241844  0.22112744 1.         0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.5746769  0.6841358  0.22834483 1.         0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.5576459  0.7167119  0.23788205 1.         0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.5790773  0.72234327 0.25162143 1.         0.57      ], Reward: 7.0\n",
            "Processed Log → State: [0.5922416  0.73996013 0.24780256 1.         0.6       ], Reward: 7.0\n",
            "Processed Log → State: [0.5922606  0.7522384  0.25833523 0.9900259  0.63      ], Reward: -1.0\n",
            "Processed Log → State: [0.6361137  0.74599665 0.26642486 1.         0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.66949445 0.73871195 0.25795773 0.97856194 0.69      ], Reward: 0.0\n",
            "Processed Log → State: [0.64806676 0.75075924 0.24471048 0.960261   0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.679245   0.73819345 0.23931259 0.97472787 0.75      ], Reward: -1.0\n",
            "Processed Log → State: [0.7103427  0.76194036 0.23429261 1.         0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.7113348  0.7729891  0.27052438 1.         0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.7450549  0.7625083  0.28755534 0.983152   0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.7378615  0.7690524  0.29814148 0.94017196 0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.7361208  0.7684186  0.30479258 0.9541564  0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.26559365 0.3418468  0.06712312 0.6267994  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.20476456 0.41391653 0.09912392 0.6166218  0.06      ], Reward: 5.0\n",
            "Processed Log → State: [0.25401917 0.416945   0.1452611  0.5933226  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.29362842 0.4014632  0.13089567 0.5807559  0.12      ], Reward: 5.0\n",
            "Processed Log → State: [0.44359708 0.48148578 0.14624509 0.57466465 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.46451864 0.5536403  0.16355252 0.48948547 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.55399036 0.6241559  0.09210999 0.4795064  0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.63537407 0.6755526  0.10764275 0.45170933 0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.71531945 0.70560205 0.16938303 0.37020865 0.27      ], Reward: 0.0\n",
            "Processed Log → State: [0.7650513  0.66852355 0.14644876 0.3211544  0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.81917566 0.6906446  0.12543656 0.25234297 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.81825083 0.79085773 0.1140691  0.24864082 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.78344154 0.8530498  0.0909249  0.32624888 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.85501343 0.9170465  0.09368621 0.39406434 0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.8964783  0.94354737 0.15270591 0.3867143  0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.854192   1.         0.17215997 0.4650036  0.48      ], Reward: 7.0\n",
            "Processed Log → State: [0.87031126 1.         0.19740725 0.33771884 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.7966003  1.         0.2590383  0.26088724 0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.8300763  1.         0.27830556 0.26196647 0.57      ], Reward: 2.0\n",
            "Processed Log → State: [0.8391047  0.96639526 0.3127163  0.2635034  0.6       ], Reward: 7.0\n",
            "Processed Log → State: [0.8441669  0.9871674  0.31980214 0.23623696 0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.86808795 0.9619043  0.3061864  0.20597579 0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.905142   1.         0.30866528 0.29297674 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.8611906  1.         0.2881239  0.24039517 0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.8308859  0.9970369  0.33473837 0.27355433 0.75      ], Reward: 7.0\n",
            "Processed Log → State: [0.9075307  0.90857285 0.3193281  0.23278399 0.78      ], Reward: 6.0\n",
            "Processed Log → State: [0.975298  0.9425406 0.2836178 0.290667  0.81     ], Reward: 3.0\n",
            "Processed Log → State: [0.9398538  0.98626184 0.2755421  0.3259998  0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.9643502  0.9922728  0.28390563 0.3470331  0.87      ], Reward: 2.0\n",
            "Processed Log → State: [0.90070176 1.         0.31011045 0.3828898  0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.2212771  0.33845887 0.06636105 0.6021734  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.2381144  0.3410773  0.13494904 0.55541116 0.06      ], Reward: 7.0\n",
            "Processed Log → State: [0.23984094 0.36804503 0.16630144 0.50399816 0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.2845625  0.3804387  0.17161997 0.42734656 0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.33760184 0.40411922 0.17862602 0.38201237 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.3812685  0.40049723 0.1769406  0.3362926  0.18      ], Reward: 2.0\n",
            "Processed Log → State: [0.4380484  0.3365893  0.25749525 0.26592448 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.45261526 0.43521672 0.3240773  0.21875311 0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.4869457  0.445537   0.30098325 0.24289213 0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.55711454 0.50383884 0.31945035 0.14889339 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.5699064  0.5656304  0.37480628 0.17261088 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.60282487 0.58646405 0.41966102 0.20098424 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.5872403  0.62080336 0.5408102  0.18003188 0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.66273725 0.64882344 0.58990467 0.17639898 0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.72614497 0.65794355 0.59881663 0.20959163 0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.7262222  0.6522585  0.6122334  0.23567685 0.48      ], Reward: 7.0\n",
            "Processed Log → State: [0.79527766 0.664221   0.61247945 0.29705766 0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.82105535 0.6575414  0.60102373 0.3238163  0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.8505616  0.66306    0.6394759  0.27091783 0.57      ], Reward: 5.0\n",
            "Processed Log → State: [0.88715607 0.65781677 0.6144312  0.31255367 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.93196535 0.71244305 0.6410439  0.3341911  0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.87378854 0.7203844  0.6915163  0.40070444 0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.93852025 0.74015975 0.65126735 0.34550706 0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.99428976 0.7668313  0.65971404 0.34658527 0.72      ], Reward: 6.0\n",
            "Processed Log → State: [0.977348   0.82047534 0.69282246 0.34695095 0.75      ], Reward: -2.0\n",
            "Processed Log → State: [1.        0.8117103 0.6890988 0.3929051 0.78     ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.81689507 0.7012007  0.39049116 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.93260986 0.807003   0.75900185 0.42074773 0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.93255055 0.78660053 0.7748606  0.40737522 0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.88702416 0.83961207 0.74017686 0.39423853 0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.1452561  0.27861834 0.05991961 0.63825154 0.03      ], Reward: -1.0\n",
            "Processed Log → State: [0.06688624 0.36824065 0.07276641 0.6442518  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.16646214 0.44964623 0.05896181 0.6250439  0.09      ], Reward: -1.0\n",
            "Processed Log → State: [0.16868214 0.54134524 0.02454567 0.59068286 0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.12500714 0.57048887 0.0776247  0.5446804  0.15      ], Reward: 6.0\n",
            "Processed Log → State: [0.1304837  0.54025066 0.12673019 0.48731762 0.18      ], Reward: 2.0\n",
            "Processed Log → State: [0.19857061 0.59117204 0.16290386 0.4581659  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.24254271 0.6568753  0.17705229 0.49050477 0.24      ], Reward: 0.0\n",
            "Processed Log → State: [0.2650522  0.7013466  0.18910307 0.47406152 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.337736   0.7471002  0.20591635 0.5085847  0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.35672113 0.6909842  0.19682847 0.5028356  0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.36668226 0.73226726 0.20486721 0.47359613 0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.41868228 0.85843056 0.23033847 0.43372664 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.32052314 0.8536609  0.22113866 0.339585   0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.29955238 0.8821449  0.22482345 0.3757097  0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.25091806 0.96443206 0.21144219 0.349029   0.48      ], Reward: 5.0\n",
            "Processed Log → State: [0.34988287 0.9663968  0.25207114 0.32787302 0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.30185398 0.95551586 0.2679167  0.2790235  0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.36316508 0.95388645 0.32668075 0.25115064 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.33777225 0.95924675 0.23841311 0.22638217 0.6       ], Reward: 6.0\n",
            "Processed Log → State: [0.3473764  0.9974852  0.27996254 0.18372376 0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.39864624 0.99930966 0.29710498 0.18047106 0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.4674002 1.        0.2902062 0.2210106 0.69     ], Reward: 3.0\n",
            "Processed Log → State: [0.4535919  0.944407   0.30828896 0.23491676 0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.47056457 1.         0.26169488 0.26431537 0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.47508854 1.         0.29019034 0.3096344  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.5157275  1.         0.34152052 0.29488078 0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.6016577  1.         0.31968915 0.3070352  0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.5960939  0.98088014 0.3280431  0.331206   0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.6156072  0.9558336  0.35643563 0.3577112  0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.25396156 0.3691904  0.11889638 0.6517292  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.29829264 0.3991931  0.2761352  0.6560969  0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.30017886 0.41163373 0.25824836 0.6222039  0.09      ], Reward: 0.0\n",
            "Processed Log → State: [0.3640657  0.47502673 0.18077993 0.67156917 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.43422818 0.49461532 0.21572417 0.6608403  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.3901061  0.49102098 0.24337146 0.62629473 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.46394557 0.5722765  0.17313617 0.54851085 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.53000224 0.6071443  0.27037343 0.52441794 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.624814   0.57557094 0.2953172  0.6207464  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.7086493  0.6276452  0.28944305 0.6300732  0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.6691273  0.65139925 0.4263094  0.5088846  0.33      ], Reward: 2.0\n",
            "Processed Log → State: [0.77293307 0.70196855 0.45680207 0.5378672  0.36      ], Reward: 7.0\n",
            "Processed Log → State: [0.8022931  0.7428011  0.51549786 0.5228719  0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.89173025 0.77158624 0.5455562  0.49270684 0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.94892883 0.79749644 0.5542016  0.4802226  0.45      ], Reward: 7.0\n",
            "Processed Log → State: [0.9811865  0.76584953 0.5827314  0.5048628  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.7279893  0.6030159  0.48422426 0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.9931214  0.75806785 0.5740624  0.43260285 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [1.        0.7749381 0.5901761 0.4188148 0.57     ], Reward: 4.0\n",
            "Processed Log → State: [0.96624756 0.7998497  0.5817017  0.33976617 0.6       ], Reward: 6.0\n",
            "Processed Log → State: [0.9236719  0.7661014  0.59316254 0.35798502 0.63      ], Reward: 7.0\n",
            "Processed Log → State: [0.90358335 0.7493769  0.63229233 0.31880158 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.9669821  0.7361453  0.594104   0.31862777 0.69      ], Reward: -2.0\n",
            "Processed Log → State: [0.9573665 0.7241807 0.6470403 0.3005484 0.72     ], Reward: 1.0\n",
            "Processed Log → State: [0.9662175  0.7366642  0.6345175  0.31952238 0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.9992735  0.70838684 0.6350515  0.32187015 0.78      ], Reward: 0.0\n",
            "Processed Log → State: [1.        0.7481444 0.6459137 0.3272592 0.81     ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.79714966 0.71122885 0.36061576 0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.95278513 0.76225555 0.6168279  0.3868479  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.82946396 0.5548655  0.4078069  0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.22509378 0.3204036  0.06281727 0.69495535 0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.2293714  0.36396214 0.07438852 0.7466739  0.06      ], Reward: 0.0\n",
            "Processed Log → State: [0.24567527 0.37807256 0.08827258 0.79935205 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.24993508 0.36882687 0.09343451 0.78523785 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.24469957 0.39052933 0.10204724 0.8049804  0.15      ], Reward: -1.0\n",
            "Processed Log → State: [0.22737847 0.38669094 0.11695985 0.8811487  0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.26835254 0.38313648 0.12655653 0.9393498  0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.34074986 0.41136596 0.15828486 0.98658144 0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.331475   0.42712957 0.17197987 1.         0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.34469688 0.43187273 0.19415295 1.         0.3       ], Reward: -1.0\n",
            "Processed Log → State: [0.3694596  0.45724058 0.18680827 1.         0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.40536347 0.46948567 0.18578088 1.         0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.4353721  0.47761074 0.20171534 0.9777324  0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.42270052 0.48060977 0.20336224 0.98009026 0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.44679606 0.5276606  0.21671887 1.         0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.47355938 0.5216365  0.2222464  1.         0.48      ], Reward: 2.0\n",
            "Processed Log → State: [0.46063346 0.53028286 0.22506267 1.         0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.5203602  0.54437226 0.23288217 1.         0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.50704676 0.5873825  0.23329304 1.         0.57      ], Reward: 2.0\n",
            "Processed Log → State: [0.47918713 0.5832026  0.23573613 1.         0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.5115274  0.6010812  0.23973328 1.         0.63      ], Reward: 5.0\n",
            "Processed Log → State: [0.46714863 0.59958696 0.23576318 1.         0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.49804297 0.5836149  0.2377244  1.         0.69      ], Reward: 4.0\n",
            "Processed Log → State: [0.49180192 0.59702635 0.22337079 0.9685172  0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.5197218  0.5735302  0.22567652 0.94988734 0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.53323334 0.59902203 0.23554324 0.90326345 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.5687422 0.6045264 0.2501378 1.        0.81     ], Reward: 8.0\n",
            "Processed Log → State: [0.5571067  0.58896774 0.25868452 1.         0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.59510344 0.6162485  0.26428553 1.         0.87      ], Reward: 2.0\n",
            "Processed Log → State: [0.5597899  0.61085445 0.2801697  1.         0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.21593684 0.31515488 0.05967328 0.5823078  0.03      ], Reward: 2.0\n",
            "Processed Log → State: [0.17246224 0.33120903 0.08228483 0.5582948  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.19702993 0.3373984  0.09226405 0.58188486 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.18756352 0.36476064 0.10533028 0.6370063  0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.22775508 0.34361708 0.10631886 0.72711045 0.15      ], Reward: 4.0\n",
            "Processed Log → State: [0.2423992  0.37244436 0.11807977 0.8030047  0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.24623041 0.37406442 0.12076624 0.8312908  0.21      ], Reward: 7.0\n",
            "Processed Log → State: [0.25253585 0.41707397 0.12329239 0.8969912  0.24      ], Reward: 0.0\n",
            "Processed Log → State: [0.27908498 0.47671565 0.13376872 0.93616265 0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.3345793  0.4930335  0.14680566 0.90119064 0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.36362392 0.5336721  0.15866418 0.9114238  0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.3756665 0.5528947 0.1566496 0.9717106 0.36     ], Reward: -2.0\n",
            "Processed Log → State: [0.39532462 0.55050963 0.1702962  0.93955576 0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.4398421  0.53723973 0.19043456 0.9529381  0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.4802779  0.5517826  0.18521358 0.97177297 0.45      ], Reward: -1.0\n",
            "Processed Log → State: [0.5124665  0.5845495  0.17519088 0.9918382  0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.52456313 0.5992243  0.1810385  1.         0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.5312428  0.60275495 0.18532635 1.         0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.51698107 0.6288175  0.19133955 1.         0.57      ], Reward: 7.0\n",
            "Processed Log → State: [0.49681324 0.65653694 0.16505326 0.9865715  0.6       ], Reward: 4.0\n",
            "Processed Log → State: [0.5070388  0.69216806 0.18568252 1.         0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.5435163 0.69113   0.1923453 1.        0.66     ], Reward: 3.0\n",
            "Processed Log → State: [0.5549406  0.7137572  0.19473477 0.99261403 0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.53201395 0.7412765  0.19221084 0.9531623  0.72      ], Reward: 6.0\n",
            "Processed Log → State: [0.5256282  0.7553733  0.18391785 0.992093   0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.50510395 0.73617196 0.18813483 0.98613024 0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.48962876 0.7223046  0.21698773 1.         0.81      ], Reward: 2.0\n",
            "Processed Log → State: [0.49981508 0.69146824 0.24776828 0.96617305 0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.47599515 0.6683793  0.25975814 1.         0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.47521806 0.6707899  0.26003066 0.9701457  0.9       ], Reward: 3.0\n",
            "Training complete and model saved.\n",
            "Recommended action after training on batch: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Module Structure Outline and Integration\n",
        "\n",
        "This outlines the proposed structure for the new modules (Idea Brainstorming, Writing Support, Emotion Support) and their integration with the existing Ethics Module, including the potential role of a central orchestrator.\n",
        "\n",
        "### 1. New Modules: Classes and Key Methods\n",
        "\n",
        "**`IdeaBrainstorming` Class:**\n",
        "\n",
        "*   **Purpose:** Assists the user in generating and exploring ideas related to their thesis topic.\n",
        "*   **Key Methods:**\n",
        "    *   `generate_ideas(prompt: str, thesis_stage: str)`: Takes a user prompt and thesis stage as input, uses an LLM to generate a list of potential ideas, and returns them.\n",
        "    *   `explore_idea(idea: str)`: Takes a specific idea and provides further details, related concepts, or potential research questions.\n",
        "    *   `refine_ideas(ideas: list, feedback: str)`: Takes a list of generated ideas and user feedback to refine or generate variations.\n",
        "\n",
        "**`WritingSupport` Class:**\n",
        "\n",
        "*   **Purpose:** Provides assistance with writing tasks, including outlining, drafting, and refining text.\n",
        "*   **Key Methods:**\n",
        "    *   `create_outline(topic: str, thesis_stage: str)`: Generates a structural outline for a given topic and thesis stage.\n",
        "    *   `draft_section(outline: dict, context: str)`: Drafts a section of the thesis based on an outline and provided context.\n",
        "    *   `summarize_text(text: str)`: Provides a summary of a given text.\n",
        "    *   `rephrase_text(text: str, style: str)`: Rewrites a given text in a different style or to improve clarity.\n",
        "    *   `check_grammar_style(text: str)`: Provides feedback on grammar, spelling, and writing style.\n",
        "\n",
        "**`EmotionSupport` Class:**\n",
        "\n",
        "*   **Purpose:** Offers emotional encouragement and support to the user based on their interactions and detected sentiment.\n",
        "*   **Key Methods:**\n",
        "    *   `detect_emotion(user_input: str)`: Analyzes user input to detect underlying emotions or sentiment (e.g., frustrated, motivated, overwhelmed).\n",
        "    *   `provide_encouragement(detected_emotion: str, thesis_stage: str)`: Provides tailored encouraging messages based on the detected emotion and thesis stage.\n",
        "    *   `suggest_break()`: Suggests taking a break based on usage patterns or detected frustration.\n",
        "    *   `celebrate_milestone(milestone: str)`: Provides positive reinforcement upon completion of a thesis milestone.\n",
        "\n",
        "### 2. Interactions Between New Modules\n",
        "\n",
        "*   **`IdeaBrainstorming` -> `WritingSupport`:** Ideas generated in `IdeaBrainstorming` could be used as input for `WritingSupport` methods like `create_outline` or `draft_section`.\n",
        "*   **`WritingSupport` -> `EmotionSupport`:** If `WritingSupport` detects difficulty or frustration in user input or repeated requests for rephrasing due to low confidence, it could signal the `EmotionSupport` module.\n",
        "*   **`EmotionSupport` -> `WritingSupport`:** The `EmotionSupport` module could provide feedback or suggestions to `WritingSupport` to adjust its tone or complexity based on the user's emotional state. For example, if the user is overwhelmed, `WritingSupport` might simplify suggestions or break down tasks into smaller steps.\n",
        "\n",
        "### 3. Interactions with the Existing `EthicsModule`\n",
        "\n",
        "Each new module will interact with the `EthicsModule` to ensure ethical usage and track activity.\n",
        "\n",
        "*   **`IdeaBrainstorming` <-> `EthicsModule`:**\n",
        "    *   **Sending to EthicsModule:** Send the user's initial prompt, the generated ideas, and any user feedback on those ideas for logging (`Usage_Logger`) and potential ethical checks (e.g., checking if the prompt is asking for unethical brainstorming).\n",
        "    *   **Receiving from EthicsModule:** Receive ethical warnings or suggestions related to the brainstorming process (e.g., if the prompt is too close to existing work, suggesting exploring significantly different angles).\n",
        "\n",
        "*   **`WritingSupport` <-> `EthicsModule`:**\n",
        "    *   **Sending to EthicsModule:** Send the user's input text (for rephrasing, summarization, etc.), the generated outlines, drafted sections, and any edited text from the user. This is crucial for authorship tracking (`Usage_Logger`), AI detection (`AI_Detector`), and potential plagiarism checks (via interfaces).\n",
        "    *   **Receiving from EthicsModule:** Receive AI detection scores, potential plagiarism alerts, suggestions for rephrasing to increase originality, and warnings about potential academic dishonesty based on prompt analysis (`HumanPromptChecker`, `EthicalViolationAlert`). The Ethics Module might also recommend logging specific usage patterns.\n",
        "\n",
        "*   **`EmotionSupport` <-> `EthicsModule`:**\n",
        "    *   **Sending to EthicsModule:** Send the detected emotion and the type of emotional support provided for logging (`Usage_Logger`). This can help the Ethics Module understand the user's state in relation to potentially unethical behavior (e.g., frustration leading to cutting corners).\n",
        "    *   **Receiving from EthicsModule:** The Ethics Module might use the emotional state as a factor in its RL state and decision-making. For example, if a user is highly frustrated, the Ethics Module might recommend a less confrontational intervention or suggest a break (`EthicalViolationAlert` could potentially incorporate this).\n",
        "\n",
        "### 4. Central Orchestrator/Manager Class (`ThesisAssistant`)\n",
        "\n",
        "A central orchestrator class, potentially named `ThesisAssistant`, would be beneficial to manage the flow of information and coordinate the interactions between all modules, including the existing and new ones.\n",
        "\n",
        "**`ThesisAssistant` Class:**\n",
        "\n",
        "*   **Purpose:** Acts as the main controller, receiving user input, directing it to the appropriate module(s), managing the state of the thesis project, and ensuring ethical oversight through interaction with the `EthicsModule`.\n",
        "*   **Responsibilities:**\n",
        "    *   Initialize all modules (`IdeaBrainstorming`, `WritingSupport`, `EmotionSupport`, `EthicsModule`, etc.).\n",
        "    *   Receive user input and determine which module(s) should process it.\n",
        "    *   Pass relevant information between modules (e.g., generated ideas from `IdeaBrainstorming` to `WritingSupport`).\n",
        "    *   Interact with the `EthicsModule` for ethical checks, logging, and receiving intervention recommendations from the RL agent.\n",
        "    *   Manage the overall state of the user's thesis project (e.g., current stage, progress).\n",
        "    *   Present output from the modules to the user.\n",
        "    *   Potentially incorporate the RL agent's recommended action from the `EthicsSupervisorRL` (via the `EthicsModule`) to influence module behavior or provide direct interventions.\n",
        "*   **Interactions:**\n",
        "    *   `ThesisAssistant` <-> `IdeaBrainstorming`\n",
        "    *   `ThesisAssistant` <-> `WritingSupport`\n",
        "    *   `ThesisAssistant` <-> `EmotionSupport`\n",
        "    *   `ThesisAssistant` <-> `EthicsModule` (This is a crucial link. The `ThesisAssistant` would query the `EthicsModule` for ethical status and potential interventions before and after processing user requests with the other modules).\n",
        "\n",
        "This structure provides a clear separation of concerns for each module while allowing for necessary interaction and central control for ethical oversight. The `EthicsModule` acts as a cross-cutting concern, influencing and being informed by the activities of all user-facing modules."
      ],
      "metadata": {
        "id": "2lCFUMI1sDku"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0158677"
      },
      "source": [
        "## Implement the idea brainstorming module\n",
        "\n",
        "### Subtask:\n",
        "Write the Python code for this module, including functionalities for generating ideas based on user input and thesis context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "055bc45a"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the `IdeaBrainstorming` class as described in the instructions, including the `__init__`, `generate_ideas`, `explore_idea`, and `refine_ideas` methods, using the OpenAI client for interactions with a language model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f665fe1a"
      },
      "source": [
        "class IdeaBrainstorming:\n",
        "    \"\"\"\n",
        "    Assists the user in generating and exploring ideas related to their thesis topic.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, openai_client):\n",
        "        \"\"\"\n",
        "        Initializes the IdeaBrainstorming module with an OpenAI client.\n",
        "\n",
        "        Args:\n",
        "            openai_client: An initialized OpenAI client object.\n",
        "        \"\"\"\n",
        "        self.client = openai_client\n",
        "        self.model = \"gpt-4o-mini\" # Or another suitable model for idea generation\n",
        "\n",
        "    def generate_ideas(self, prompt: str, thesis_stage: str, n_ideas: int = 5):\n",
        "        \"\"\"\n",
        "        Generates a list of relevant ideas based on the user prompt and thesis stage.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): The user's input prompt or topic.\n",
        "            thesis_stage (str): The current stage of the thesis (e.g., \"planning\", \"literature review\").\n",
        "            n_ideas (int): The number of ideas to generate. Defaults to 5.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of generated ideas (strings).\n",
        "        \"\"\"\n",
        "        print(f\"Generating {n_ideas} ideas for prompt: '{prompt}' at thesis stage: '{thesis_stage}'\")\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": f\"You are a creative thesis idea brainstorming assistant. Generate {n_ideas} distinct and relevant ideas based on the user's prompt and their current thesis stage: {thesis_stage}.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=300, # Adjust max_tokens as needed\n",
        "                n=1 # We only need one completion containing the list of ideas\n",
        "            )\n",
        "            # Assuming the model returns a list of ideas, perhaps separated by newlines or numbers\n",
        "            ideas_text = response.choices[0].message.content.strip()\n",
        "            # Basic parsing: split by newline and filter empty lines\n",
        "            ideas = [idea.strip() for idea in ideas_text.split('\\n') if idea.strip()]\n",
        "            print(f\"Generated ideas: {ideas}\")\n",
        "            return ideas\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating ideas: {e}\")\n",
        "            return []\n",
        "\n",
        "    def explore_idea(self, idea: str, depth: int = 2):\n",
        "        \"\"\"\n",
        "        Generates further details, related concepts, or potential research questions for a given idea.\n",
        "\n",
        "        Args:\n",
        "            idea (str): The specific idea to explore.\n",
        "            depth (int): A parameter to influence the level of detail (e.g., 1 for brief, 2 for more detail). Defaults to 2.\n",
        "\n",
        "        Returns:\n",
        "            str: Detailed information or related concepts about the idea.\n",
        "        \"\"\"\n",
        "        print(f\"Exploring idea: '{idea}' with depth {depth}\")\n",
        "        try:\n",
        "            prompt_text = f\"Explore the following thesis idea in detail (depth level {depth}). Provide related concepts, potential research questions, and possible angles:\\n\\nIdea: {idea}\"\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a thesis idea exploration assistant. Provide detailed insights and related concepts for a given idea.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt_text}\n",
        "                ],\n",
        "                max_tokens=500 # Adjust max_tokens as needed\n",
        "            )\n",
        "            exploration_details = response.choices[0].message.content.strip()\n",
        "            print(f\"Exploration details for '{idea}': {exploration_details[:100]}...\") # Print snippet\n",
        "            return exploration_details\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error exploring idea: {e}\")\n",
        "            return \"Could not explore idea.\"\n",
        "\n",
        "    def refine_ideas(self, ideas: list, feedback: str):\n",
        "        \"\"\"\n",
        "        Refines existing ideas or generates new variations based on user feedback.\n",
        "\n",
        "        Args:\n",
        "            ideas (list): A list of existing ideas (strings).\n",
        "            feedback (str): User feedback on the ideas.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of refined or new ideas (strings).\n",
        "        \"\"\"\n",
        "        print(f\"Refining ideas based on feedback: '{feedback}'\")\n",
        "        try:\n",
        "            ideas_list_text = \"\\n\".join([f\"- {idea}\" for idea in ideas])\n",
        "            prompt_text = f\"Refine the following thesis ideas based on the user's feedback, or generate new variations that address the feedback. Provide a list of refined or new ideas.\\n\\nExisting Ideas:\\n{ideas_list_text}\\n\\nUser Feedback: {feedback}\"\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a thesis idea refinement assistant. Improve existing ideas or generate new ones based on user feedback.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt_text}\n",
        "                ],\n",
        "                max_tokens=400, # Adjust max_tokens as needed\n",
        "                n=1\n",
        "            )\n",
        "            refined_ideas_text = response.choices[0].message.content.strip()\n",
        "            # Basic parsing: split by newline and filter empty lines\n",
        "            refined_ideas = [idea.strip() for idea in refined_ideas_text.split('\\n') if idea.strip()]\n",
        "            print(f\"Refined ideas: {refined_ideas}\")\n",
        "            return refined_ideas\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error refining ideas: {e}\")\n",
        "            return []\n",
        "\n",
        "# Example Usage (requires 'client' object from a previous cell)\n",
        "# if 'client' in locals():\n",
        "#     idea_brainstorming_module = IdeaBrainstorming(openai_client=client)\n",
        "#     generated_ideas = idea_brainstorming_module.generate_ideas(\"Impact of AI on education\", \"planning\", n_ideas=3)\n",
        "#     if generated_ideas:\n",
        "#         exploration = idea_brainstorming_module.explore_idea(generated_ideas[0])\n",
        "#         print(f\"\\nExploration of first idea:\\n{exploration}\")\n",
        "#         refined = idea_brainstorming_module.refine_ideas(generated_ideas, \"These are too broad, focus on K-12.\")\n",
        "#         print(f\"\\nRefined ideas:\\n{refined}\")\n",
        "# else:\n",
        "#     print(\"OpenAI client not initialized. Please run the cell setting up the client.\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b8ea0a9"
      },
      "source": [
        "**Reasoning**:\n",
        "Test the implemented `IdeaBrainstorming` class by creating an instance and calling its methods with example inputs to verify their functionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45607d82",
        "outputId": "facad042-7f9f-4700-c300-2dfd50c68602"
      },
      "source": [
        "# Example Usage (requires 'client' object from a previous cell)\n",
        "if 'client' in locals():\n",
        "    print(\"--- Testing IdeaBrainstorming Module ---\")\n",
        "    idea_brainstorming_module = IdeaBrainstorming(openai_client=client)\n",
        "\n",
        "    print(\"\\n--- Testing generate_ideas ---\")\n",
        "    generated_ideas = idea_brainstorming_module.generate_ideas(\"Impact of climate change on coastal cities\", \"literature review\", n_ideas=3)\n",
        "    print(f\"\\nGenerated Ideas Result: {generated_ideas}\")\n",
        "\n",
        "    if generated_ideas:\n",
        "        print(\"\\n--- Testing explore_idea ---\")\n",
        "        exploration = idea_brainstorming_module.explore_idea(generated_ideas[0], depth=1)\n",
        "        print(f\"\\nExploration of first idea:\\n{exploration}\")\n",
        "\n",
        "        print(\"\\n--- Testing refine_ideas ---\")\n",
        "        feedback = \"These ideas are good, but focus more on adaptation strategies rather than just impacts.\"\n",
        "        refined_ideas = idea_brainstorming_module.refine_ideas(generated_ideas, feedback)\n",
        "        print(f\"\\nRefined Ideas Result:\\n{refined_ideas}\")\n",
        "    else:\n",
        "        print(\"\\nNo ideas were generated, skipping explore_idea and refine_ideas tests.\")\n",
        "\n",
        "else:\n",
        "    print(\"OpenAI client not initialized. Please run the cell setting up the client.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing IdeaBrainstorming Module ---\n",
            "\n",
            "--- Testing generate_ideas ---\n",
            "Generating 3 ideas for prompt: 'Impact of climate change on coastal cities' at thesis stage: 'literature review'\n",
            "Generated ideas: ['Here are three distinct thesis ideas focusing on the impact of climate change on coastal cities that can be developed further in your literature review stage:', '1. **Social Vulnerability and Resilience in Coastal Cities**: Explore how climate change disproportionately affects marginalized communities in coastal cities. The literature review could examine socio-economic factors, historical inequities, and adaptive capacities that influence resilience. It could draw on case studies of specific cities, highlighting how local policies and community initiatives address (or fail to address) these disparities in the face of rising sea levels and extreme weather events.', '2. **Urban Planning and Sustainable Development**: Investigate the role of urban planning in mitigating the impacts of climate change on coastal cities. This literature review could synthesize existing research on sustainable development practices, land-use planning, and infrastructure resilience. You could analyze frameworks from various cities to identify successful strategies and common challenges in adapting to climate risks, including the integration of green infrastructure and community participation in urban planning initiatives.', '3. **Cultural Heritage and Climate Adaptation**: Focus on the impact of climate change on the cultural heritage of coastal cities, and how these communities are adapting to protect their historical sites and cultural identities. Your literature review could assess research on preservation efforts, community engagement, and regulatory challenges. It could explore how climate adaptation strategies may need to balance modern infrastructure requirements with the preservation of cultural landmarks that define a city’s identity.', 'These ideas offer a foundation for diving into existing literature, identifying gaps, and framing your research questions for']\n",
            "\n",
            "Generated Ideas Result: ['Here are three distinct thesis ideas focusing on the impact of climate change on coastal cities that can be developed further in your literature review stage:', '1. **Social Vulnerability and Resilience in Coastal Cities**: Explore how climate change disproportionately affects marginalized communities in coastal cities. The literature review could examine socio-economic factors, historical inequities, and adaptive capacities that influence resilience. It could draw on case studies of specific cities, highlighting how local policies and community initiatives address (or fail to address) these disparities in the face of rising sea levels and extreme weather events.', '2. **Urban Planning and Sustainable Development**: Investigate the role of urban planning in mitigating the impacts of climate change on coastal cities. This literature review could synthesize existing research on sustainable development practices, land-use planning, and infrastructure resilience. You could analyze frameworks from various cities to identify successful strategies and common challenges in adapting to climate risks, including the integration of green infrastructure and community participation in urban planning initiatives.', '3. **Cultural Heritage and Climate Adaptation**: Focus on the impact of climate change on the cultural heritage of coastal cities, and how these communities are adapting to protect their historical sites and cultural identities. Your literature review could assess research on preservation efforts, community engagement, and regulatory challenges. It could explore how climate adaptation strategies may need to balance modern infrastructure requirements with the preservation of cultural landmarks that define a city’s identity.', 'These ideas offer a foundation for diving into existing literature, identifying gaps, and framing your research questions for']\n",
            "\n",
            "--- Testing explore_idea ---\n",
            "Exploring idea: 'Here are three distinct thesis ideas focusing on the impact of climate change on coastal cities that can be developed further in your literature review stage:' with depth 1\n",
            "Exploration details for 'Here are three distinct thesis ideas focusing on the impact of climate change on coastal cities that can be developed further in your literature review stage:': ### Thesis Idea Exploration: Impact of Climate Change on Coastal Cities\n",
            "\n",
            "**Overview**: The phenomeno...\n",
            "\n",
            "Exploration of first idea:\n",
            "### Thesis Idea Exploration: Impact of Climate Change on Coastal Cities\n",
            "\n",
            "**Overview**: The phenomenon of climate change presents significant challenges to coastal cities, which face threats from rising sea levels, increased storm intensity, and changing weather patterns. Understanding these impacts is crucial for urban planning, public health, and environmental sustainability. Here, we’ll explore three distinct aspects of this idea, focusing on related concepts, potential research questions, and possible analytical angles.\n",
            "\n",
            "---\n",
            "\n",
            "#### 1. **Economic Implications of Climate-Induced Relocation in Coastal Cities**\n",
            "   - **Related Concepts**:\n",
            "     - Economic displacement\n",
            "     - Resilience economics\n",
            "     - Urban planning and infrastructure adaptation\n",
            "     - Property value fluctuations\n",
            "     - Climate refugees and social equity\n",
            "\n",
            "   - **Potential Research Questions**:\n",
            "     - How do economic factors influence the decision-making process for relocating communities affected by climate change in coastal cities?\n",
            "     - What are the long-term economic impacts of climate-induced relocation on urban infrastructures in affected areas?\n",
            "     - How can coastal cities implement effective economic policies to mitigate the shock from climate change?\n",
            "\n",
            "   - **Possible Angles**:\n",
            "     - A comparative analysis of coastal cities that have successfully implemented relocation strategies versus those that have faced significant economic challenges.\n",
            "     - Assessing the role of private investment in funding climate adaptation measures in coastal cities.\n",
            "     - Evaluating the social equity implications of economic decisions made in the face of climate change.\n",
            "\n",
            "---\n",
            "\n",
            "#### 2. **Public Health Challenges in Coastal Cities Due to Climate Change**\n",
            "   - **Related Concepts**:\n",
            "     - Vector-borne diseases\n",
            "     - Mental health and climate anxiety\n",
            "     - Water quality and resource management\n",
            "     - Heat-related illnesses\n",
            "     - Emergency preparedness and response systems\n",
            "\n",
            "   - **Potential Research Questions**:\n",
            "     - How does climate change exacerbate public health threats in coastal cities, particularly regarding vector-borne diseases?\n",
            "     - What strategies are coastal cities employing to enhance public health resilience in light of climate change impacts?\n",
            "     - How does climate change affect mental health in populations living in vulnerable coastal areas?\n",
            "\n",
            "   - **Possible Angles**:\n",
            "     - Studying specific case reports or data from coastal cities that highlight the correlation between climate events and public health crises.\n",
            "     - Analyzing the effectiveness of interventions designed to improve health outcomes in response to climate-related health threats in coastal regions.\n",
            "     - Investigating the psychological impacts of climate change on communities in coastal cities, focusing on concepts such as climate anxiety.\n",
            "\n",
            "---\n",
            "\n",
            "#### 3. **Environmental Justice\n",
            "\n",
            "--- Testing refine_ideas ---\n",
            "Refining ideas based on feedback: 'These ideas are good, but focus more on adaptation strategies rather than just impacts.'\n",
            "Refined ideas: ['Based on your feedback to emphasize adaptation strategies rather than solely impacts, here are refined and new thesis ideas:', '1. **Adaptive Strategies for Social Resilience in Coastal Cities**: Investigate the adaptive strategies employed by marginalized communities in coastal cities to enhance their resilience against climate change. The literature review could explore community-led initiatives, grassroots movements, and participatory governance that empower vulnerable populations to respond effectively to climate threats. Case studies could highlight successful adaptations that bridge social justice and environmental sustainability.', '2. **Innovative Urban Planning for Climate Resilience**: Analyze innovative urban planning approaches that integrate climate adaptation strategies for coastal cities. This literature review could evaluate case studies showcasing successful urban design, zoning reforms, and the implementation of climate-resilient infrastructure. It could also highlight how urban planners are incorporating future climate scenarios into long-term development goals, emphasizing public-private partnerships and stakeholder engagement in resilience-building.', '3. **Cultural Heritage Preservation through Climate Adaptation**: Explore how coastal cities are implementing adaptation strategies that protect cultural heritage while addressing climate risks. This literature review could focus on multi-disciplinary approaches that combine heritage conservation with climate resilience planning. It may investigate innovative methods of engaging local communities in adaptive practices, including climate-responsive architecture, heritage tourism adjustments, and the integration of cultural considerations into broader disaster risk reduction plans.', '4. **Community-Led Climate Adaptation Initiatives**: Examine case studies of coastal cities where community-led adaptation initiatives have significantly improved local resilience to climate change. This literature review could analyze the effectiveness of these grassroots approaches, including the role of education, community organizing, and access to resources in fostering adaptive capacity. The exploration of successful collaborations between local governments and residents could yield valuable insights into scalable models for other regions.', '5. **Technology and Climate Adaptation in Coastal Urban Contexts**: Investigate the role of emerging technologies in facilitating climate adaptation strategies in coastal cities. The literature review could evaluate how data analytics, GIS mapping, and smart infrastructure contribute to enhanced urban resilience. Case studies could focus']\n",
            "\n",
            "Refined Ideas Result:\n",
            "['Based on your feedback to emphasize adaptation strategies rather than solely impacts, here are refined and new thesis ideas:', '1. **Adaptive Strategies for Social Resilience in Coastal Cities**: Investigate the adaptive strategies employed by marginalized communities in coastal cities to enhance their resilience against climate change. The literature review could explore community-led initiatives, grassroots movements, and participatory governance that empower vulnerable populations to respond effectively to climate threats. Case studies could highlight successful adaptations that bridge social justice and environmental sustainability.', '2. **Innovative Urban Planning for Climate Resilience**: Analyze innovative urban planning approaches that integrate climate adaptation strategies for coastal cities. This literature review could evaluate case studies showcasing successful urban design, zoning reforms, and the implementation of climate-resilient infrastructure. It could also highlight how urban planners are incorporating future climate scenarios into long-term development goals, emphasizing public-private partnerships and stakeholder engagement in resilience-building.', '3. **Cultural Heritage Preservation through Climate Adaptation**: Explore how coastal cities are implementing adaptation strategies that protect cultural heritage while addressing climate risks. This literature review could focus on multi-disciplinary approaches that combine heritage conservation with climate resilience planning. It may investigate innovative methods of engaging local communities in adaptive practices, including climate-responsive architecture, heritage tourism adjustments, and the integration of cultural considerations into broader disaster risk reduction plans.', '4. **Community-Led Climate Adaptation Initiatives**: Examine case studies of coastal cities where community-led adaptation initiatives have significantly improved local resilience to climate change. This literature review could analyze the effectiveness of these grassroots approaches, including the role of education, community organizing, and access to resources in fostering adaptive capacity. The exploration of successful collaborations between local governments and residents could yield valuable insights into scalable models for other regions.', '5. **Technology and Climate Adaptation in Coastal Urban Contexts**: Investigate the role of emerging technologies in facilitating climate adaptation strategies in coastal cities. The literature review could evaluate how data analytics, GIS mapping, and smart infrastructure contribute to enhanced urban resilience. Case studies could focus']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f6f3fdb"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous test failed because the OpenAI client object was not available. I need to ensure the client is initialized before attempting to use it. I will re-run the cell that initializes the OpenAI client and then re-run the test code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80c9f818",
        "outputId": "761f78be-1334-4e1b-f440-c191c63e8020"
      },
      "source": [
        "# Import the OpenAI library\n",
        "from openai import OpenAI\n",
        "# Used to securely store your API key - uncomment if using Colab Secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get your OpenAI API key securely\n",
        "# Replace \"<YOUR_OPENAI_API_KEY>\" with your key, or use Colab Secrets\n",
        "# Or if using Colab Secrets:\n",
        "openai_api_key_secure = userdata.get('OPENAI_API_KEY')\n",
        "openai_organization = userdata.get('OPENAI_ORGANIZATION')\n",
        "openai_project = userdata.get('OPENAI_PROJECT_ID')\n",
        "\n",
        "# Set your project API key\n",
        "OpenAI.api_key = openai_api_key_secure\n",
        "# You must also set organization and project ID\n",
        "OpenAI.organization = openai_organization\n",
        "OpenAI.project = openai_project\n",
        "\n",
        "# Create the OpenAI client\n",
        "client = OpenAI(api_key= OpenAI.api_key)\n",
        "\n",
        "# Test the client connection (optional)\n",
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "      model=\"gpt-4o-mini\",\n",
        "      messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "      max_tokens=10\n",
        "    )\n",
        "    print(\"OpenAI client initialized and connected successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing or connecting to OpenAI client: {e}\")\n",
        "\n",
        "\n",
        "# Example Usage (requires 'client' object from a previous cell)\n",
        "if 'client' in locals():\n",
        "    print(\"--- Testing IdeaBrainstorming Module ---\")\n",
        "    idea_brainstorming_module = IdeaBrainstorming(openai_client=client)\n",
        "\n",
        "    print(\"\\n--- Testing generate_ideas ---\")\n",
        "    generated_ideas = idea_brainstorming_module.generate_ideas(\"Impact of climate change on coastal cities\", \"literature review\", n_ideas=3)\n",
        "    print(f\"\\nGenerated Ideas Result: {generated_ideas}\")\n",
        "\n",
        "    if generated_ideas:\n",
        "        print(\"\\n--- Testing explore_idea ---\")\n",
        "        exploration = idea_brainstorming_module.explore_idea(generated_ideas[0], depth=1)\n",
        "        print(f\"\\nExploration of first idea:\\n{exploration}\")\n",
        "\n",
        "        print(\"\\n--- Testing refine_ideas ---\")\n",
        "        feedback = \"These ideas are good, but focus more on adaptation strategies rather than just impacts.\"\n",
        "        refined_ideas = idea_brainstorming_module.refine_ideas(generated_ideas, feedback)\n",
        "        print(f\"\\nRefined Ideas Result:\\n{refined_ideas}\")\n",
        "    else:\n",
        "        print(\"\\nNo ideas were generated, skipping explore_idea and refine_ideas tests.\")\n",
        "\n",
        "else:\n",
        "    print(\"OpenAI client not initialized after attempt.\")\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI client initialized and connected successfully.\n",
            "--- Testing IdeaBrainstorming Module ---\n",
            "\n",
            "--- Testing generate_ideas ---\n",
            "Generating 3 ideas for prompt: 'Impact of climate change on coastal cities' at thesis stage: 'literature review'\n",
            "Generated ideas: ['Here are three distinct ideas you could explore for your thesis related to the impact of climate change on coastal cities, focusing on various aspects reflected in your literature review:', '1. **Urban Resilience Strategies in Coastal Cities**: Conduct a comprehensive review of existing literature on urban resilience strategies adopted by coastal cities in response to climate change. Investigate how different cities are implementing adaptive measures such as green infrastructure, seawalls, and sustainable urban planning. Analyze the effectiveness of these strategies by assessing notable case studies and highlighting best practices that can serve as models for other cities facing similar challenges.', '2. **Socioeconomic Impacts of Climate Change on Vulnerable Populations**: Review the literature focusing on the socioeconomic implications of climate change for marginalized communities in coastal cities. Explore themes such as displacement, economic vulnerability, and access to resources amidst rising sea levels and extreme weather events. Highlight the intersectionality of race, class, and geography in shaping these vulnerabilities, and propose frameworks for equitable policy responses that address both environmental and social justice concerns.', '3. **Cultural Heritage and Identity in the Face of Climate Change**: Investigate the impact of climate change on the cultural heritage and identity of coastal cities, drawing on literature that covers historical sites, indigenous communities, and architectural heritage at risk due to rising tides and erosion. Assess how climate change affects local narratives and identities, and explore potential preservation strategies that can safeguard cultural sites while promoting community resilience. This could also include a discussion of how cultural heritage can play a role in']\n",
            "\n",
            "Generated Ideas Result: ['Here are three distinct ideas you could explore for your thesis related to the impact of climate change on coastal cities, focusing on various aspects reflected in your literature review:', '1. **Urban Resilience Strategies in Coastal Cities**: Conduct a comprehensive review of existing literature on urban resilience strategies adopted by coastal cities in response to climate change. Investigate how different cities are implementing adaptive measures such as green infrastructure, seawalls, and sustainable urban planning. Analyze the effectiveness of these strategies by assessing notable case studies and highlighting best practices that can serve as models for other cities facing similar challenges.', '2. **Socioeconomic Impacts of Climate Change on Vulnerable Populations**: Review the literature focusing on the socioeconomic implications of climate change for marginalized communities in coastal cities. Explore themes such as displacement, economic vulnerability, and access to resources amidst rising sea levels and extreme weather events. Highlight the intersectionality of race, class, and geography in shaping these vulnerabilities, and propose frameworks for equitable policy responses that address both environmental and social justice concerns.', '3. **Cultural Heritage and Identity in the Face of Climate Change**: Investigate the impact of climate change on the cultural heritage and identity of coastal cities, drawing on literature that covers historical sites, indigenous communities, and architectural heritage at risk due to rising tides and erosion. Assess how climate change affects local narratives and identities, and explore potential preservation strategies that can safeguard cultural sites while promoting community resilience. This could also include a discussion of how cultural heritage can play a role in']\n",
            "\n",
            "--- Testing explore_idea ---\n",
            "Exploring idea: 'Here are three distinct ideas you could explore for your thesis related to the impact of climate change on coastal cities, focusing on various aspects reflected in your literature review:' with depth 1\n",
            "Exploration details for 'Here are three distinct ideas you could explore for your thesis related to the impact of climate change on coastal cities, focusing on various aspects reflected in your literature review:': Certainly! Here’s an exploration of the thesis idea related to the impact of climate change on coast...\n",
            "\n",
            "Exploration of first idea:\n",
            "Certainly! Here’s an exploration of the thesis idea related to the impact of climate change on coastal cities, broken down into sub-ideas with potential research questions and angles:\n",
            "\n",
            "### 1. **Urban Resilience and Adaptation Strategies**\n",
            "   - **Concepts:**\n",
            "     - Urban resilience refers to the capacity of cities to absorb, recover, and adapt to climate-related shocks and stresses.\n",
            "     - Adaptation strategies can include infrastructure improvements, policy changes, community engagement, and ecosystem-based approaches.\n",
            "   - **Potential Research Questions:**\n",
            "     - What specific adaptation strategies are most effective for enhancing resilience in coastal cities?\n",
            "     - How do social equity considerations influence the development of urban resilience plans in different coastal communities?\n",
            "     - In what ways do public-private partnerships contribute to the implementation of adaptive measures in coastal urban areas?\n",
            "   - **Possible Angles:**\n",
            "     - Case studies of cities that have successfully implemented adaptation strategies.\n",
            "     - Comparative analysis of different coastal cities’ vulnerability levels and resilience approaches.\n",
            "     - The role of community native knowledge and grassroots organizations in shaping urban adaptation initiatives.\n",
            "\n",
            "### 2. **Socioeconomic Impacts on Coastal Populations**\n",
            "   - **Concepts:**\n",
            "     - Climate change disproportionately affects marginalized communities, exacerbating preexisting socioeconomic inequalities.\n",
            "     - Factors such as housing, displacement, health, and access to resources must be examined in the context of climate impacts.\n",
            "   - **Potential Research Questions:**\n",
            "     - How does climate change-induced displacement affect housing markets and economic stability in coastal cities?\n",
            "     - What are the health implications for coastal populations as a result of climate-related stressors such as flooding and heatwaves?\n",
            "     - How do different demographic groups within coastal urban areas perceive and respond to the risks associated with climate change?\n",
            "   - **Possible Angles:**\n",
            "     - Analysis of specific coastal cities that have faced significant socioeconomic challenges due to climate impacts.\n",
            "     - Surveys or interviews with affected communities to better understand their views and adaptive strategies.\n",
            "     - Examination of policy frameworks aimed at protecting vulnerable populations in the face of climate change.\n",
            "\n",
            "### 3. **Ecosystem Services and Biodiversity Loss**\n",
            "   - **Concepts:**\n",
            "     - Coastal ecosystems (e.g., wetlands, mangroves, coral reefs) provide essential services such as flood protection, carbon sequestration, and habitat for biodiversity.\n",
            "     - Climate change threatens these ecosystems through sea-level rise, erosion, and temperature changes, affecting both people and wildlife.\n",
            "   - **Potential Research Questions:**\n",
            "\n",
            "--- Testing refine_ideas ---\n",
            "Refining ideas based on feedback: 'These ideas are good, but focus more on adaptation strategies rather than just impacts.'\n",
            "Refined ideas: ['Based on your feedback to emphasize adaptation strategies, here are refined or new variations of the original thesis ideas:', '1. **Adaptive Urban Resilience Strategies in Coastal Cities**: Expand the original idea to not only review existing resilience strategies but also to assess how effectively these cities are adapting to ongoing climate changes. Highlight innovative case studies that showcase adaptive measures—such as the development of multifunctional green spaces, adaptive building codes, and community-led initiatives. Examine the role of local governments and community organizations in fostering adaptation and propose a framework for cities to enhance their resilience further.', '2. **Empowering Vulnerable Communities through Adaptation Policies**: Shift the focus towards how coastal cities can develop inclusive adaptation policies that empower marginalized populations. Analyze successful case studies where local governments have implemented participatory approaches to engaging vulnerable communities in adaptation planning. Discuss potential mechanisms for integrating social equity into adaptation strategies, emphasizing the importance of building adaptive capacities in these populations while addressing climate-related socioeconomic challenges.', '3. **Cultural Heritage Adaptation in the Face of Climate Change**: Instead of solely investigating the impacts on cultural heritage, this idea would focus on adaptive strategies that have been employed by coastal cities to protect their cultural identity in the context of climate change. Explore how communities are adapting their preservation efforts, including the use of technology in site monitoring and engaging local artisans in resilience planning. Discuss co-creation of strategies that maintain cultural significance while allowing for adjustments to the changing environment.', '4. **Water Management Adaptation Strategies in Coastal Cities**: Focus on innovative water management strategies that coastal cities are implementing to adapt to climate-related challenges such as flooding and saltwater intrusion. Investigate the role of stormwater management systems, aquifer recharge initiatives, and the use of nature-based solutions. Evaluate case studies of cities that have successfully improved their water resilience and discuss lessons learned that could inform future adaptation efforts.', '5. **Community-Driven Coastal Adaptation Initiatives**: Emphasize grassroots approaches to adaptation in coastal cities. This thesis could explore how']\n",
            "\n",
            "Refined Ideas Result:\n",
            "['Based on your feedback to emphasize adaptation strategies, here are refined or new variations of the original thesis ideas:', '1. **Adaptive Urban Resilience Strategies in Coastal Cities**: Expand the original idea to not only review existing resilience strategies but also to assess how effectively these cities are adapting to ongoing climate changes. Highlight innovative case studies that showcase adaptive measures—such as the development of multifunctional green spaces, adaptive building codes, and community-led initiatives. Examine the role of local governments and community organizations in fostering adaptation and propose a framework for cities to enhance their resilience further.', '2. **Empowering Vulnerable Communities through Adaptation Policies**: Shift the focus towards how coastal cities can develop inclusive adaptation policies that empower marginalized populations. Analyze successful case studies where local governments have implemented participatory approaches to engaging vulnerable communities in adaptation planning. Discuss potential mechanisms for integrating social equity into adaptation strategies, emphasizing the importance of building adaptive capacities in these populations while addressing climate-related socioeconomic challenges.', '3. **Cultural Heritage Adaptation in the Face of Climate Change**: Instead of solely investigating the impacts on cultural heritage, this idea would focus on adaptive strategies that have been employed by coastal cities to protect their cultural identity in the context of climate change. Explore how communities are adapting their preservation efforts, including the use of technology in site monitoring and engaging local artisans in resilience planning. Discuss co-creation of strategies that maintain cultural significance while allowing for adjustments to the changing environment.', '4. **Water Management Adaptation Strategies in Coastal Cities**: Focus on innovative water management strategies that coastal cities are implementing to adapt to climate-related challenges such as flooding and saltwater intrusion. Investigate the role of stormwater management systems, aquifer recharge initiatives, and the use of nature-based solutions. Evaluate case studies of cities that have successfully improved their water resilience and discuss lessons learned that could inform future adaptation efforts.', '5. **Community-Driven Coastal Adaptation Initiatives**: Emphasize grassroots approaches to adaptation in coastal cities. This thesis could explore how']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbb28d03"
      },
      "source": [
        "## Implement the writing support module\n",
        "\n",
        "### Subtask:\n",
        "Write the Python code for this module, focusing on features like outlining, drafting, and refining text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ae9da35"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the `WritingSupport` class with methods for outlining, drafting, summarizing, rephrasing, and checking grammar/style using the OpenAI client, and include example usage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "facd1f63",
        "outputId": "03a9fa6e-edb3-4333-f033-e8631175e54e"
      },
      "source": [
        "class WritingSupport:\n",
        "    \"\"\"\n",
        "    Provides assistance with writing tasks, including outlining, drafting,\n",
        "    summarizing, rephrasing, and checking text.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, openai_client):\n",
        "        \"\"\"\n",
        "        Initializes the WritingSupport module with an OpenAI client.\n",
        "\n",
        "        Args:\n",
        "            openai_client: An initialized OpenAI client object.\n",
        "        \"\"\"\n",
        "        self.client = openai_client\n",
        "        self.model = \"gpt-4o-mini\" # Default model for writing tasks\n",
        "\n",
        "    def create_outline(self, topic: str, thesis_stage: str):\n",
        "        \"\"\"\n",
        "        Generates a structured outline for a given topic and thesis stage.\n",
        "\n",
        "        Args:\n",
        "            topic (str): The topic for the outline.\n",
        "            thesis_stage (str): The current stage of the thesis\n",
        "                                (e.g., \"introduction\", \"literature review\").\n",
        "\n",
        "        Returns:\n",
        "            str: A generated outline as a string.\n",
        "        \"\"\"\n",
        "        print(f\"Generating outline for topic: '{topic}' at thesis stage: '{thesis_stage}'\")\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": f\"You are a thesis writing assistant. Create a structured outline for a thesis section on the topic '{topic}' relevant to the '{thesis_stage}' stage.\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Create an outline for: {topic}\"}\n",
        "                ],\n",
        "                max_tokens=400,\n",
        "                n=1\n",
        "            )\n",
        "            outline = response.choices[0].message.content.strip()\n",
        "            print(f\"Generated outline: {outline[:100]}...\") # Print snippet\n",
        "            return outline\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating outline: {e}\")\n",
        "            return \"Could not generate outline.\"\n",
        "\n",
        "    def draft_section(self, outline: str, context: str):\n",
        "        \"\"\"\n",
        "        Drafts a section of text based on an outline and relevant context.\n",
        "\n",
        "        Args:\n",
        "            outline (str): The outline or part of the outline for the section.\n",
        "            context (str): Relevant context or notes to inform the drafting.\n",
        "\n",
        "        Returns:\n",
        "            str: A drafted section of text.\n",
        "        \"\"\"\n",
        "        print(f\"Drafting section based on outline: '{outline[:50]}...' and context: '{context[:50]}...'\")\n",
        "        try:\n",
        "            prompt_text = f\"Write a draft section of a thesis based on the following outline and context.\\n\\nOutline:\\n{outline}\\n\\nContext:\\n{context}\\n\\nDraft:\"\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a thesis writing assistant. Draft a section of text based on an outline and provided context.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt_text}\n",
        "                ],\n",
        "                max_tokens=800\n",
        "            )\n",
        "            draft = response.choices[0].message.content.strip()\n",
        "            print(f\"Drafted section: {draft[:100]}...\") # Print snippet\n",
        "            return draft\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error drafting section: {e}\")\n",
        "            return \"Could not draft section.\"\n",
        "\n",
        "    def summarize_text(self, text: str):\n",
        "        \"\"\"\n",
        "        Generates a concise summary of a given text.\n",
        "\n",
        "        Args:\n",
        "            text (str): The text to summarize.\n",
        "\n",
        "        Returns:\n",
        "            str: A concise summary of the text.\n",
        "        \"\"\"\n",
        "        print(f\"Summarizing text: '{text[:100]}...'\")\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a text summarization assistant. Summarize the following text concisely.\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Summarize this text:\\n{text}\"}\n",
        "                ],\n",
        "                max_tokens=150\n",
        "            )\n",
        "            summary = response.choices[0].message.content.strip()\n",
        "            print(f\"Summary: {summary[:100]}...\") # Print snippet\n",
        "            return summary\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error summarizing text: {e}\")\n",
        "            return \"Could not summarize text.\"\n",
        "\n",
        "    def rephrase_text(self, text: str, style: str = \"academic and clear\"):\n",
        "        \"\"\"\n",
        "        Rewrites a given text in a different style or to improve clarity.\n",
        "\n",
        "        Args:\n",
        "            text (str): The text to rephrase.\n",
        "            style (str): The desired writing style (e.g., \"concise\", \"formal\",\n",
        "                         \"academic and clear\"). Defaults to \"academic and clear\".\n",
        "\n",
        "        Returns:\n",
        "            str: The rephrased text.\n",
        "        \"\"\"\n",
        "        print(f\"Rephrasing text: '{text[:100]}...' in style: '{style}'\")\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": f\"You are a text rephrasing assistant. Rephrase the following text in a {style} style, improving clarity if possible.\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Rephrase this text:\\n{text}\"}\n",
        "                ],\n",
        "                max_tokens=300\n",
        "            )\n",
        "            rephrased_text = response.choices[0].message.content.strip()\n",
        "            print(f\"Rephrased text: {rephrased_text[:100]}...\") # Print snippet\n",
        "            return rephrased_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error rephrasing text: {e}\")\n",
        "            return \"Could not rephrase text.\"\n",
        "\n",
        "    def check_grammar_style(self, text: str):\n",
        "        \"\"\"\n",
        "        Provides feedback on grammar, spelling, and writing style for a given text.\n",
        "\n",
        "        Args:\n",
        "            text (str): The text to check.\n",
        "\n",
        "        Returns:\n",
        "            str: Feedback on grammar, spelling, and style.\n",
        "        \"\"\"\n",
        "        print(f\"Checking grammar and style for text: '{text[:100]}...'\")\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a grammar and style checker. Provide feedback on the following text, focusing on grammar, spelling, punctuation, clarity, and academic writing style.\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Check this text for grammar and style:\\n{text}\"}\n",
        "                ],\n",
        "                max_tokens=500\n",
        "            )\n",
        "            feedback = response.choices[0].message.content.strip()\n",
        "            print(f\"Grammar and style feedback: {feedback[:100]}...\") # Print snippet\n",
        "            return feedback\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error checking grammar and style: {e}\")\n",
        "            return \"Could not check grammar and style.\"\n",
        "\n",
        "# Example Usage (requires 'client' object from a previous cell)\n",
        "if 'client' in locals():\n",
        "    print(\"\\n--- Testing WritingSupport Module ---\")\n",
        "    writing_support_module = WritingSupport(openai_client=client)\n",
        "\n",
        "    print(\"\\n--- Testing create_outline ---\")\n",
        "    outline = writing_support_module.create_outline(\"Methodology Section\", \"methodology\")\n",
        "    print(f\"\\nOutline Result:\\n{outline}\")\n",
        "\n",
        "    print(\"\\n--- Testing draft_section ---\")\n",
        "    context = \"This section should describe the qualitative data collection methods, specifically semi-structured interviews with participants.\"\n",
        "    draft = writing_support_module.draft_section(outline, context)\n",
        "    print(f\"\\nDraft Section Result:\\n{draft}\")\n",
        "\n",
        "    print(\"\\n--- Testing summarize_text ---\")\n",
        "    text_to_summarize = \"The quick brown fox jumps over the lazy dog. This is a classic pangram used to test typewriters and computer keyboards. It contains all the letters of the English alphabet. It's often used for demonstrating fonts or testing equipment. It doesn't have any deep meaning beyond this purpose.\"\n",
        "    summary = writing_support_module.summarize_text(text_to_summarize)\n",
        "    print(f\"\\nSummarize Text Result:\\n{summary}\")\n",
        "\n",
        "    print(\"\\n--- Testing rephrase_text ---\")\n",
        "    text_to_rephrase = \"The study showed that the results were very good and important.\"\n",
        "    rephrased = writing_support_module.rephrase_text(text_to_rephrase, style=\"formal academic\")\n",
        "    print(f\"\\nRephrase Text Result:\\n{rephrased}\")\n",
        "\n",
        "    print(\"\\n--- Testing check_grammar_style ---\")\n",
        "    text_to_check = \"The research was done, the data was collected, and analyse was finished. I think the finding are interesting.\"\n",
        "    feedback = writing_support_module.check_grammar_style(text_to_check)\n",
        "    print(f\"\\nCheck Grammar/Style Result:\\n{feedback}\")\n",
        "\n",
        "else:\n",
        "    print(\"OpenAI client not initialized. Please run the cell setting up the client.\")\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing WritingSupport Module ---\n",
            "\n",
            "--- Testing create_outline ---\n",
            "Generating outline for topic: 'Methodology Section' at thesis stage: 'methodology'\n",
            "Generated outline: # Thesis Outline: Methodology Section\n",
            "\n",
            "## I. Introduction to the Methodology Section\n",
            "   A. Purpose o...\n",
            "\n",
            "Outline Result:\n",
            "# Thesis Outline: Methodology Section\n",
            "\n",
            "## I. Introduction to the Methodology Section\n",
            "   A. Purpose of the Methodology Section\n",
            "   B. Importance of methodological rigor in research\n",
            "   C. Overview of the main components of the methodology\n",
            "\n",
            "## II. Research Design\n",
            "   A. Definition and explanation of research design\n",
            "   B. Types of research designs\n",
            "      1. Quantitative\n",
            "         a. Experimental\n",
            "         b. Quasi-experimental\n",
            "      2. Qualitative\n",
            "         a. Case study\n",
            "         b. Ethnographic\n",
            "         c. Phenomenological\n",
            "      3. Mixed-methods\n",
            "   C. Justification for the chosen research design\n",
            "\n",
            "## III. Research Setting\n",
            "   A. Description of the research context\n",
            "   B. Rationale for selecting the specific setting\n",
            "   C. Consideration of ethical and logistical factors \n",
            "\n",
            "## IV. Sample Selection\n",
            "   A. Target population\n",
            "   B. Sampling method \n",
            "      1. Probability sampling\n",
            "      2. Non-probability sampling\n",
            "   C. Sample size determination\n",
            "   D. Recruitment procedures\n",
            "   E. Inclusion and exclusion criteria\n",
            "\n",
            "## V. Data Collection Methods\n",
            "   A. Overview of data collection techniques\n",
            "   B. Quantitative methods\n",
            "      1. Surveys\n",
            "      2. Experiments\n",
            "   C. Qualitative methods\n",
            "      1. Interviews\n",
            "      2. Focus groups\n",
            "      3. Observations\n",
            "   D. Instrumentation\n",
            "      1. Development of tools and instruments\n",
            "      2. Reliability and validity testing of instruments\n",
            "\n",
            "## VI. Data Analysis Procedures\n",
            "   A. Quantitative data analysis\n",
            "      1. Statistical techniques\n",
            "      2. Software used (e.g., SPSS, R)\n",
            "   B. Qualitative data analysis\n",
            "      1. Thematic analysis\n",
            "      2. Coding procedures\n",
            "      3. Software used (e.g., NVivo, Dedoose)\n",
            "\n",
            "--- Testing draft_section ---\n",
            "Drafting section based on outline: '# Thesis Outline: Methodology Section\n",
            "\n",
            "## I. Intro...' and context: 'This section should describe the qualitative data ...'\n",
            "Drafted section: ## V. Data Collection Methods\n",
            "\n",
            "### Overview of Data Collection Techniques\n",
            "In this study, a comprehen...\n",
            "\n",
            "Draft Section Result:\n",
            "## V. Data Collection Methods\n",
            "\n",
            "### Overview of Data Collection Techniques\n",
            "In this study, a comprehensive approach to data collection was employed to ensure a nuanced understanding of the research question. Given the exploratory nature of the inquiry, both quantitative and qualitative methods were utilized; however, this section will primarily focus on the qualitative data collection methods—specifically, the use of semi-structured interviews. This method allowed for in-depth exploration of participants’ experiences, perceptions, and insights in a flexible yet guided manner.\n",
            "\n",
            "### Qualitative Methods: Semi-Structured Interviews\n",
            "Semi-structured interviews were selected as the primary qualitative method for gathering data in this research due to their ability to elicit rich, detailed responses while still maintaining a degree of structure that allows for comparability across interviews. This method enabled the researcher to ask specific questions while also encouraging participants to elaborate on their responses, providing opportunities for unexpected themes to emerge. The interviews were conducted in a conversational style, allowing participants to express their thoughts freely while guiding the discussion towards the core areas of interest outlined in the interview protocol.\n",
            "\n",
            "The interview protocol was carefully developed, consisting of open-ended questions that aligned with the research objectives. These questions were designed to probe into participants' lived experiences related to the central themes of the study. The flexibility of semi-structured interviews allowed the researcher to explore additional questions that arose during the conversation, further enriching the data collected.\n",
            "\n",
            "### Recruitment Procedures and Participant Selection\n",
            "Participants for the interviews were recruited purposefully to ensure a diverse representation of perspectives. This purposive sampling approach focused on individuals who have direct experience relevant to the study's objectives. Potential participants were identified through networks and recommendations, as well as through outreach efforts, such as informational flyers and social media announcements within relevant communities.\n",
            "\n",
            "Prior to participation, informed consent was obtained, ensuring that participants understood the study's purpose, their right to withdraw, and the measures taken to maintain confidentiality. This emphasis on ethical considerations not only fostered trust between the researcher and participants but also adhered to the principles of ethical research conduct.\n",
            "\n",
            "### Instrumentation: Development and Testing\n",
            "To ensure the reliability and validity of the semi-structured interview questions, a pilot test was conducted with a small sample of individuals who shared characteristics with the target population. Feedback from this pilot study led to necessary adjustments in the wording of questions and the overall flow of the interview protocol. Additionally, the main interview questions were aligned with the theoretical framework of the study, ensuring they resonated with prior research findings while capturing the unique context of the participants’ experiences.\n",
            "\n",
            "### Conclusion\n",
            "The method of semi-structured interviews was deemed highly appropriate for this research, enabling a comprehensive exploration of participants' insights while remaining structured enough to facilitate meaningful comparison across interviews. Through careful planning and ethical considerations, this qualitative approach provided a robust means of gathering data that are both rich and relevant to the research question at hand. The data collected from these interviews will form the foundation for a deeper analysis in the subsequent sections of the methodology, allowing for the drawing of significant conclusions and insights related to the study's aims.\n",
            "\n",
            "--- Testing summarize_text ---\n",
            "Summarizing text: 'The quick brown fox jumps over the lazy dog. This is a classic pangram used to test typewriters and ...'\n",
            "Summary: The quick brown fox jumps over the lazy dog is a classic pangram that includes all English alphabet ...\n",
            "\n",
            "Summarize Text Result:\n",
            "The quick brown fox jumps over the lazy dog is a classic pangram that includes all English alphabet letters, commonly used to test typewriters, keyboards, and demonstrate fonts, with no deeper meaning.\n",
            "\n",
            "--- Testing rephrase_text ---\n",
            "Rephrasing text: 'The study showed that the results were very good and important....' in style: 'formal academic'\n",
            "Rephrased text: The study indicated that the findings were both significant and impactful....\n",
            "\n",
            "Rephrase Text Result:\n",
            "The study indicated that the findings were both significant and impactful.\n",
            "\n",
            "--- Testing check_grammar_style ---\n",
            "Checking grammar and style for text: 'The research was done, the data was collected, and analyse was finished. I think the finding are int...'\n",
            "Grammar and style feedback: The text contains a few grammatical errors and issues related to clarity and style. Here’s a revised...\n",
            "\n",
            "Check Grammar/Style Result:\n",
            "The text contains a few grammatical errors and issues related to clarity and style. Here’s a revised version along with feedback:\n",
            "\n",
            "**Revised Text:**\n",
            "The research was conducted, the data were collected, and the analysis was completed. I believe the findings are interesting.\n",
            "\n",
            "**Feedback:**\n",
            "\n",
            "1. **Verb Usage:** \n",
            "   - \"done\" should be replaced with \"conducted\" for a more formal academic tone.\n",
            "   - \"analyse\" should be changed to \"analysis,\" as \"analyse\" is the British spelling of the verb, while \"analysis\" is the noun form.\n",
            "\n",
            "2. **Agreement in Number:**\n",
            "   - \"data was\" should be replaced with \"data were\" because \"data\" is a plural noun in formal writing.\n",
            "\n",
            "3. **Clarity and Precision:** \n",
            "   - \"finished\" can be replaced with \"completed\" for greater clarity and formality.\n",
            "   - \"I think\" should be altered to \"I believe\" to enhance the academic tone.\n",
            "\n",
            "4. **Spelling:** \n",
            "   - \"finding\" should be corrected to \"findings\" to indicate multiple results from the research.\n",
            "\n",
            "Overall, clarity and formality are improved by making these adjustments, which is essential in academic writing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aece17b0"
      },
      "source": [
        "## Implement the emotion support module\n",
        "\n",
        "### Subtask:\n",
        "Write the Python code for this module, incorporating functionalities for recognizing and responding to the user's emotional state.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ada2549"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the `EmotionSupport` class with the specified methods for emotion detection, encouragement, break suggestion, and milestone celebration, using the OpenAI client for interactions, and include example usage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0de204bb",
        "outputId": "fafe601f-f9f5-43b9-de83-e03268723426"
      },
      "source": [
        "class EmotionSupport:\n",
        "    \"\"\"\n",
        "    Offers emotional encouragement and support to the user based on their\n",
        "    interactions and detected sentiment.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, openai_client):\n",
        "        \"\"\"\n",
        "        Initializes the EmotionSupport module with an OpenAI client.\n",
        "\n",
        "        Args:\n",
        "            openai_client: An initialized OpenAI client object.\n",
        "        \"\"\"\n",
        "        self.client = openai_client\n",
        "        self.model = \"gpt-4o-mini\" # Default model for emotional support tasks\n",
        "\n",
        "    def detect_emotion(self, user_input: str):\n",
        "        \"\"\"\n",
        "        Analyzes user input to detect underlying emotions or sentiment.\n",
        "\n",
        "        Args:\n",
        "            user_input (str): The user's input string.\n",
        "\n",
        "        Returns:\n",
        "            str: A string or categorized representation of the detected emotion.\n",
        "        \"\"\"\n",
        "        print(f\"Detecting emotion for input: '{user_input[:100]}...'\")\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an emotion detection assistant. Analyze the following user input and identify the most prominent emotional state. Respond with a single word or short phrase describing the emotion (e.g., 'Neutral', 'Frustrated', 'Motivated', 'Overwhelmed', 'Happy', 'Confused').\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Analyze the emotion in this text: {user_input}\"}\n",
        "                ],\n",
        "                max_tokens=10\n",
        "            )\n",
        "            detected_emotion = response.choices[0].message.content.strip()\n",
        "            print(f\"Detected emotion: {detected_emotion}\")\n",
        "            return detected_emotion\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error detecting emotion: {e}\")\n",
        "            return \"Undetermined\"\n",
        "\n",
        "    def provide_encouragement(self, detected_emotion: str, thesis_stage: str = \"general\"):\n",
        "        \"\"\"\n",
        "        Provides tailored encouraging messages based on the detected emotion and thesis stage.\n",
        "\n",
        "        Args:\n",
        "            detected_emotion (str): The detected emotional state.\n",
        "            thesis_stage (str): The current stage of the thesis (optional). Defaults to \"general\".\n",
        "\n",
        "        Returns:\n",
        "            str: A tailored encouraging message.\n",
        "        \"\"\"\n",
        "        print(f\"Providing encouragement for emotion: '{detected_emotion}' at stage: '{thesis_stage}'\")\n",
        "        try:\n",
        "            prompt_text = f\"The user is currently feeling {detected_emotion} and is in the '{thesis_stage}' stage of their thesis. Provide a brief and encouraging message tailored to their emotional state and thesis progress.\"\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a supportive thesis assistant. Provide encouraging messages.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt_text}\n",
        "                ],\n",
        "                max_tokens=50\n",
        "            )\n",
        "            encouraging_message = response.choices[0].message.content.strip()\n",
        "            print(f\"Encouraging message: {encouraging_message}\")\n",
        "            return encouraging_message\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error providing encouragement: {e}\")\n",
        "            return \"Keep up the great work!\"\n",
        "\n",
        "    def suggest_break(self, needs_break_flag: bool):\n",
        "        \"\"\"\n",
        "        Generates a suggestion for the user to take a break based on criteria.\n",
        "\n",
        "        Args:\n",
        "            needs_break_flag (bool): A flag indicating if a break is suggested.\n",
        "\n",
        "        Returns:\n",
        "            str: A suggestion message or an empty string if no break is needed.\n",
        "        \"\"\"\n",
        "        if not needs_break_flag:\n",
        "            return \"\"\n",
        "\n",
        "        print(\"Suggesting a break...\")\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful thesis assistant. Suggest taking a short break to recharge.\"},\n",
        "                    {\"role\": \"user\", \"content\": \"Suggest that the user takes a short break.\"}\n",
        "                ],\n",
        "                max_tokens=30\n",
        "            )\n",
        "            break_suggestion = response.choices[0].message.content.strip()\n",
        "            print(f\"Break suggestion: {break_suggestion}\")\n",
        "            return break_suggestion\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error suggesting break: {e}\")\n",
        "            return \"Remember to take breaks!\"\n",
        "\n",
        "    def celebrate_milestone(self, milestone: str):\n",
        "        \"\"\"\n",
        "        Generates a congratulatory message for a completed milestone.\n",
        "\n",
        "        Args:\n",
        "            milestone (str): The completed thesis milestone.\n",
        "\n",
        "        Returns:\n",
        "            str: A celebratory message.\n",
        "        \"\"\"\n",
        "        print(f\"Celebrating milestone: '{milestone}'\")\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a positive thesis assistant. Congratulate the user on completing a significant milestone.\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Congratulate the user on completing the following thesis milestone: {milestone}\"}\n",
        "                ],\n",
        "                max_tokens=40\n",
        "            )\n",
        "            celebratory_message = response.choices[0].message.content.strip()\n",
        "            print(f\"Celebratory message: {celebratory_message}\")\n",
        "            return celebratory_message\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error celebrating milestone: {e}\")\n",
        "            return f\"Congratulations on completing the {milestone}!\"\n",
        "\n",
        "\n",
        "# Example Usage (requires 'client' object from a previous cell)\n",
        "if 'client' in locals():\n",
        "    print(\"\\n--- Testing EmotionSupport Module ---\")\n",
        "    emotion_support_module = EmotionSupport(openai_client=client)\n",
        "\n",
        "    print(\"\\n--- Testing detect_emotion ---\")\n",
        "    emotion1 = emotion_support_module.detect_emotion(\"I'm feeling really stuck on this part of the literature review.\")\n",
        "    emotion2 = emotion_support_module.detect_emotion(\"This analysis is going well, I feel confident!\")\n",
        "    emotion3 = emotion_support_module.detect_emotion(\"Just finished writing the entire methodology section!\")\n",
        "    print(f\"Detected Emotion 1: {emotion1}\")\n",
        "    print(f\"Detected Emotion 2: {emotion2}\")\n",
        "    print(f\"Detected Emotion 3: {emotion3}\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Testing provide_encouragement ---\")\n",
        "    encouragement1 = emotion_support_module.provide_encouragement(emotion1, \"literature review\")\n",
        "    encouragement2 = emotion_support_module.provide_encouragement(emotion2, \"data analysis\")\n",
        "    encouragement3 = emotion_support_module.provide_encouragement(\"Overwhelmed\", \"writing\") # Test with a different emotion/stage\n",
        "    print(f\"\\nEncouragement 1: {encouragement1}\")\n",
        "    print(f\"\\nEncouragement 2: {encouragement2}\")\n",
        "    print(f\"\\nEncouragement 3: {encouragement3}\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Testing suggest_break ---\")\n",
        "    break_suggestion1 = emotion_support_module.suggest_break(True) # Suggest break\n",
        "    break_suggestion2 = emotion_support_module.suggest_break(False) # Do not suggest break\n",
        "    print(f\"\\nBreak Suggestion 1: {break_suggestion1}\")\n",
        "    print(f\"Break Suggestion 2: {break_suggestion2}\") # Should be empty\n",
        "\n",
        "    print(\"\\n--- Testing celebrate_milestone ---\")\n",
        "    celebration1 = emotion_support_module.celebrate_milestone(\"first chapter draft\")\n",
        "    celebration2 = emotion_support_module.celebrate_milestone(\"data collection\")\n",
        "    print(f\"\\nCelebration 1: {celebration1}\")\n",
        "    print(f\"Celebration 2: {celebration2}\")\n",
        "\n",
        "else:\n",
        "    print(\"OpenAI client not initialized. Please run the cell setting up the client.\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing EmotionSupport Module ---\n",
            "\n",
            "--- Testing detect_emotion ---\n",
            "Detecting emotion for input: 'I'm feeling really stuck on this part of the literature review....'\n",
            "Detected emotion: Frustrated\n",
            "Detecting emotion for input: 'This analysis is going well, I feel confident!...'\n",
            "Detected emotion: Confident\n",
            "Detecting emotion for input: 'Just finished writing the entire methodology section!...'\n",
            "Detected emotion: Accomplished\n",
            "Detected Emotion 1: Frustrated\n",
            "Detected Emotion 2: Confident\n",
            "Detected Emotion 3: Accomplished\n",
            "\n",
            "--- Testing provide_encouragement ---\n",
            "Providing encouragement for emotion: 'Frustrated' at stage: 'literature review'\n",
            "Encouraging message: It's completely normal to feel frustrated during the literature review stage—it's often one of the most challenging parts of the thesis process. Remember, every bit of research you sift through is bringing you closer to your goals. Take a deep breath and celebrate the progress\n",
            "Providing encouragement for emotion: 'Confident' at stage: 'data analysis'\n",
            "Encouraging message: That's fantastic to hear you're feeling confident! You're in an exciting part of the process with data analysis—this is where your hard work starts to truly shine! Stay focused and trust in your skills. You've got this, and I'm sure you'll uncover amazing insights\n",
            "Providing encouragement for emotion: 'Overwhelmed' at stage: 'writing'\n",
            "Encouraging message: You’re doing an amazing job, even if it feels overwhelming right now! Remember, every great thesis comes together one word at a time. Take a deep breath and give yourself permission to write without judgment. You're in the thick of it, and that\n",
            "\n",
            "Encouragement 1: It's completely normal to feel frustrated during the literature review stage—it's often one of the most challenging parts of the thesis process. Remember, every bit of research you sift through is bringing you closer to your goals. Take a deep breath and celebrate the progress\n",
            "\n",
            "Encouragement 2: That's fantastic to hear you're feeling confident! You're in an exciting part of the process with data analysis—this is where your hard work starts to truly shine! Stay focused and trust in your skills. You've got this, and I'm sure you'll uncover amazing insights\n",
            "\n",
            "Encouragement 3: You’re doing an amazing job, even if it feels overwhelming right now! Remember, every great thesis comes together one word at a time. Take a deep breath and give yourself permission to write without judgment. You're in the thick of it, and that\n",
            "\n",
            "--- Testing suggest_break ---\n",
            "Suggesting a break...\n",
            "Break suggestion: It sounds like you're making great progress! Why not take a short break to recharge? A little time away can help clear your mind and boost your productivity\n",
            "\n",
            "Break Suggestion 1: It sounds like you're making great progress! Why not take a short break to recharge? A little time away can help clear your mind and boost your productivity\n",
            "Break Suggestion 2: \n",
            "\n",
            "--- Testing celebrate_milestone ---\n",
            "Celebrating milestone: 'first chapter draft'\n",
            "Celebratory message: Congratulations on completing your first chapter draft! 🎉 This is a significant milestone in your thesis journey, and it speaks to your hard work, dedication, and commitment to your research. You've taken a big\n",
            "Celebrating milestone: 'data collection'\n",
            "Celebratory message: Congratulations on completing your data collection phase! That's a significant milestone in your thesis journey, and it takes a lot of dedication and hard work to reach this point. You've put in the effort to gather valuable\n",
            "\n",
            "Celebration 1: Congratulations on completing your first chapter draft! 🎉 This is a significant milestone in your thesis journey, and it speaks to your hard work, dedication, and commitment to your research. You've taken a big\n",
            "Celebration 2: Congratulations on completing your data collection phase! That's a significant milestone in your thesis journey, and it takes a lot of dedication and hard work to reach this point. You've put in the effort to gather valuable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b2ebba0"
      },
      "source": [
        "## Integrate the new modules with the ethics module\n",
        "\n",
        "> Blockzitat einfügen\n",
        "\n",
        "\n",
        "\n",
        "### Subtask:\n",
        "Modify the existing `EthicsModule` or create a new orchestration layer to incorporate checks and logging related to the usage of the new modules (Idea Brainstorming, Writing Support, Emotion Support).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import the OpenAI library\n",
        "from openai import OpenAI\n",
        "# Used to securely store your API key - uncomment if using Colab Secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get your OpenAI API key securely\n",
        "# Replace \"<YOUR_OPENAI_API_KEY>\" with your key, or use Colab Secrets\n",
        "# Or if using Colab Secrets:\n",
        "openai_api_key_secure = userdata.get('OPENAI_API_KEY')\n",
        "openai_organization = userdata.get('OPENAI_ORGANIZATION')\n",
        "openai_project = userdata.get('OPENAI_PROJECT_ID')\n",
        "\n",
        "# Set your project API key\n",
        "OpenAI.api_key = openai_api_key_secure\n",
        "# You must also set organization and project ID\n",
        "OpenAI.organization = openai_organization\n",
        "OpenAI.project = openai_project\n",
        "\n",
        "# Create the OpenAI client\n",
        "client = OpenAI(api_key= OpenAI.api_key)\n",
        "\n"
      ],
      "metadata": {
        "id": "8zx760dou9eZ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a6eed87"
      },
      "source": [
        "**Reasoning**:\n",
        "Review the existing EthicsModule class to understand its structure and methods, particularly `log_usage` and `check_ethical_usage`. Determine how to integrate logging and ethical checks for the new modules (Idea Brainstorming, Writing Support, Emotion Support) by modifying `EthicsModule` or creating a new orchestrator. Implement the chosen integration approach in Python code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4894645d",
        "outputId": "bfc1c043-5b84-4d3b-969a-80cefc951648"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "from openai import OpenAI\n",
        "from scipy.spatial import distance\n",
        "import gymnasium as gym\n",
        "import json\n",
        "import os\n",
        "from stable_baselines3 import PPO\n",
        "import streamlit as st\n",
        "\n",
        "# Assuming the existing EthicsModule, IdeaBrainstorming, WritingSupport, and EmotionSupport\n",
        "# classes are defined in previous cells and available in the environment.\n",
        "\n",
        "class ThesisAssistant:\n",
        "    \"\"\"\n",
        "    Central orchestrator for the thesis assistant, managing interactions\n",
        "    between modules and ensuring ethical oversight.\n",
        "    \"\"\"\n",
        "    def __init__(self, openai_client):\n",
        "        \"\"\"\n",
        "        Initializes the ThesisAssistant with instances of all modules.\n",
        "\n",
        "        Args:\n",
        "            openai_client: An initialized OpenAI client object.\n",
        "        \"\"\"\n",
        "        self.client = openai_client\n",
        "        self.ethics_module = EthicsModule(openai_client=self.client)\n",
        "        self.idea_brainstorming = IdeaBrainstorming(openai_client=self.client)\n",
        "        self.writing_support = WritingSupport(openai_client=self.client)\n",
        "        self.emotion_support = EmotionSupport(openai_client=self.client)\n",
        "        # Assume other necessary components like RLConfigManager, DataPreprocessor,\n",
        "        # EthicsSupervisorRL, etc., are also available or initialized here if needed.\n",
        "\n",
        "    def process_user_request(self, user_input: str, request_type: str, thesis_stage: str = \"unknown\", context: dict = None):\n",
        "        \"\"\"\n",
        "        Processes a user request, directs it to the appropriate module,\n",
        "        logs the interaction, and performs ethical checks.\n",
        "\n",
        "        Args:\n",
        "            user_input (str): The user's input or prompt.\n",
        "            request_type (str): The type of request (e.g., \"brainstorm_ideas\",\n",
        "                                \"draft_text\", \"get_encouragement\").\n",
        "            thesis_stage (str): The current stage of the thesis.\n",
        "            context (dict, optional): Additional context relevant to the request\n",
        "                                      (e.g., outline, previous text, detected emotion).\n",
        "                                      Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the response from the relevant module\n",
        "                  and any ethical feedback or alerts.\n",
        "        \"\"\"\n",
        "        print(f\"\\nProcessing user request: '{request_type}' at stage '{thesis_stage}'\")\n",
        "\n",
        "        response_data = {\"module_response\": None, \"ethical_feedback\": []}\n",
        "        generated_content = \"\" # To store content generated by AI\n",
        "\n",
        "        # Log the initial request\n",
        "        self.ethics_module.log_usage(\n",
        "            prompt=user_input,\n",
        "            intent=request_type,\n",
        "            thesis_stage=thesis_stage\n",
        "        )\n",
        "\n",
        "        # Route the request to the appropriate module\n",
        "        if request_type == \"brainstorm_ideas\":\n",
        "            n_ideas = context.get(\"n_ideas\", 5) if context else 5\n",
        "            response_data[\"module_response\"] = self.idea_brainstorming.generate_ideas(\n",
        "                prompt=user_input,\n",
        "                thesis_stage=thesis_stage,\n",
        "                n_ideas=n_ideas\n",
        "            )\n",
        "            # For brainstorming, the generated content is the list of ideas (convert to string for checking)\n",
        "            generated_content = str(response_data[\"module_response\"])\n",
        "\n",
        "        elif request_type == \"explore_idea\":\n",
        "             idea_to_explore = context.get(\"idea\", \"\") if context else \"\"\n",
        "             response_data[\"module_response\"] = self.idea_brainstorming.explore_idea(\n",
        "                 idea=idea_to_explore,\n",
        "                 depth=context.get(\"depth\", 2) if context else 2\n",
        "             )\n",
        "             generated_content = response_data[\"module_response\"]\n",
        "\n",
        "        elif request_type == \"refine_ideas\":\n",
        "             ideas_to_refine = context.get(\"ideas\", []) if context else []\n",
        "             feedback = user_input # User input is the feedback\n",
        "             response_data[\"module_response\"] = self.idea_brainstorming.refine_ideas(\n",
        "                 ideas=ideas_to_refine,\n",
        "                 feedback=feedback\n",
        "             )\n",
        "             generated_content = str(response_data[\"module_response\"])\n",
        "\n",
        "        elif request_type == \"create_outline\":\n",
        "            response_data[\"module_response\"] = self.writing_support.create_outline(\n",
        "                topic=user_input,\n",
        "                thesis_stage=thesis_stage\n",
        "            )\n",
        "            generated_content = response_data[\"module_response\"]\n",
        "\n",
        "        elif request_type == \"draft_section\":\n",
        "             outline = context.get(\"outline\", \"\") if context else \"\"\n",
        "             response_data[\"module_response\"] = self.writing_support.draft_section(\n",
        "                 outline=outline,\n",
        "                 context=user_input # User input provides context for drafting\n",
        "             )\n",
        "             generated_content = response_data[\"module_response\"]\n",
        "\n",
        "        elif request_type == \"summarize_text\":\n",
        "             text_to_summarize = user_input # User input is the text to summarize\n",
        "             response_data[\"module_response\"] = self.writing_support.summarize_text(\n",
        "                 text=text_to_summarize\n",
        "             )\n",
        "             generated_content = response_data[\"module_response\"]\n",
        "\n",
        "        elif request_type == \"rephrase_text\":\n",
        "             text_to_rephrase = user_input # User input is the text to rephrase\n",
        "             style = context.get(\"style\", \"academic and clear\") if context else \"academic and clear\"\n",
        "             response_data[\"module_response\"] = self.writing_support.rephrase_text(\n",
        "                 text=text_to_rephrase,\n",
        "                 style=style\n",
        "             )\n",
        "             generated_content = response_data[\"module_response\"]\n",
        "\n",
        "        elif request_type == \"check_grammar_style\":\n",
        "             text_to_check = user_input # User input is the text to check\n",
        "             response_data[\"module_response\"] = self.writing_support.check_grammar_style(\n",
        "                 text=text_to_check\n",
        "             )\n",
        "             # Grammar/style check provides feedback, not necessarily AI generated content\n",
        "             generated_content = \"\" # Or handle this case differently if feedback is considered 'generated'\n",
        "\n",
        "        elif request_type == \"detect_emotion\":\n",
        "             response_data[\"module_response\"] = self.emotion_support.detect_emotion(\n",
        "                 user_input=user_input\n",
        "             )\n",
        "             # Emotion detection result is not content for AI check\n",
        "             generated_content = \"\"\n",
        "\n",
        "        elif request_type == \"provide_encouragement\":\n",
        "             detected_emotion = context.get(\"detected_emotion\", \"Neutral\") if context else \"Neutral\"\n",
        "             response_data[\"module_response\"] = self.emotion_support.provide_encouragement(\n",
        "                 detected_emotion=detected_emotion,\n",
        "                 thesis_stage=thesis_stage\n",
        "             )\n",
        "             generated_content = response_data[\"module_response\"]\n",
        "\n",
        "        elif request_type == \"suggest_break\":\n",
        "             needs_break = context.get(\"needs_break\", False) if context else False\n",
        "             response_data[\"module_response\"] = self.emotion_support.suggest_break(\n",
        "                 needs_break_flag=needs_break\n",
        "             )\n",
        "             generated_content = response_data[\"module_response\"]\n",
        "\n",
        "        elif request_type == \"celebrate_milestone\":\n",
        "             milestone = context.get(\"milestone\", \"a milestone\") if context else \"a milestone\"\n",
        "             response_data[\"module_response\"] = self.emotion_support.celebrate_milestone(\n",
        "                 milestone=milestone\n",
        "             )\n",
        "             generated_content = response_data[\"module_response\"]\n",
        "\n",
        "        else:\n",
        "            response_data[\"module_response\"] = \"Unknown request type.\"\n",
        "\n",
        "\n",
        "        # Perform ethical checks on the generated content (if any) and the user's prompt/intent\n",
        "        if generated_content:\n",
        "            is_ai, score, llm_response = self.ethics_module.detect_ai(generated_content)\n",
        "            if is_ai:\n",
        "                 response_data[\"ethical_feedback\"].append(f\"Potential AI-generated content detected (score: {score:.2f}). Remember to review and rephrase.\")\n",
        "\n",
        "        # Perform broader ethical checks based on prompt intent and request type\n",
        "        # This logic is simplified here and would be more complex in a real system\n",
        "        if request_type in [\"draft_section\", \"rephrase_text\"] and len(user_input) > 100: # Simple heuristic for potential misuse\n",
        "             response_data[\"ethical_feedback\"].append(\"Consider reviewing and significantly editing generated text to maintain authorship.\")\n",
        "\n",
        "        if request_type == \"generate_ideas\" and \"plagiarism\" in user_input.lower():\n",
        "             response_data[\"ethical_feedback\"].append(\"Ethical concern: Prompt mentions plagiarism. Ensure ideas promote original work.\")\n",
        "\n",
        "        # Example of checking ethical usage based on the prompt and generated text (using EthicsModule's method)\n",
        "        # Note: The check_ethical_usage method in EthicsModule is currently basic;\n",
        "        # a real implementation would need more sophisticated logic here or within the orchestrator.\n",
        "        # self.ethics_module.check_ethical_usage(user_input, generated_content)\n",
        "        # Ethical alerts from check_ethical_usage would need to be captured and added to response_data[\"ethical_feedback\"]\n",
        "\n",
        "        # In a real system, the RL agent's recommendation would be queried here\n",
        "        # and potentially influence the response or trigger interventions.\n",
        "        # action = self.ethics_module.ethics_supervisor_rl.recommend_action() # Assuming EthicsModule holds the RL agent\n",
        "        # response_data[\"rl_recommended_action\"] = action\n",
        "        # response_data[\"ethical_feedback\"].append(f\"RL Agent Recommendation (Action Index): {action}\")\n",
        "\n",
        "\n",
        "        return response_data\n",
        "\n",
        "# Example Usage (requires 'client' object from a previous cell)\n",
        "if 'client' in locals():\n",
        "    print(\"--- Initializing ThesisAssistant ---\")\n",
        "    # Assume EthicsModule, IdeaBrainstorming, WritingSupport, EmotionSupport classes are defined\n",
        "    thesis_assistant = ThesisAssistant(openai_client=client)\n",
        "\n",
        "    print(\"\\n--- Testing Brainstorming Request ---\")\n",
        "    context_brainstorm = {\"n_ideas\": 3}\n",
        "    response_brainstorm = thesis_assistant.process_user_request(\n",
        "        user_input=\"Research questions for a study on remote work productivity\",\n",
        "        request_type=\"brainstorm_ideas\",\n",
        "        thesis_stage=\"planning\",\n",
        "        context=context_brainstorm\n",
        "    )\n",
        "    print(\"Brainstorming Response:\")\n",
        "    print(response_brainstorm)\n",
        "\n",
        "    print(\"\\n--- Testing Drafting Request ---\")\n",
        "    # Need a mock outline for drafting\n",
        "    mock_outline = \"1. Introduction\\n2. Literature Review\\n   2.1 Previous studies\\n   2.2 Key theories\"\n",
        "    context_draft = {\"outline\": mock_outline}\n",
        "    response_draft = thesis_assistant.process_user_request(\n",
        "        user_input=\"Write a paragraph for section 2.1 summarizing recent findings on remote work.\",\n",
        "        request_type=\"draft_section\",\n",
        "        thesis_stage=\"literature review\",\n",
        "        context=context_draft\n",
        "    )\n",
        "    print(\"Drafting Response:\")\n",
        "    print(response_draft)\n",
        "\n",
        "    print(\"\\n--- Testing Emotion Detection Request ---\")\n",
        "    response_emotion = thesis_assistant.process_user_request(\n",
        "        user_input=\"I'm really struggling to make progress this week.\",\n",
        "        request_type=\"detect_emotion\",\n",
        "        thesis_stage=\"writing\"\n",
        "    )\n",
        "    print(\"Emotion Detection Response:\")\n",
        "    print(response_emotion)\n",
        "\n",
        "    print(\"\\n--- Testing Encouragement Request ---\")\n",
        "    context_encourage = {\"detected_emotion\": response_emotion[\"module_response\"]} # Use detected emotion from previous step\n",
        "    response_encourage = thesis_assistant.process_user_request(\n",
        "         user_input=\"Provide encouragement.\", # User input can be generic here\n",
        "         request_type=\"provide_encouragement\",\n",
        "         thesis_stage=\"writing\",\n",
        "         context=context_encourage\n",
        "    )\n",
        "    print(\"Encouragement Response:\")\n",
        "    print(response_encourage)\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"OpenAI client not initialized. Please run the cell setting up the client.\")\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initializing ThesisAssistant ---\n",
            "\n",
            "--- Testing Brainstorming Request ---\n",
            "\n",
            "Processing user request: 'brainstorm_ideas' at stage 'planning'\n",
            "Usage logged: Timestamp=2025-06-26 10:10:35.656719, Prompt='Research questions for a study on remote work productivity', Intent='brainstorm_ideas', Thesis Stage='planning'\n",
            "Generating 3 ideas for prompt: 'Research questions for a study on remote work productivity' at thesis stage: 'planning'\n",
            "Generated ideas: ['Here are three distinct and relevant research questions for your study on remote work productivity, perfect for the planning stage of your thesis:', '1. **Impact of Home Office Environment on Productivity:**', '- How do variations in home office conditions (e.g., space, ergonomics, distractions) influence the productivity levels of remote workers across different industries?', '2. **Role of Technology in Enhancing Remote Work Efficiency:**', '- What technological tools and platforms are most effective in mitigating productivity challenges faced by remote workers, and how do they vary in effectiveness based on employee role or job function?', '3. **Work-Life Balance and Its Relationship with Productivity:**', '- In what ways does the balance (or imbalance) between work and personal life in remote work settings affect overall job satisfaction and productivity among employees during and after the COVID-19 pandemic?', 'These questions can guide your research and help you explore various dimensions of remote work productivity comprehensively.']\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2\n",
            "\n",
            "The text is well-structured and presents coherent research questions, which indicates human-like reasoning. However, the formal tone and systematic approach could also align with AI-generated content, hence a low likelihood score.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "Brainstorming Response:\n",
            "{'module_response': ['Here are three distinct and relevant research questions for your study on remote work productivity, perfect for the planning stage of your thesis:', '1. **Impact of Home Office Environment on Productivity:**', '- How do variations in home office conditions (e.g., space, ergonomics, distractions) influence the productivity levels of remote workers across different industries?', '2. **Role of Technology in Enhancing Remote Work Efficiency:**', '- What technological tools and platforms are most effective in mitigating productivity challenges faced by remote workers, and how do they vary in effectiveness based on employee role or job function?', '3. **Work-Life Balance and Its Relationship with Productivity:**', '- In what ways does the balance (or imbalance) between work and personal life in remote work settings affect overall job satisfaction and productivity among employees during and after the COVID-19 pandemic?', 'These questions can guide your research and help you explore various dimensions of remote work productivity comprehensively.'], 'ethical_feedback': []}\n",
            "\n",
            "--- Testing Drafting Request ---\n",
            "\n",
            "Processing user request: 'draft_section' at stage 'literature review'\n",
            "Usage logged: Timestamp=2025-06-26 10:10:40.110085, Prompt='Write a paragraph for section 2.1 summarizing recent findings on remote work.', Intent='draft_section', Thesis Stage='literature review'\n",
            "Drafting section based on outline: '1. Introduction\n",
            "2. Literature Review\n",
            "   2.1 Previo...' and context: 'Write a paragraph for section 2.1 summarizing rece...'\n",
            "Drafted section: ### 2.1 Previous Studies\n",
            "\n",
            "Recent studies have increasingly focused on the implications of remote wor...\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2\n",
            "\n",
            "The text presents a coherent and informative overview of recent studies on remote work, using proper citations and a structured format that is typical of academic writing. While AI can generate such content, the specific references, nuanced discussion, and logical flow\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "Drafting Response:\n",
            "{'module_response': '### 2.1 Previous Studies\\n\\nRecent studies have increasingly focused on the implications of remote work across various sectors, revealing a complex landscape of both advantages and challenges. A comprehensive analysis by Cowan et al. (2021) highlights that employee productivity has often improved in remote settings, primarily due to reduced commuting time and enhanced work-life balance. However, another study by Allen et al. (2022) underscores the emergence of feelings of isolation and the potential for decreased team cohesion, emphasizing the need for organizations to implement robust communication strategies. Furthermore, a meta-analysis by Smith and Roberts (2023) synthesizes findings from over 50 articles, suggesting that while remote work allows for greater flexibility, it also raises significant concerns regarding employee burnout and the blurring of boundaries between personal and professional life. These studies collectively illustrate a paradigm shift in work arrangements and point to the necessity of tailored management approaches to optimize the benefits of remote work while mitigating its drawbacks.', 'ethical_feedback': []}\n",
            "\n",
            "--- Testing Emotion Detection Request ---\n",
            "\n",
            "Processing user request: 'detect_emotion' at stage 'writing'\n",
            "Usage logged: Timestamp=2025-06-26 10:10:44.602856, Prompt='I'm really struggling to make progress this week.', Intent='detect_emotion', Thesis Stage='writing'\n",
            "Detecting emotion for input: 'I'm really struggling to make progress this week....'\n",
            "Detected emotion: Frustrated\n",
            "Emotion Detection Response:\n",
            "{'module_response': 'Frustrated', 'ethical_feedback': []}\n",
            "\n",
            "--- Testing Encouragement Request ---\n",
            "\n",
            "Processing user request: 'provide_encouragement' at stage 'writing'\n",
            "Usage logged: Timestamp=2025-06-26 10:10:44.909662, Prompt='Provide encouragement.', Intent='provide_encouragement', Thesis Stage='writing'\n",
            "Providing encouragement for emotion: 'Frustrated' at stage: 'writing'\n",
            "Encouraging message: I know writing your thesis can feel overwhelming at times, but remember that every word you put down is a step closer to your goal. Frustration is a sign that you're pushing yourself, and that's when the best ideas often emerge. Take a deep\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.3  \n",
            "The text has a supportive and motivational tone commonly found in human-generated content. It reflects personal engagement and empathy, which are characteristics typically less prevalent in AI-generated text.\n",
            "AI detection score from LLM: 0.30. Is likely AI: False\n",
            "Encouragement Response:\n",
            "{'module_response': \"I know writing your thesis can feel overwhelming at times, but remember that every word you put down is a step closer to your goal. Frustration is a sign that you're pushing yourself, and that's when the best ideas often emerge. Take a deep\", 'ethical_feedback': []}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8293777"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `stable_baselines3` library was not found. Although `stable_baselines3` is imported in the code block, it is not used in the `ThesisAssistant` class definition itself. The import is likely for the `EthicsSupervisorRL` class which is assumed to be present. However, to resolve the `ModuleNotFoundError` without removing the import (as it's assumed to be needed by other parts of the code not explicitly shown in the orchestrator), I will ensure `stable_baselines3` is installed using `pip`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a84e3b71"
      },
      "source": [
        "## Update the RL Environment and Configuration (if necessary)\n",
        "\n",
        "### Subtask:\n",
        "Consider if the functionalities of the new modules (Idea Brainstorming, Writing Support, Emotion Support) introduce new state variables, actions, or reward signals that should be incorporated into the Reinforcement Learning environment (`EthicsSupervisorEnv`) and the RL configuration (`rl_config.json`).\n",
        "\n",
        "**Reasoning**:\n",
        "The addition of the Idea Brainstorming, Writing Support, and Emotion Support modules expands the potential interactions and states within the thesis assistant. The RL agent, the Ethics Supervisor, needs to be aware of these new aspects to make informed decisions about interventions. Therefore, we need to evaluate if the configuration and environment need to be updated to include relevant information from these modules as part of the observable state, define new possible actions related to these modules, or introduce new reward signals based on user interactions with these modules or their outcomes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import the OpenAI library\n",
        "from openai import OpenAI\n",
        "# Used to securely store your API key - uncomment if using Colab Secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get your OpenAI API key securely\n",
        "# Replace \"<YOUR_OPENAI_API_KEY>\" with your key, or use Colab Secrets\n",
        "# Or if using Colab Secrets:\n",
        "openai_api_key_secure = userdata.get('OPENAI_API_KEY')\n",
        "openai_organization = userdata.get('OPENAI_ORGANIZATION')\n",
        "openai_project = userdata.get('OPENAI_PROJECT_ID')\n",
        "\n",
        "# Set your project API key\n",
        "OpenAI.api_key = openai_api_key_secure\n",
        "# You must also set organization and project ID\n",
        "OpenAI.organization = openai_organization\n",
        "OpenAI.project = openai_project\n",
        "\n",
        "# Create the OpenAI client\n",
        "client = OpenAI(api_key= OpenAI.api_key)\n",
        "\n",
        "# Load the RL configuration using the RLConfigManager\n",
        "config = RLConfigManager.load_config()\n",
        "\n",
        "print(\"RL configuration loaded successfully.\")\n",
        "# Optionally, display the loaded config to verify\n",
        "# display(config)\n"
      ],
      "metadata": {
        "id": "Mj97vcN4vCMC"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c7edab7",
        "outputId": "dc39c5c5-a4cf-43c7-d59d-ea5bf2545caa"
      },
      "source": [],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RL configuration loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd69dabd"
      },
      "source": [
        "## Develop a unified interface for the Thesis Assistant\n",
        "\n",
        "### Subtask:\n",
        "Create a main class or function that brings all the modules together, allowing the user to interact with the complete system through a single interface.\n",
        "\n",
        "**Reasoning**:\n",
        "A unified interface is necessary to make the thesis assistant usable. This central component will receive user input, route it to the appropriate module(s), manage the overall state of the user's thesis project, interact with the ethics module for oversight, and present the combined output and feedback to the user. This aligns with the previously outlined `ThesisAssistant` orchestrator concept."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56d50a9e",
        "outputId": "4522704f-eb23-4bc2-f815-267022e10212"
      },
      "source": [
        "class ThesisAssistant:\n",
        "    \"\"\"\n",
        "    Central orchestrator for the thesis assistant, managing interactions\n",
        "    between modules and ensuring ethical oversight.\n",
        "    \"\"\"\n",
        "    def __init__(self, openai_client, config):\n",
        "        \"\"\"\n",
        "        Initializes the ThesisAssistant with instances of all modules and RL components.\n",
        "\n",
        "        Args:\n",
        "            openai_client: An initialized OpenAI client object.\n",
        "            config (dict): The loaded RL configuration dictionary.\n",
        "        \"\"\"\n",
        "        self.client = openai_client\n",
        "        self.config = config\n",
        "        self.ethics_module = EthicsModule(openai_client=self.client)\n",
        "        self.idea_brainstorming = IdeaBrainstorming(openai_client=self.client)\n",
        "        self.writing_support = WritingSupport(openai_client=self.client)\n",
        "        self.emotion_support = EmotionSupport(openai_client=self.client)\n",
        "\n",
        "        # Initialize RL components\n",
        "        self.data_preprocessor = DataPreprocessor(self.config)\n",
        "        self.mock_ethics_state = MockEthicsModule() # Using MockEthicsModule to simulate state for RL\n",
        "        self.ethics_env = EthicsSupervisorEnv(self.mock_ethics_state, self.config)\n",
        "        self.rl_supervisor = EthicsSupervisorRL(self.config) # Assuming model_path is handled internally\n",
        "\n",
        "    def process_user_request(self, user_input: str, request_type: str, thesis_stage: str = \"unknown\", context: dict = None):\n",
        "        \"\"\"\n",
        "        Processes a user request, directs it to the appropriate module,\n",
        "        logs the interaction, performs ethical checks, and gets RL recommendation.\n",
        "\n",
        "        Args:\n",
        "            user_input (str): The user's input or prompt.\n",
        "            request_type (str): The type of request (e.g., \"brainstorm_ideas\",\n",
        "                                \"draft_text\", \"get_encouragement\").\n",
        "            thesis_stage (str): The current stage of the thesis.\n",
        "            context (dict, optional): Additional context relevant to the request\n",
        "                                      (e.g., outline, previous text, detected emotion).\n",
        "                                      Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the response from the relevant module,\n",
        "                  any ethical feedback or alerts, and the RL recommended action.\n",
        "        \"\"\"\n",
        "        print(f\"\\nProcessing user request: '{request_type}' at stage '{thesis_stage}'\")\n",
        "\n",
        "        response_data = {\"module_response\": None, \"ethical_feedback\": [], \"rl_recommended_action\": None}\n",
        "        generated_content = \"\" # To store content generated by AI\n",
        "        log_entry_for_rl = { # Prepare log entry for potential use by RL (simplified)\n",
        "            \"prompt\": user_input,\n",
        "            \"intent\": request_type,\n",
        "            \"thesis_stage\": thesis_stage,\n",
        "            \"timestamp\": pd.Timestamp.now().isoformat(), # Use ISO format for logging\n",
        "            # Add placeholder values for state variables expected by DataPreprocessor\n",
        "            \"embedding_drift\": getattr(self.mock_ethics_state, 'embedding_drift', 0.0),\n",
        "            \"ai_usage\": getattr(self.mock_ethics_state, 'ai_usage', 0.0),\n",
        "            \"ethical_flags\": getattr(self.mock_ethics_state, 'ethical_flags', 0.0),\n",
        "            \"advisor_feedback\": getattr(self.mock_ethics_state, 'advisor_feedback', 0.0),\n",
        "            \"deadline_ratio\": getattr(self.mock_ethics_state, 'deadline_ratio', 0.0),\n",
        "            # Add placeholder boolean flags for reward calculation (will be updated based on outcome)\n",
        "            \"user_revised\": False,\n",
        "            \"ai_violation\": False,\n",
        "            \"advisor_positive\": False,\n",
        "            \"rewrite_accepted\": False,\n",
        "            \"milestone_completed\": False,\n",
        "            \"hallucination_detected\": False,\n",
        "            \"idea_accepted\": False,\n",
        "            \"writing_assistance_successful\": False,\n",
        "            \"emotion_support_positive_response\": False,\n",
        "            \"break_taken\": False,\n",
        "        }\n",
        "\n",
        "\n",
        "        # Log the initial request using the ethics module's method\n",
        "        self.ethics_module.log_usage(\n",
        "            prompt=user_input,\n",
        "            intent=request_type,\n",
        "            thesis_stage=thesis_stage\n",
        "        )\n",
        "\n",
        "\n",
        "        # Route the request to the appropriate module\n",
        "        if request_type == \"brainstorm_ideas\":\n",
        "            n_ideas = context.get(\"n_ideas\", 5) if context else 5\n",
        "            response_data[\"module_response\"] = self.idea_brainstorming.generate_ideas(\n",
        "                prompt=user_input,\n",
        "                thesis_stage=thesis_stage,\n",
        "                n_ideas=n_ideas\n",
        "            )\n",
        "            generated_content = str(response_data[\"module_response\"]) # Convert list to string for AI check\n",
        "\n",
        "        elif request_type == \"explore_idea\":\n",
        "             idea_to_explore = context.get(\"idea\", \"\") if context else \"\"\n",
        "             response_data[\"module_response\"] = self.idea_brainstorming.explore_idea(\n",
        "                 idea=idea_to_explore,\n",
        "                 depth=context.get(\"depth\", 2) if context else 2\n",
        "             )\n",
        "             generated_content = response_data[\"module_response\"]\n",
        "\n",
        "        elif request_type == \"refine_ideas\":\n",
        "             ideas_to_refine = context.get(\"ideas\", []) if context else []\n",
        "             feedback = user_input # User input is the feedback for refinement\n",
        "             response_data[\"module_response\"] = self.idea_brainstorming.refine_ideas(\n",
        "                 ideas=ideas_to_refine,\n",
        "                 feedback=feedback\n",
        "             )\n",
        "             generated_content = str(response_data[\"module_response\"])\n",
        "\n",
        "        elif request_type == \"create_outline\":\n",
        "            response_data[\"module_response\"] = self.writing_support.create_outline(\n",
        "                topic=user_input,\n",
        "                thesis_stage=thesis_stage\n",
        "            )\n",
        "            generated_content = response_data[\"module_response\"]\n",
        "\n",
        "        elif request_type == \"draft_section\":\n",
        "             outline = context.get(\"outline\", \"\") if context else \"\"\n",
        "             response_data[\"module_response\"] = self.writing_support.draft_section(\n",
        "                 outline=outline,\n",
        "                 context=user_input # User input provides context for drafting\n",
        "             )\n",
        "             generated_content = response_data[\"module_response\"]\n",
        "\n",
        "        elif request_type == \"summarize_text\":\n",
        "             text_to_summarize = user_input # User input is the text to summarize\n",
        "             response_data[\"module_response\"] = self.writing_support.summarize_text(\n",
        "                 text=text_to_summarize\n",
        "             )\n",
        "             generated_content = response_data[\"module_response\"]\n",
        "\n",
        "        elif request_type == \"rephrase_text\":\n",
        "             text_to_rephrase = user_input # User input is the text to rephrase\n",
        "             style = context.get(\"style\", \"academic and clear\") if context else \"academic and clear\"\n",
        "             response_data[\"module_response\"] = self.writing_support.rephrase_text(\n",
        "                 text=text_to_rephrase,\n",
        "                 style=style\n",
        "             )\n",
        "             generated_content = response_data[\"module_response\"]\n",
        "\n",
        "        elif request_type == \"check_grammar_style\":\n",
        "             text_to_check = user_input # User input is the text to check\n",
        "             response_data[\"module_response\"] = self.writing_support.check_grammar_style(\n",
        "                 text=text_to_check\n",
        "             )\n",
        "             # Grammar/style check provides feedback, not typically considered 'generated content' for AI detection\n",
        "             generated_content = \"\" # Or handle this differently if feedback itself needs checking\n",
        "\n",
        "        elif request_type == \"detect_emotion\":\n",
        "             response_data[\"module_response\"] = self.emotion_support.detect_emotion(\n",
        "                 user_input=user_input\n",
        "             )\n",
        "             # Emotion detection result is a label, not content for AI check\n",
        "             generated_content = \"\"\n",
        "             # Update mock state for RL with detected emotion (simplified mapping)\n",
        "             if response_data[\"module_response\"] in [\"Frustrated\", \"Overwhelmed\", \"Confused\"]:\n",
        "                 self.mock_ethics_state.emotion_state = min(self.mock_ethics_state.emotion_state + 0.2, 1.0)\n",
        "             elif response_data[\"module_response\"] in [\"Motivated\", \"Happy\", \"Confident\"]:\n",
        "                 self.mock_ethics_state.emotion_state = max(self.mock_ethics_state.emotion_state - 0.1, 0.0)\n",
        "\n",
        "\n",
        "        elif request_type == \"provide_encouragement\":\n",
        "             detected_emotion = context.get(\"detected_emotion\", \"Neutral\") if context else \"Neutral\"\n",
        "             response_data[\"module_response\"] = self.emotion_support.provide_encouragement(\n",
        "                 detected_emotion=detected_emotion,\n",
        "                 thesis_stage=thesis_stage\n",
        "             )\n",
        "             generated_content = response_data[\"module_response\"]\n",
        "\n",
        "        elif request_type == \"suggest_break\":\n",
        "             needs_break = context.get(\"needs_break\", False) if context else False\n",
        "             response_data[\"module_response\"] = self.emotion_support.suggest_break(\n",
        "                 needs_break_flag=needs_break\n",
        "             )\n",
        "             generated_content = response_data[\"module_response\"]\n",
        "\n",
        "        elif request_type == \"celebrate_milestone\":\n",
        "             milestone = context.get(\"milestone\", \"a milestone\") if context else \"a milestone\"\n",
        "             response_data[\"module_response\"] = self.emotion_support.celebrate_milestone(\n",
        "                 milestone=milestone\n",
        "             )\n",
        "             generated_content = response_data[\"module_response\"]\n",
        "             log_entry_for_rl[\"milestone_completed\"] = True # Set flag for reward calculation\n",
        "\n",
        "        else:\n",
        "            response_data[\"module_response\"] = \"Unknown request type.\"\n",
        "\n",
        "\n",
        "        # Perform ethical checks on the generated content (if any) and the user's prompt/intent\n",
        "        if generated_content:\n",
        "            is_ai, score, llm_response = self.ethics_module.detect_ai(generated_content)\n",
        "            if is_ai:\n",
        "                 response_data[\"ethical_feedback\"].append(f\"Potential AI-generated content detected (score: {score:.2f}). Remember to review and rephrase.\")\n",
        "                 log_entry_for_rl[\"ai_violation\"] = True # Set flag for reward calculation\n",
        "            # Update mock state for RL based on AI detection score\n",
        "            self.mock_ethics_state.ai_usage = max(self.mock_ethics_state.ai_usage, score) # Assume AI usage state is the max detection score\n",
        "\n",
        "        # Perform broader ethical checks based on prompt intent and request type (can be simplified or more complex)\n",
        "        if request_type in [\"draft_section\", \"rephrase_text\"] and len(user_input) > 100: # Simple heuristic for potential misuse\n",
        "             response_data[\"ethical_feedback\"].append(\"Consider reviewing and significantly editing generated text to maintain authorship.\")\n",
        "             self.mock_ethics_state.ethical_flags = min(self.mock_ethics_state.ethical_flags + 0.1, 1.0) # Increase ethical flags\n",
        "             log_entry_for_rl[\"ai_violation\"] = True # Consider this a potential AI violation for reward\n",
        "\n",
        "        if request_type == \"generate_ideas\" and \"plagiarism\" in user_input.lower():\n",
        "             response_data[\"ethical_feedback\"].append(\"Ethical concern: Prompt mentions plagiarism. Ensure ideas promote original work.\")\n",
        "             self.mock_ethics_state.ethical_flags = min(self.mock_ethics_state.ethical_flags + 0.2, 1.0) # Increase ethical flags significantly\n",
        "             log_entry_for_rl[\"ai_violation\"] = True # Consider this a potential AI violation for reward\n",
        "\n",
        "\n",
        "        # --- RL Agent Interaction ---\n",
        "        # Get the current state for the RL agent from the environment (which reads from mock_ethics_state)\n",
        "        current_state = self.ethics_env._get_state()\n",
        "        # Get the recommended action from the RL supervisor based on the current state\n",
        "        rl_action = self.rl_supervisor.recommend_action()\n",
        "        response_data[\"rl_recommended_action\"] = rl_action\n",
        "        response_data[\"ethical_feedback\"].append(f\"RL Agent Recommendation (Action Index): {rl_action} - '{self.config['actions'][rl_action]}'\")\n",
        "\n",
        "        # Simulate stepping the environment with the chosen action (this will update mock_ethics_state based on action_effects)\n",
        "        # In a real system, the RL action might directly influence the user's experience or the assistant's behavior\n",
        "        # Here, we simulate the effect of the action on the internal state for training purposes.\n",
        "        # We also compute a reward based on the log entry and the action taken.\n",
        "        # Note: The reward computation in EthicsSupervisorEnv._compute_reward is currently a placeholder.\n",
        "        # A more sophisticated approach would compute the reward here based on the log_entry_for_rl and the action.\n",
        "        # For now, we'll just step the environment which uses its internal (placeholder) reward logic.\n",
        "        next_state, reward, done, truncated, info = self.ethics_env.step(rl_action)\n",
        "        print(f\"RL Environment stepped - Next State: {next_state}, Simulated Reward: {reward}, Done: {done}\")\n",
        "\n",
        "        # In a real system, you would store the experience (current_state, rl_action, reward, next_state, done)\n",
        "        # for training the RL agent later, possibly in a replay buffer.\n",
        "        # For this example, we are not implementing the full RL training loop within process_user_request.\n",
        "\n",
        "\n",
        "        return response_data\n",
        "\n",
        "# Example Usage (requires 'client' object and 'config' object from previous cells)\n",
        "if 'client' in locals() and 'config' in locals():\n",
        "    print(\"\\n--- Initializing ThesisAssistant with RL ---\")\n",
        "    # Assume all required classes (EthicsModule, IdeaBrainstorming, etc.) are defined\n",
        "    thesis_assistant = ThesisAssistant(openai_client=client, config=config)\n",
        "\n",
        "    print(\"\\n--- Testing Brainstorming Request with RL Interaction ---\")\n",
        "    context_brainstorm = {\"n_ideas\": 3}\n",
        "    response_brainstorm = thesis_assistant.process_user_request(\n",
        "        user_input=\"Research questions for a study on artificial intelligence in healthcare\",\n",
        "        request_type=\"brainstorm_ideas\",\n",
        "        thesis_stage=\"planning\",\n",
        "        context=context_brainstorm\n",
        "    )\n",
        "    print(\"Brainstorming Response with RL:\")\n",
        "    print(response_brainstorm)\n",
        "\n",
        "    print(\"\\n--- Testing Writing Support Request with RL Interaction ---\")\n",
        "    mock_outline = \"1. Introduction\\n2. Background\\n   2.1 AI in healthcare history\"\n",
        "    context_draft = {\"outline\": mock_outline}\n",
        "    response_draft = thesis_assistant.process_user_request(\n",
        "        user_input=\"Draft a short paragraph for section 2.1 outlining the key historical milestones of AI in healthcare.\",\n",
        "        request_type=\"draft_section\",\n",
        "        thesis_stage=\"literature review\",\n",
        "        context=context_draft\n",
        "    )\n",
        "    print(\"Drafting Response with RL:\")\n",
        "    print(response_draft)\n",
        "\n",
        "    print(\"\\n--- Testing Emotion Support Request with RL Interaction ---\")\n",
        "    response_emotion = thesis_assistant.process_user_request(\n",
        "        user_input=\"I'm feeling really overwhelmed by the amount of literature.\",\n",
        "        request_type=\"detect_emotion\",\n",
        "        thesis_stage=\"literature review\"\n",
        "    )\n",
        "    print(\"Emotion Detection Response with RL:\")\n",
        "    print(response_emotion)\n",
        "\n",
        "    # Now test providing encouragement based on the detected emotion\n",
        "    if response_emotion[\"module_response\"] != \"Undetermined\":\n",
        "        context_encourage = {\"detected_emotion\": response_emotion[\"module_response\"]}\n",
        "        response_encourage = thesis_assistant.process_user_request(\n",
        "            user_input=\"Can you give me some encouragement?\",\n",
        "            request_type=\"provide_encouragement\",\n",
        "            thesis_stage=\"literature review\",\n",
        "            context=context_encourage\n",
        "        )\n",
        "        print(\"Encouragement Response with RL:\")\n",
        "        print(response_encourage)\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"OpenAI client or config not initialized. Please run the necessary cells.\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Initializing ThesisAssistant with RL ---\n",
            "Initialized new PPO model.\n",
            "\n",
            "--- Testing Brainstorming Request with RL Interaction ---\n",
            "\n",
            "Processing user request: 'brainstorm_ideas' at stage 'planning'\n",
            "Usage logged: Timestamp=2025-06-26 10:18:55.691208, Prompt='Research questions for a study on artificial intelligence in healthcare', Intent='brainstorm_ideas', Thesis Stage='planning'\n",
            "Generating 3 ideas for prompt: 'Research questions for a study on artificial intelligence in healthcare' at thesis stage: 'planning'\n",
            "Generated ideas: ['Here are three distinct research question ideas for your study on artificial intelligence in healthcare:', '1. **Impact Assessment of AI Tools on Patient Outcomes**: \"How do AI-driven diagnostic tools compare to traditional diagnostic methods in terms of accuracy and patient outcomes across various medical specialties?\"', '- This question aims to explore the effectiveness of AI technologies in improving diagnostic accuracy and the resulting impact on patient health outcomes compared to traditional approaches.', '2. **Ethical Implications and Patient Trust**: \"What are the ethical implications associated with the use of AI in patient data management, and how do these implications affect patient trust in healthcare providers?\"', \"- This question focuses on understanding the ethical considerations of using AI when handling sensitive patient information and how it influences patients' confidence in their healthcare systems.\", '3. **AI Integration and Workflow Efficiency**: \"How does the integration of AI technologies in clinical workflows influence operational efficiency and resource allocation in healthcare settings?\"', '- Here, the focus is on investigating the practical effects of AI implementation on the efficiency of healthcare operations, including time management and resource distribution within healthcare facilities.', 'Each of these questions offers a unique perspective on the intersection of artificial intelligence and healthcare, setting the stage for a comprehensive study.']\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.8  \n",
            "The structured format and academic tone of the text, along with its focus on specific research questions, suggest characteristics commonly associated with AI-generated content. The detailed exploration of various angles about AI in healthcare further indicates a high likelihood of AI involvement\n",
            "AI detection score from LLM: 0.80. Is likely AI: True\n",
            "RL Environment stepped - Next State: [0.2025926 0.8       0.7303228 0.2801921 0.01     ], Simulated Reward: 0.94, Done: False\n",
            "Brainstorming Response with RL:\n",
            "{'module_response': ['Here are three distinct research question ideas for your study on artificial intelligence in healthcare:', '1. **Impact Assessment of AI Tools on Patient Outcomes**: \"How do AI-driven diagnostic tools compare to traditional diagnostic methods in terms of accuracy and patient outcomes across various medical specialties?\"', '- This question aims to explore the effectiveness of AI technologies in improving diagnostic accuracy and the resulting impact on patient health outcomes compared to traditional approaches.', '2. **Ethical Implications and Patient Trust**: \"What are the ethical implications associated with the use of AI in patient data management, and how do these implications affect patient trust in healthcare providers?\"', \"- This question focuses on understanding the ethical considerations of using AI when handling sensitive patient information and how it influences patients' confidence in their healthcare systems.\", '3. **AI Integration and Workflow Efficiency**: \"How does the integration of AI technologies in clinical workflows influence operational efficiency and resource allocation in healthcare settings?\"', '- Here, the focus is on investigating the practical effects of AI implementation on the efficiency of healthcare operations, including time management and resource distribution within healthcare facilities.', 'Each of these questions offers a unique perspective on the intersection of artificial intelligence and healthcare, setting the stage for a comprehensive study.'], 'ethical_feedback': ['Potential AI-generated content detected (score: 0.80). Remember to review and rephrase.', \"RL Agent Recommendation (Action Index): 3 - 'Suggest rewriting'\"], 'rl_recommended_action': 3}\n",
            "\n",
            "--- Testing Writing Support Request with RL Interaction ---\n",
            "\n",
            "Processing user request: 'draft_section' at stage 'literature review'\n",
            "Usage logged: Timestamp=2025-06-26 10:19:00.962085, Prompt='Draft a short paragraph for section 2.1 outlining the key historical milestones of AI in healthcare.', Intent='draft_section', Thesis Stage='literature review'\n",
            "Drafting section based on outline: '1. Introduction\n",
            "2. Background\n",
            "   2.1 AI in healthc...' and context: 'Draft a short paragraph for section 2.1 outlining ...'\n",
            "Drafted section: ### 2.1 AI in Healthcare History\n",
            "\n",
            "The integration of artificial intelligence (AI) into healthcare ha...\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.3  \n",
            "The text exhibits a formal, structured writing style with historical context and specific details, which is characteristic of human writing, particularly in academic or informative contexts. While AI can produce similar text, this particular passage lacks certain repetitive patterns and generic\n",
            "AI detection score from LLM: 0.30. Is likely AI: False\n",
            "RL Environment stepped - Next State: [0.2025926  0.8        0.63032275 0.2801921  0.02      ], Simulated Reward: 0.9649999999999999, Done: False\n",
            "Drafting Response with RL:\n",
            "{'module_response': '### 2.1 AI in Healthcare History\\n\\nThe integration of artificial intelligence (AI) into healthcare has evolved significantly over the past several decades, marked by several key milestones that have shaped its current landscape. The journey began in the 1950s with the development of early programs focused on medical diagnosis, such as the “DENDRAL” project, which utilized rule-based systems to analyze chemical structures. The 1970s saw the emergence of MYCIN, a pioneering expert system designed to diagnose bacterial infections and recommend treatments, demonstrating the potential of AI to support clinical decision-making. The 1990s and early 2000s marked a shift towards more advanced machine learning techniques, with the introduction of data-driven approaches and the collection of large healthcare datasets, paving the way for predictive analytics. As computational power surged in the 2010s, AI technologies began to proliferate, particularly in imaging and personalized medicine, culminating in significant advancements in deep learning algorithms. Today, AI continues to transform healthcare through applications in diagnostics, treatment planning, and patient monitoring, driven by ongoing research and innovation.', 'ethical_feedback': [\"RL Agent Recommendation (Action Index): 3 - 'Suggest rewriting'\"], 'rl_recommended_action': 3}\n",
            "\n",
            "--- Testing Emotion Support Request with RL Interaction ---\n",
            "\n",
            "Processing user request: 'detect_emotion' at stage 'literature review'\n",
            "Usage logged: Timestamp=2025-06-26 10:19:05.555780, Prompt='I'm feeling really overwhelmed by the amount of literature.', Intent='detect_emotion', Thesis Stage='literature review'\n",
            "Detecting emotion for input: 'I'm feeling really overwhelmed by the amount of literature....'\n",
            "Detected emotion: Overwhelmed\n",
            "RL Environment stepped - Next State: [0.2025926 0.8       0.5303228 0.2801921 0.03     ], Simulated Reward: 0.99, Done: False\n",
            "Emotion Detection Response with RL:\n",
            "{'module_response': 'Overwhelmed', 'ethical_feedback': [\"RL Agent Recommendation (Action Index): 3 - 'Suggest rewriting'\"], 'rl_recommended_action': 3}\n",
            "\n",
            "Processing user request: 'provide_encouragement' at stage 'literature review'\n",
            "Usage logged: Timestamp=2025-06-26 10:19:05.834312, Prompt='Can you give me some encouragement?', Intent='provide_encouragement', Thesis Stage='literature review'\n",
            "Providing encouragement for emotion: 'Overwhelmed' at stage: 'literature review'\n",
            "Encouraging message: I completely understand that the literature review can feel daunting, but remember, you're laying the foundation for your entire thesis! Take it one step at a time, and know that every piece of research you gather brings you closer to your goal. You've got this\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2\n",
            "\n",
            "The text has a motivational tone and personal encouragement, which are more characteristic of human expression than AI-generated content. While it is possible for AI to produce similar text, the emotional depth and supportive language suggest a higher likelihood of being written\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "RL Environment stepped - Next State: [0.2025926 0.8       0.4303228 0.2801921 0.04     ], Simulated Reward: 1.015, Done: False\n",
            "Encouragement Response with RL:\n",
            "{'module_response': \"I completely understand that the literature review can feel daunting, but remember, you're laying the foundation for your entire thesis! Take it one step at a time, and know that every piece of research you gather brings you closer to your goal. You've got this\", 'ethical_feedback': [\"RL Agent Recommendation (Action Index): 3 - 'Suggest rewriting'\"], 'rl_recommended_action': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad5d521f"
      },
      "source": [
        "## Test and Refine\n",
        "\n",
        "### Subtask:\n",
        "Test the integrated system with various scenarios to ensure all modules are working together correctly and that ethical considerations are being addressed.\n",
        "\n",
        "**Reasoning**:\n",
        "Testing is crucial to identify any bugs or unexpected behavior in the integrated system. By simulating different user interactions and thesis stages, we can verify that the `ThesisAssistant` correctly routes requests, the individual modules provide appropriate responses, the ethics module performs its checks and logging, and the RL agent provides recommendations based on the simulated state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19cbc904",
        "outputId": "74c19023-b46e-49ca-d315-d4ff3deef80c"
      },
      "source": [
        "# Assuming 'client' and 'config' are available from previous cells,\n",
        "# and the ThesisAssistant class is defined.\n",
        "\n",
        "if 'client' in locals() and 'config' in locals() and 'ThesisAssistant' in locals():\n",
        "    print(\"--- Testing Integrated ThesisAssistant System ---\")\n",
        "\n",
        "    # Initialize the ThesisAssistant, explicitly passing client and config\n",
        "    try:\n",
        "        thesis_assistant = ThesisAssistant(openai_client=client, config=config)\n",
        "\n",
        "        # --- Test Case 1: Brainstorming Request with potential ethical flag ---\n",
        "        print(\"\\n--- Test Case 1: Brainstorming Request (focus on a sensitive topic) ---\")\n",
        "        response1 = thesis_assistant.process_user_request(\n",
        "            user_input=\"Generate controversial ideas for my thesis on political science.\",\n",
        "            request_type=\"brainstorm_ideas\",\n",
        "            thesis_stage=\"planning\"\n",
        "        )\n",
        "        print(\"Response 1 (Controversial Brainstorming):\")\n",
        "        print(response1)\n",
        "\n",
        "        # --- Test Case 2: Writing Support Request (drafting a section) ---\n",
        "        print(\"\\n--- Test Case 2: Writing Support Request (drafting a literature review paragraph) ---\")\n",
        "        mock_outline_lr = \"1. Introduction\\n2. Literature Review\\n   2.1 Key concepts\\n   2.2 Relevant studies\"\n",
        "        context_draft_lr = {\"outline\": mock_outline_lr}\n",
        "        response2 = thesis_assistant.process_user_request(\n",
        "            user_input=\"Draft a paragraph for section 2.2 summarizing a recent study on climate modeling.\",\n",
        "            request_type=\"draft_section\",\n",
        "            thesis_stage=\"literature review\",\n",
        "            context=context_draft_lr\n",
        "        )\n",
        "        print(\"Response 2 (Drafting Section):\")\n",
        "        print(response2)\n",
        "\n",
        "        # --- Test Case 3: Emotion Support Request (detecting frustration) ---\n",
        "        print(\"\\n--- Test Case 3: Emotion Support Request (detecting frustration) ---\")\n",
        "        response3 = thesis_assistant.process_user_request(\n",
        "            user_input=\"I've been working on this chapter for hours and it's still not right!\",\n",
        "            request_type=\"detect_emotion\",\n",
        "            thesis_stage=\"writing\"\n",
        "        )\n",
        "        print(\"Response 3 (Detecting Frustration):\")\n",
        "        print(response3)\n",
        "\n",
        "        # --- Test Case 4: Emotion Support Request (providing encouragement based on frustration) ---\n",
        "        if response3[\"module_response\"] != \"Undetermined\":\n",
        "            print(\"\\n--- Test Case 4: Emotion Support Request (providing encouragement) ---\")\n",
        "            context_encourage = {\"detected_emotion\": response3[\"module_response\"]}\n",
        "            response4 = thesis_assistant.process_user_request(\n",
        "                user_input=\"I need some encouragement to keep going.\",\n",
        "                request_type=\"provide_encouragement\",\n",
        "                thesis_stage=\"writing\",\n",
        "                context=context_encourage\n",
        "            )\n",
        "            print(\"Response 4 (Providing Encouragement):\")\n",
        "            print(response4)\n",
        "\n",
        "        # --- Test Case 5: Writing Support Request (rephrasing a long sentence - potential AI flag) ---\n",
        "        print(\"\\n--- Test Case 5: Writing Support Request (rephrasing complex text) ---\")\n",
        "        long_complex_sentence = \"The epistemological implications of quantum entanglement necessitate a re-evaluation of classical deterministic paradigms in the context of non-local hidden variable theories, thereby challenging the foundational tenets of conventional realism in empirical observation.\"\n",
        "        response5 = thesis_assistant.process_user_request(\n",
        "            user_input=long_complex_sentence,\n",
        "            request_type=\"rephrase_text\",\n",
        "            thesis_stage=\"methodology\"\n",
        "        )\n",
        "        print(\"Response 5 (Rephrasing Complex Text):\")\n",
        "        print(response5)\n",
        "\n",
        "        # --- Test Case 6: Emotion Support Request (celebrating a milestone) ---\n",
        "        print(\"\\n--- Test Case 6: Emotion Support Request (celebrating a milestone) ---\")\n",
        "        context_milestone = {\"milestone\": \"completion of data analysis\"}\n",
        "        response6 = thesis_assistant.process_user_request(\n",
        "            user_input=\"I finished analyzing all the data!\",\n",
        "            request_type=\"celebrate_milestone\",\n",
        "            thesis_stage=\"data analysis\",\n",
        "            context=context_milestone\n",
        "        )\n",
        "        print(\"Response 6 (Celebrating Milestone):\")\n",
        "        print(response6)\n",
        "\n",
        "    except NameError as e:\n",
        "        print(f\"Error initializing ThesisAssistant: {e}. Make sure 'client' and 'config' are defined in preceding cells.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Required objects (client, config, ThesisAssistant) not initialized before the check.\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing Integrated ThesisAssistant System ---\n",
            "Initialized new PPO model.\n",
            "\n",
            "--- Test Case 1: Brainstorming Request (focus on a sensitive topic) ---\n",
            "\n",
            "Processing user request: 'brainstorm_ideas' at stage 'planning'\n",
            "Usage logged: Timestamp=2025-06-26 10:18:37.424405, Prompt='Generate controversial ideas for my thesis on political science.', Intent='brainstorm_ideas', Thesis Stage='planning'\n",
            "Generating 5 ideas for prompt: 'Generate controversial ideas for my thesis on political science.' at thesis stage: 'planning'\n",
            "Generated ideas: ['Here are five distinct and potentially controversial thesis ideas related to political science that you could explore in your research:', '1. **The Ethics of Political Assassination: A Justifiable Tool for Regime Change?**', 'Investigate historical cases where political assassinations were utilized to effect regime change. Discuss the moral implications, the outcomes, and the potential justification for such actions in the context of global governance and human rights. Analyze the tension between ethical considerations and political effectiveness.', '2. **Populism and Democracy: Catalysts for Political Reform or Threats to Liberal Order?**', 'Examine the rise of populist movements around the world and their impact on established democratic institutions. Delve into the argument that populism can be both a necessary response to elitism and a peril to democratic norms, alongside the implications for future governance and civil society.', '3. **The Role of Social Media in Shaping Political Polarization: Democratic Tool or Weapon of Division?**', 'Explore how social media platforms contribute to political polarization. Assess whether these platforms serve as a democracy-enhancing tool for political engagement or if they function more as amplifiers of division, misinformation, and extremist ideologies.', '4. **Climate Change Policy and National Sovereignty: The Case for Global Governance?**', 'Debate whether climate change action necessitates sacrificing aspects of national sovereignty in favor of a global governance model. Consider the political, ethical, and practical implications of enforced international policies and cooperation to combat climate change, potentially challenging traditional notions of state autonomy']\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.8\n",
            "\n",
            "The text displays a structured and academic tone typical of AI-generated content, with well-formed thesis ideas that align with current discussions in political science. The complexity and clarity of the ideas suggest a high likelihood of AI generation, although it\n",
            "AI detection score from LLM: 0.80. Is likely AI: True\n",
            "RL Environment stepped - Next State: [0.2723775  0.8        0.00586476 0.18187383 0.01      ], Simulated Reward: 0.9, Done: False\n",
            "Response 1 (Controversial Brainstorming):\n",
            "{'module_response': ['Here are five distinct and potentially controversial thesis ideas related to political science that you could explore in your research:', '1. **The Ethics of Political Assassination: A Justifiable Tool for Regime Change?**', 'Investigate historical cases where political assassinations were utilized to effect regime change. Discuss the moral implications, the outcomes, and the potential justification for such actions in the context of global governance and human rights. Analyze the tension between ethical considerations and political effectiveness.', '2. **Populism and Democracy: Catalysts for Political Reform or Threats to Liberal Order?**', 'Examine the rise of populist movements around the world and their impact on established democratic institutions. Delve into the argument that populism can be both a necessary response to elitism and a peril to democratic norms, alongside the implications for future governance and civil society.', '3. **The Role of Social Media in Shaping Political Polarization: Democratic Tool or Weapon of Division?**', 'Explore how social media platforms contribute to political polarization. Assess whether these platforms serve as a democracy-enhancing tool for political engagement or if they function more as amplifiers of division, misinformation, and extremist ideologies.', '4. **Climate Change Policy and National Sovereignty: The Case for Global Governance?**', 'Debate whether climate change action necessitates sacrificing aspects of national sovereignty in favor of a global governance model. Consider the political, ethical, and practical implications of enforced international policies and cooperation to combat climate change, potentially challenging traditional notions of state autonomy'], 'ethical_feedback': ['Potential AI-generated content detected (score: 0.80). Remember to review and rephrase.', \"RL Agent Recommendation (Action Index): 5 - 'Disable AI feature'\"], 'rl_recommended_action': 5}\n",
            "\n",
            "--- Test Case 2: Writing Support Request (drafting a literature review paragraph) ---\n",
            "\n",
            "Processing user request: 'draft_section' at stage 'literature review'\n",
            "Usage logged: Timestamp=2025-06-26 10:18:44.094792, Prompt='Draft a paragraph for section 2.2 summarizing a recent study on climate modeling.', Intent='draft_section', Thesis Stage='literature review'\n",
            "Drafting section based on outline: '1. Introduction\n",
            "2. Literature Review\n",
            "   2.1 Key co...' and context: 'Draft a paragraph for section 2.2 summarizing a re...'\n",
            "Drafted section: 2.2 Relevant Studies\n",
            "\n",
            "Recent advancements in climate modeling have been underscored by the work of S...\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.3\n",
            "\n",
            "The text reflects a structured and sophisticated analysis typical of human-written academic work, with detailed references to studies and methodologies that suggest expert knowledge. While AI can generate similar content, the depth of specificity and contextual relevance here leans more towards\n",
            "AI detection score from LLM: 0.30. Is likely AI: False\n",
            "RL Environment stepped - Next State: [0.2723775  0.8        0.00586476 0.18187383 0.02      ], Simulated Reward: 0.9249999999999999, Done: False\n",
            "Response 2 (Drafting Section):\n",
            "{'module_response': '2.2 Relevant Studies\\n\\nRecent advancements in climate modeling have been underscored by the work of Smith et al. (2023), who conducted an extensive analysis on the efficacy of new integrated assessment models (IAMs) in projecting future climate scenarios. Their study aimed to evaluate the accuracy of predictions generated by IAMs compared to traditional climate models, especially in light of escalating greenhouse gas emissions. The researchers utilized a comparative framework, assessing several modeling approaches against historical climate data and high-resolution climate projections. Their findings revealed that IAMs, while computationally intensive, provided significantly more nuanced insights into regional climate impacts, offering decision-makers detailed information on potential socio-economic consequences. Moreover, the study highlighted the importance of incorporating adaptive management strategies within these models, as they found that dynamic adaptability could enhance resilience to climate variability. This research not only contributes to theoretical advancements in climate modeling but also emphasizes the practical implications of incorporating innovative methodologies to better inform policy and response strategies in addressing climate change.', 'ethical_feedback': [\"RL Agent Recommendation (Action Index): 5 - 'Disable AI feature'\"], 'rl_recommended_action': 5}\n",
            "\n",
            "--- Test Case 3: Emotion Support Request (detecting frustration) ---\n",
            "\n",
            "Processing user request: 'detect_emotion' at stage 'writing'\n",
            "Usage logged: Timestamp=2025-06-26 10:18:47.970800, Prompt='I've been working on this chapter for hours and it's still not right!', Intent='detect_emotion', Thesis Stage='writing'\n",
            "Detecting emotion for input: 'I've been working on this chapter for hours and it's still not right!...'\n",
            "Detected emotion: Frustrated\n",
            "RL Environment stepped - Next State: [0.2723775  0.8        0.00586476 0.18187383 0.03      ], Simulated Reward: 0.9500000000000001, Done: False\n",
            "Response 3 (Detecting Frustration):\n",
            "{'module_response': 'Frustrated', 'ethical_feedback': [\"RL Agent Recommendation (Action Index): 5 - 'Disable AI feature'\"], 'rl_recommended_action': 5}\n",
            "\n",
            "--- Test Case 4: Emotion Support Request (providing encouragement) ---\n",
            "\n",
            "Processing user request: 'provide_encouragement' at stage 'writing'\n",
            "Usage logged: Timestamp=2025-06-26 10:18:48.260979, Prompt='I need some encouragement to keep going.', Intent='provide_encouragement', Thesis Stage='writing'\n",
            "Providing encouragement for emotion: 'Frustrated' at stage: 'writing'\n",
            "Encouraging message: I understand that the writing stage can feel overwhelming and frustrating at times, but remember that this is a crucial part of your journey, and you're already making incredible progress! Each word you write brings you one step closer to your goal. Take a deep breath\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "This text exhibits a motivational tone and a personal touch, which are often characteristic of human-generated content. The supportive language and emotional connection suggest a human writer.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "RL Environment stepped - Next State: [0.2723775  0.8        0.00586476 0.18187383 0.04      ], Simulated Reward: 0.975, Done: False\n",
            "Response 4 (Providing Encouragement):\n",
            "{'module_response': \"I understand that the writing stage can feel overwhelming and frustrating at times, but remember that this is a crucial part of your journey, and you're already making incredible progress! Each word you write brings you one step closer to your goal. Take a deep breath\", 'ethical_feedback': [\"RL Agent Recommendation (Action Index): 5 - 'Disable AI feature'\"], 'rl_recommended_action': 5}\n",
            "\n",
            "--- Test Case 5: Writing Support Request (rephrasing complex text) ---\n",
            "\n",
            "Processing user request: 'rephrase_text' at stage 'methodology'\n",
            "Usage logged: Timestamp=2025-06-26 10:18:50.487219, Prompt='The epistemological implications of quantum entanglement necessitate a re-evaluation of classical deterministic paradigms in the context of non-local hidden variable theories, thereby challenging the foundational tenets of conventional realism in empirical observation.', Intent='rephrase_text', Thesis Stage='methodology'\n",
            "Rephrasing text: 'The epistemological implications of quantum entanglement necessitate a re-evaluation of classical de...' in style: 'academic and clear'\n",
            "Rephrased text: The epistemological consequences of quantum entanglement require a reassessment of classical determi...\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.8  \n",
            "The text demonstrates advanced vocabulary and complex concepts typical of AI-generated content, particularly in academic or theoretical discourse. However, its coherence and depth suggest a familiarity with philosophical topics, which could also imply human authorship.\n",
            "AI detection score from LLM: 0.80. Is likely AI: True\n",
            "RL Environment stepped - Next State: [0.2723775  0.8        0.10586476 0.18187383 0.05      ], Simulated Reward: 1.0, Done: False\n",
            "Response 5 (Rephrasing Complex Text):\n",
            "{'module_response': 'The epistemological consequences of quantum entanglement require a reassessment of classical deterministic frameworks, particularly within the realm of non-local hidden variable theories. This reassessment calls into question the fundamental principles of traditional realism as it relates to empirical observation.', 'ethical_feedback': ['Potential AI-generated content detected (score: 0.80). Remember to review and rephrase.', 'Consider reviewing and significantly editing generated text to maintain authorship.', \"RL Agent Recommendation (Action Index): 5 - 'Disable AI feature'\"], 'rl_recommended_action': 5}\n",
            "\n",
            "--- Test Case 6: Emotion Support Request (celebrating a milestone) ---\n",
            "\n",
            "Processing user request: 'celebrate_milestone' at stage 'data analysis'\n",
            "Usage logged: Timestamp=2025-06-26 10:18:53.295866, Prompt='I finished analyzing all the data!', Intent='celebrate_milestone', Thesis Stage='data analysis'\n",
            "Celebrating milestone: 'completion of data analysis'\n",
            "Celebratory message: Congratulations on completing your data analysis! This is a significant milestone in your thesis journey, and it reflects your hard work, dedication, and analytical skills. You’ve taken a crucial step toward drawing meaningful insights\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "The text contains a personal touch and motivational language that suggests human-like engagement, making it less likely to be AI-generated.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "RL Environment stepped - Next State: [0.2723775  0.8        0.10586476 0.18187383 0.06      ], Simulated Reward: 1.025, Done: False\n",
            "Response 6 (Celebrating Milestone):\n",
            "{'module_response': 'Congratulations on completing your data analysis! This is a significant milestone in your thesis journey, and it reflects your hard work, dedication, and analytical skills. You’ve taken a crucial step toward drawing meaningful insights', 'ethical_feedback': [\"RL Agent Recommendation (Action Index): 5 - 'Disable AI feature'\"], 'rl_recommended_action': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eea51bf9",
        "outputId": "c5eedd83-3127-4cc8-9e6a-1da98e055336"
      },
      "source": [
        "# Assuming 'client' and 'config' are available from previous cells,\n",
        "# and the ThesisAssistant class is defined.\n",
        "\n",
        "if 'client' in locals() and 'config' in locals() and 'ThesisAssistant' in locals():\n",
        "    print(\"--- Testing Integrated ThesisAssistant System ---\")\n",
        "\n",
        "    # Initialize the ThesisAssistant\n",
        "    thesis_assistant = ThesisAssistant(openai_client=client, config=config)\n",
        "\n",
        "    # --- Test Case 1: Brainstorming Request with potential ethical flag ---\n",
        "    print(\"\\n--- Test Case 1: Brainstorming Request (focus on a sensitive topic) ---\")\n",
        "    response1 = thesis_assistant.process_user_request(\n",
        "        user_input=\"Generate controversial ideas for my thesis on political science.\",\n",
        "        request_type=\"brainstorm_ideas\",\n",
        "        thesis_stage=\"planning\"\n",
        "    )\n",
        "    print(\"Response 1 (Controversial Brainstorming):\")\n",
        "    print(response1)\n",
        "\n",
        "    # --- Test Case 2: Writing Support Request (drafting a section) ---\n",
        "    print(\"\\n--- Test Case 2: Writing Support Request (drafting a literature review paragraph) ---\")\n",
        "    mock_outline_lr = \"1. Introduction\\n2. Literature Review\\n   2.1 Key concepts\\n   2.2 Relevant studies\"\n",
        "    context_draft_lr = {\"outline\": mock_outline_lr}\n",
        "    response2 = thesis_assistant.process_user_request(\n",
        "        user_input=\"Draft a paragraph for section 2.2 summarizing a recent study on climate modeling.\",\n",
        "        request_type=\"draft_section\",\n",
        "        thesis_stage=\"literature review\",\n",
        "        context=context_draft_lr\n",
        "    )\n",
        "    print(\"Response 2 (Drafting Section):\")\n",
        "    print(response2)\n",
        "\n",
        "    # --- Test Case 3: Emotion Support Request (detecting frustration) ---\n",
        "    print(\"\\n--- Test Case 3: Emotion Support Request (detecting frustration) ---\")\n",
        "    response3 = thesis_assistant.process_user_request(\n",
        "        user_input=\"I've been working on this chapter for hours and it's still not right!\",\n",
        "        request_type=\"detect_emotion\",\n",
        "        thesis_stage=\"writing\"\n",
        "    )\n",
        "    print(\"Response 3 (Detecting Frustration):\")\n",
        "    print(response3)\n",
        "\n",
        "    # --- Test Case 4: Emotion Support Request (providing encouragement based on frustration) ---\n",
        "    if response3[\"module_response\"] != \"Undetermined\":\n",
        "        print(\"\\n--- Test Case 4: Emotion Support Request (providing encouragement) ---\")\n",
        "        context_encourage = {\"detected_emotion\": response3[\"module_response\"]}\n",
        "        response4 = thesis_assistant.process_user_request(\n",
        "            user_input=\"I need some encouragement to keep going.\",\n",
        "            request_type=\"provide_encouragement\",\n",
        "            thesis_stage=\"writing\",\n",
        "            context=context_encourage\n",
        "        )\n",
        "        print(\"Response 4 (Providing Encouragement):\")\n",
        "        print(response4)\n",
        "\n",
        "    # --- Test Case 5: Writing Support Request (rephrasing a long sentence - potential AI flag) ---\n",
        "    print(\"\\n--- Test Case 5: Writing Support Request (rephrasing complex text) ---\")\n",
        "    long_complex_sentence = \"The epistemological implications of quantum entanglement necessitate a re-evaluation of classical deterministic paradigms in the context of non-local hidden variable theories, thereby challenging the foundational tenets of conventional realism in empirical observation.\"\n",
        "    response5 = thesis_assistant.process_user_request(\n",
        "        user_input=long_complex_sentence,\n",
        "        request_type=\"rephrase_text\",\n",
        "        thesis_stage=\"methodology\"\n",
        "    )\n",
        "    print(\"Response 5 (Rephrasing Complex Text):\")\n",
        "    print(response5)\n",
        "\n",
        "    # --- Test Case 6: Emotion Support Request (celebrating a milestone) ---\n",
        "    print(\"\\n--- Test Case 6: Emotion Support Request (celebrating a milestone) ---\")\n",
        "    context_milestone = {\"milestone\": \"completion of data analysis\"}\n",
        "    response6 = thesis_assistant.process_user_request(\n",
        "        user_input=\"I finished analyzing all the data!\",\n",
        "        request_type=\"celebrate_milestone\",\n",
        "        thesis_stage=\"data analysis\",\n",
        "        context=context_milestone\n",
        "    )\n",
        "    print(\"Response 6 (Celebrating Milestone):\")\n",
        "    print(response6)\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Required objects (client, config, ThesisAssistant) not initialized.\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing Integrated ThesisAssistant System ---\n",
            "Initialized new PPO model.\n",
            "\n",
            "--- Test Case 1: Brainstorming Request (focus on a sensitive topic) ---\n",
            "\n",
            "Processing user request: 'brainstorm_ideas' at stage 'planning'\n",
            "Usage logged: Timestamp=2025-06-26 10:17:44.514996, Prompt='Generate controversial ideas for my thesis on political science.', Intent='brainstorm_ideas', Thesis Stage='planning'\n",
            "Generating 5 ideas for prompt: 'Generate controversial ideas for my thesis on political science.' at thesis stage: 'planning'\n",
            "Generated ideas: ['Here are five distinct and controversial ideas for your political science thesis:', '1. **The Impact of Social Media Algorithms on Political Polarization**: Explore how social media platforms influence voter behavior and political polarization through their algorithms. Analyze whether these algorithms promote divisive content and if so, propose regulatory measures to mitigate these effects.', '2. **The Legitimacy of Political Violence in Protest Movements**: Investigate the ethical implications of political violence within protest movements, focusing on events like the Capitol riots or Black Lives Matter. Assess when, if ever, such actions can be justified and how these events influence public perception of the movements involved.', '3. **Rethinking National Sovereignty in a Globalized World**: Argue for or against the concept of national sovereignty in the face of global challenges such as climate change and pandemics. Discuss whether sovereignty should be compromised for global governance models that prioritize collective action over individual state interests.', '4. **The Role of Populism in Democratic Erosion**: Analyze the rise of populist leaders worldwide and their impact on democratic institutions. Evaluate whether populist movements serve as a necessary critique of elite governance or if they contribute to the erosion of democratic norms and values.', '5. **Exploring the Ethics of Surveillance States in Times of Crisis**: Delve into the trade-offs between national security and public privacy during crises such as pandemics or terrorism. Discuss the moral implications of surveillance technologies employed by governments and whether they justify the infringement on individual liberties in the name']\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.8\n",
            "\n",
            "The text demonstrates a structured format, advanced vocabulary, and well-defined thesis ideas typical of academic writing. While such content could be crafted by a human, its clarity and precision suggest a high likelihood of aid from AI, which often excels\n",
            "AI detection score from LLM: 0.80. Is likely AI: True\n",
            "RL Environment stepped - Next State: [0.22982244 0.8        0.76543844 0.5578047  0.01      ], Simulated Reward: 0.9, Done: False\n",
            "Response 1 (Controversial Brainstorming):\n",
            "{'module_response': ['Here are five distinct and controversial ideas for your political science thesis:', '1. **The Impact of Social Media Algorithms on Political Polarization**: Explore how social media platforms influence voter behavior and political polarization through their algorithms. Analyze whether these algorithms promote divisive content and if so, propose regulatory measures to mitigate these effects.', '2. **The Legitimacy of Political Violence in Protest Movements**: Investigate the ethical implications of political violence within protest movements, focusing on events like the Capitol riots or Black Lives Matter. Assess when, if ever, such actions can be justified and how these events influence public perception of the movements involved.', '3. **Rethinking National Sovereignty in a Globalized World**: Argue for or against the concept of national sovereignty in the face of global challenges such as climate change and pandemics. Discuss whether sovereignty should be compromised for global governance models that prioritize collective action over individual state interests.', '4. **The Role of Populism in Democratic Erosion**: Analyze the rise of populist leaders worldwide and their impact on democratic institutions. Evaluate whether populist movements serve as a necessary critique of elite governance or if they contribute to the erosion of democratic norms and values.', '5. **Exploring the Ethics of Surveillance States in Times of Crisis**: Delve into the trade-offs between national security and public privacy during crises such as pandemics or terrorism. Discuss the moral implications of surveillance technologies employed by governments and whether they justify the infringement on individual liberties in the name'], 'ethical_feedback': ['Potential AI-generated content detected (score: 0.80). Remember to review and rephrase.', \"RL Agent Recommendation (Action Index): 5 - 'Disable AI feature'\"], 'rl_recommended_action': 5}\n",
            "\n",
            "--- Test Case 2: Writing Support Request (drafting a literature review paragraph) ---\n",
            "\n",
            "Processing user request: 'draft_section' at stage 'literature review'\n",
            "Usage logged: Timestamp=2025-06-26 10:17:49.894946, Prompt='Draft a paragraph for section 2.2 summarizing a recent study on climate modeling.', Intent='draft_section', Thesis Stage='literature review'\n",
            "Drafting section based on outline: '1. Introduction\n",
            "2. Literature Review\n",
            "   2.1 Key co...' and context: 'Draft a paragraph for section 2.2 summarizing a re...'\n",
            "Drafted section: ### 2.2 Relevant Studies\n",
            "\n",
            "Recent advancements in climate modeling have been significantly informed b...\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.7\n",
            "\n",
            "The text exhibits a structured and formal writing style typical of academic literature, which could suggest AI generation. However, the specific reference to a recent study, detailed results, and nuanced discussion of interdisciplinary approaches imply a level of specificity and scholarly\n",
            "AI detection score from LLM: 0.70. Is likely AI: False\n",
            "RL Environment stepped - Next State: [0.22982244 0.8        0.76543844 0.5578047  0.02      ], Simulated Reward: 0.9249999999999999, Done: False\n",
            "Response 2 (Drafting Section):\n",
            "{'module_response': '### 2.2 Relevant Studies\\n\\nRecent advancements in climate modeling have been significantly informed by the study conducted by Smith et al. (2023), which introduced a novel hybrid approach that integrates machine learning techniques with traditional climate simulation frameworks. This study emphasized the potential of leveraging artificial intelligence to enhance the accuracy and efficiency of climate predictions, particularly in regional modeling scenarios. By employing a vast dataset collected over three decades, the researchers demonstrated that their hybrid model could reduce the computational time required for simulations by up to 40% while simultaneously improving predictive accuracy compared to conventional methods. The findings indicated that incorporating machine learning not only aids in refining parameters but also helps capture complex climate interactions that are often overlooked in standard models. This study highlights the transformative potential of interdisciplinary approaches in climate science, suggesting that future research could benefit from further integration of technological innovations within climate modeling methodologies.', 'ethical_feedback': [\"RL Agent Recommendation (Action Index): 5 - 'Disable AI feature'\"], 'rl_recommended_action': 5}\n",
            "\n",
            "--- Test Case 3: Emotion Support Request (detecting frustration) ---\n",
            "\n",
            "Processing user request: 'detect_emotion' at stage 'writing'\n",
            "Usage logged: Timestamp=2025-06-26 10:17:57.837541, Prompt='I've been working on this chapter for hours and it's still not right!', Intent='detect_emotion', Thesis Stage='writing'\n",
            "Detecting emotion for input: 'I've been working on this chapter for hours and it's still not right!...'\n",
            "Detected emotion: Frustrated\n",
            "RL Environment stepped - Next State: [0.22982244 0.8        0.76543844 0.5578047  0.03      ], Simulated Reward: 0.9500000000000001, Done: False\n",
            "Response 3 (Detecting Frustration):\n",
            "{'module_response': 'Frustrated', 'ethical_feedback': [\"RL Agent Recommendation (Action Index): 5 - 'Disable AI feature'\"], 'rl_recommended_action': 5}\n",
            "\n",
            "--- Test Case 4: Emotion Support Request (providing encouragement) ---\n",
            "\n",
            "Processing user request: 'provide_encouragement' at stage 'writing'\n",
            "Usage logged: Timestamp=2025-06-26 10:17:58.210266, Prompt='I need some encouragement to keep going.', Intent='provide_encouragement', Thesis Stage='writing'\n",
            "Providing encouragement for emotion: 'Frustrated' at stage: 'writing'\n",
            "Encouraging message: I understand that the writing stage can be incredibly frustrating, but remember that every word you put down is a step closer to your goal! It’s completely normal to hit bumps along the way, but keep pushing through. You’ve come so far already,\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "The text reflects a motivational and supportive human tone, characteristic of personal encouragement. While AI can produce similar content, the warmth and personalized touch suggest it is more likely written by a person.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "RL Environment stepped - Next State: [0.22982244 0.8        0.76543844 0.5578047  0.04      ], Simulated Reward: 0.975, Done: False\n",
            "Response 4 (Providing Encouragement):\n",
            "{'module_response': 'I understand that the writing stage can be incredibly frustrating, but remember that every word you put down is a step closer to your goal! It’s completely normal to hit bumps along the way, but keep pushing through. You’ve come so far already,', 'ethical_feedback': [\"RL Agent Recommendation (Action Index): 5 - 'Disable AI feature'\"], 'rl_recommended_action': 5}\n",
            "\n",
            "--- Test Case 5: Writing Support Request (rephrasing complex text) ---\n",
            "\n",
            "Processing user request: 'rephrase_text' at stage 'methodology'\n",
            "Usage logged: Timestamp=2025-06-26 10:18:00.813616, Prompt='The epistemological implications of quantum entanglement necessitate a re-evaluation of classical deterministic paradigms in the context of non-local hidden variable theories, thereby challenging the foundational tenets of conventional realism in empirical observation.', Intent='rephrase_text', Thesis Stage='methodology'\n",
            "Rephrasing text: 'The epistemological implications of quantum entanglement necessitate a re-evaluation of classical de...' in style: 'academic and clear'\n",
            "Rephrased text: The epistemological consequences of quantum entanglement require a reconsideration of classical dete...\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "The text demonstrates a complex understanding of quantum mechanics and philosophical concepts, suggesting a high level of expertise and nuance that is less common in AI-generated content, which often lacks depth in specialized discussions.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "RL Environment stepped - Next State: [0.22982244 0.8        0.86543846 0.5578047  0.05      ], Simulated Reward: 1.0, Done: False\n",
            "Response 5 (Rephrasing Complex Text):\n",
            "{'module_response': 'The epistemological consequences of quantum entanglement require a reconsideration of classical deterministic frameworks, particularly in relation to non-local hidden variable theories. This re-evaluation brings into question the fundamental principles of traditional realism concerning empirical observation.', 'ethical_feedback': ['Consider reviewing and significantly editing generated text to maintain authorship.', \"RL Agent Recommendation (Action Index): 5 - 'Disable AI feature'\"], 'rl_recommended_action': 5}\n",
            "\n",
            "--- Test Case 6: Emotion Support Request (celebrating a milestone) ---\n",
            "\n",
            "Processing user request: 'celebrate_milestone' at stage 'data analysis'\n",
            "Usage logged: Timestamp=2025-06-26 10:18:04.574182, Prompt='I finished analyzing all the data!', Intent='celebrate_milestone', Thesis Stage='data analysis'\n",
            "Celebrating milestone: 'completion of data analysis'\n",
            "Celebratory message: Congratulations on completing the data analysis phase of your thesis! This is a significant milestone and a testament to your hard work and dedication. You've gathered insights that will shape your research, and it's exciting to see\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.3\n",
            "\n",
            "The text displays a personal and motivational tone, typical of human communication, especially in academic contexts. While it’s possible for AI to generate similar content, the specific reference to the recipient's efforts and emotions suggests a human touch.\n",
            "AI detection score from LLM: 0.30. Is likely AI: False\n",
            "RL Environment stepped - Next State: [0.22982244 0.8        0.86543846 0.5578047  0.06      ], Simulated Reward: 1.025, Done: False\n",
            "Response 6 (Celebrating Milestone):\n",
            "{'module_response': \"Congratulations on completing the data analysis phase of your thesis! This is a significant milestone and a testament to your hard work and dedication. You've gathered insights that will shape your research, and it's exciting to see\", 'ethical_feedback': [\"RL Agent Recommendation (Action Index): 5 - 'Disable AI feature'\"], 'rl_recommended_action': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b3b90f3"
      },
      "source": [
        "# Thesis Assistant with Ethics Module and New Functionalities\n",
        "\n",
        "We have successfully programmed the ethics module based on the provided structure, focusing on authorship tracking, AI labelling, and human-in-the-loop prompts, orchestrated by an EthicsSupervisor Agent (simulated via RL components). We have also designed and implemented three new modules: Idea Brainstorming, Writing Support, and Emotion Support, and integrated them with the existing ethics framework.\n",
        "\n",
        "Here's a summary of the key components implemented:\n",
        "\n",
        "1.  **Ethics Module (`EthicsModule`):** Handles core ethical considerations, including usage logging (`log_usage`), AI detection (`detect_ai`), and basic ethical checks (`check_ethical_usage`). It also interacts with the RL components.\n",
        "2.  **RL System Components (`RLConfigManager`, `DataPreprocessor`, `MockEthicsModule`, `EthicsSupervisorEnv`, `EthicsSupervisorRL`, `RLTrainingLoop`, `ThesisStudentSimulator`, `ThesisCohortSimulator`, `SyntheticRLPretrainer`, `RLTrainingLauncher`):** These classes provide the framework for the Reinforcement Learning supervisor. They manage the RL configuration, process data into states and rewards, simulate the environment and student behavior, train the PPO agent, and orchestrate the training pipeline.\n",
        "3.  **Idea Brainstorming Module (`IdeaBrainstorming`):** Assists with generating and refining thesis ideas using an LLM.\n",
        "4.  **Writing Support Module (`WritingSupport`):** Provides functionalities for creating outlines, drafting text, summarizing, rephrasing, and checking grammar/style using an LLM.\n",
        "5.  **Emotion Support Module (`EmotionSupport`):** Offers emotional encouragement and support based on detected user sentiment using an LLM.\n",
        "6.  **Thesis Assistant Orchestrator (`ThesisAssistant`):** This central class integrates all the modules. It receives user requests, directs them to the appropriate module, logs usage, performs ethical checks (including AI detection), interacts with the RL agent for action recommendations, and provides a combined response.\n",
        "\n",
        "## How to Use the Complete Thesis Assistant\n",
        "\n",
        "To use the integrated thesis assistant, you will primarily interact with the `ThesisAssistant` class.\n",
        "\n",
        "1.  **Ensure all necessary cells have been run:** Before using the `ThesisAssistant`, make sure you have executed all the code cells that define the classes (`EthicsModule`, `IdeaBrainstorming`, `WritingSupport`, `EmotionSupport`, all RL components, and `ThesisAssistant`) and initialize the required objects like the OpenAI `client` and the `config` dictionary.\n",
        "2.  **Initialize the `ThesisAssistant`:** Create an instance of the `ThesisAssistant` class, passing your initialized OpenAI client and the loaded RL configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "159f5ee3",
        "outputId": "f52e072c-6c84-4a9c-d52b-5a7b0f41159e"
      },
      "source": [
        "    # Example: Brainstorming request\n",
        "    response = thesis_assistant.process_user_request(\n",
        "        user_input=\"Suggest topics for my thesis on renewable energy.\",\n",
        "        request_type=\"brainstorm_ideas\",\n",
        "        thesis_stage=\"planning\",\n",
        "        context={\"n_ideas\": 5}\n",
        "    )\n",
        "    print(\"Brainstorming Response:\", response)\n",
        "\n",
        "    # Example: Writing support request (summarizing text)\n",
        "    text_to_summarize = \"Your long text goes here...\"\n",
        "    response = thesis_assistant.process_user_request(\n",
        "        user_input=text_to_summarize,\n",
        "        request_type=\"summarize_text\",\n",
        "        thesis_stage=\"literature review\"\n",
        "    )\n",
        "    print(\"Summarization Response:\", response)\n",
        "\n",
        "    # Example: Emotion support request (getting encouragement)\n",
        "    response = thesis_assistant.process_user_request(\n",
        "        user_input=\"I'm feeling really stuck.\",\n",
        "        request_type=\"provide_encouragement\",\n",
        "        thesis_stage=\"writing\",\n",
        "        context={\"detected_emotion\": \"Frustrated\"} # You would get the detected emotion first\n",
        "    )\n",
        "    print(\"Encouragement Response:\", response)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing user request: 'brainstorm_ideas' at stage 'planning'\n",
            "Usage logged: Timestamp=2025-06-26 10:20:22.310683, Prompt='Suggest topics for my thesis on renewable energy.', Intent='brainstorm_ideas', Thesis Stage='planning'\n",
            "Generating 5 ideas for prompt: 'Suggest topics for my thesis on renewable energy.' at thesis stage: 'planning'\n",
            "Generated ideas: ['Here are five distinct and relevant thesis topics centered on renewable energy that you can consider as you plan your research:', '1. **Integrating Renewable Energy Sources in Urban Environments**: Examine the feasibility and impact of combining solar, wind, and biomass energy solutions to create a sustainable energy system for urban areas. This study could involve case studies of cities that have successfully implemented such systems and analyze the social, economic, and environmental impacts.', '2. **The Role of Energy Storage Technologies in Facilitating Renewable Energy Adoption**: Investigate the advancements in battery storage technologies and their implications for integrating renewable energy sources into the grid. Focus on how these technologies can mitigate the intermittency issues of solar and wind power and the challenges of scaling them for widespread use.', '3. **Evaluating the Economic Viability of Community-Supported Renewable Energy Projects**: Analyze the financial models of community-driven renewable energy initiatives, such as solar cooperatives or wind farms. Assess how these projects can provide economic benefits, enhance local engagement, and contribute to energy independence in rural areas.', '4. **Impact of Policy and Regulation on Renewable Energy Development in Developing Nations**: Study the regulatory frameworks and policies that affect the growth of renewable energy sectors in a specific developing country. Explore how government incentives, international agreements, and local initiatives can spur renewable energy adoption and address energy poverty.', '5. **The Future of Offshore Wind Energy: Environmental and Technological Challenges**: Investigate the potential of offshore wind farms as a significant source of renewable energy.']\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.9\n",
            "\n",
            "The text exhibits characteristics typical of AI-generated content, including a structured format, clear organization, and formal language. The coherent presentation of thesis topics suggests a systematic approach often seen in AI outputs. However, the specificity in topic selection could\n",
            "AI detection score from LLM: 0.90. Is likely AI: True\n",
            "RL Environment stepped - Next State: [0.2025926  0.9        0.33032277 0.2801921  0.05      ], Simulated Reward: 1.04, Done: False\n",
            "Brainstorming Response: {'module_response': ['Here are five distinct and relevant thesis topics centered on renewable energy that you can consider as you plan your research:', '1. **Integrating Renewable Energy Sources in Urban Environments**: Examine the feasibility and impact of combining solar, wind, and biomass energy solutions to create a sustainable energy system for urban areas. This study could involve case studies of cities that have successfully implemented such systems and analyze the social, economic, and environmental impacts.', '2. **The Role of Energy Storage Technologies in Facilitating Renewable Energy Adoption**: Investigate the advancements in battery storage technologies and their implications for integrating renewable energy sources into the grid. Focus on how these technologies can mitigate the intermittency issues of solar and wind power and the challenges of scaling them for widespread use.', '3. **Evaluating the Economic Viability of Community-Supported Renewable Energy Projects**: Analyze the financial models of community-driven renewable energy initiatives, such as solar cooperatives or wind farms. Assess how these projects can provide economic benefits, enhance local engagement, and contribute to energy independence in rural areas.', '4. **Impact of Policy and Regulation on Renewable Energy Development in Developing Nations**: Study the regulatory frameworks and policies that affect the growth of renewable energy sectors in a specific developing country. Explore how government incentives, international agreements, and local initiatives can spur renewable energy adoption and address energy poverty.', '5. **The Future of Offshore Wind Energy: Environmental and Technological Challenges**: Investigate the potential of offshore wind farms as a significant source of renewable energy.'], 'ethical_feedback': ['Potential AI-generated content detected (score: 0.90). Remember to review and rephrase.', \"RL Agent Recommendation (Action Index): 3 - 'Suggest rewriting'\"], 'rl_recommended_action': 3}\n",
            "\n",
            "Processing user request: 'summarize_text' at stage 'literature review'\n",
            "Usage logged: Timestamp=2025-06-26 10:20:28.676980, Prompt='Your long text goes here...', Intent='summarize_text', Thesis Stage='literature review'\n",
            "Summarizing text: 'Your long text goes here......'\n",
            "Summary: Please provide the text you would like summarized....\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.1  \n",
            "The text is a simple, straightforward request for information, lacking the complexity or stylistic traits typically associated with AI-generated content.\n",
            "AI detection score from LLM: 0.10. Is likely AI: False\n",
            "RL Environment stepped - Next State: [0.2025926  0.9        0.23032278 0.2801921  0.06      ], Simulated Reward: 1.065, Done: False\n",
            "Summarization Response: {'module_response': 'Please provide the text you would like summarized.', 'ethical_feedback': [\"RL Agent Recommendation (Action Index): 3 - 'Suggest rewriting'\"], 'rl_recommended_action': 3}\n",
            "\n",
            "Processing user request: 'provide_encouragement' at stage 'writing'\n",
            "Usage logged: Timestamp=2025-06-26 10:20:30.175966, Prompt='I'm feeling really stuck.', Intent='provide_encouragement', Thesis Stage='writing'\n",
            "Providing encouragement for emotion: 'Frustrated' at stage: 'writing'\n",
            "Encouraging message: I know writing your thesis can feel overwhelming at times, but remember that every word you put down brings you one step closer to your goal. Frustration is a sign that you care deeply about your work, and that passion will shine through in your\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "The text has a motivational tone and personal touch that suggests it may be human-generated. It reflects empathy and understanding of the challenges of thesis writing, which AI typically does not express as naturally.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "RL Environment stepped - Next State: [0.2025926  0.9        0.13032278 0.2801921  0.07      ], Simulated Reward: 1.0899999999999999, Done: False\n",
            "Encouragement Response: {'module_response': 'I know writing your thesis can feel overwhelming at times, but remember that every word you put down brings you one step closer to your goal. Frustration is a sign that you care deeply about your work, and that passion will shine through in your', 'ethical_feedback': [\"RL Agent Recommendation (Action Index): 3 - 'Suggest rewriting'\"], 'rl_recommended_action': 3}\n"
          ]
        }
      ]
    }
  ]
}