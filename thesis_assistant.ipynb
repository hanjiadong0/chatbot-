{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkaeMNwQuyrM/2Wn+mfyQF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanjiadong0/chatbot-/blob/RL/thesis_assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s04Q8trlh9nJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8008a577"
      },
      "source": [
        "# Task\n",
        "Program the ethics module for your thesis killer project based on the provided structure, focusing on authorship tracking, AI labelling, and human-in-the-loop prompts, orchestrated by an EthicsSupervisor Agent and utilizing the defined submodules (AI_Detector, Usage_Logger, HumanPromptChecker, AdvisorFeedbackSync, EthicalViolationAlert) and interfaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d2bc5dd"
      },
      "source": [
        "## Define the scope of the ethics module\n",
        "\n",
        "### Subtask:\n",
        "Clarify what aspects of ethics the module should address within the context of your thesis killer project, focusing on authorship tracking, AI labelling, and human-in-the-loop prompts, orchestrated by an EthicsSupervisor Agent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bdc5a79"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the ethical concerns, explain the relation of authorship tracking, AI labelling, and human-in-the-loop prompts, outline the responsibilities of the EthicsSupervisor Agent, and briefly explain the contribution of the submodules, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b709aa3a"
      },
      "source": [
        "# Task\n",
        "Program the ethics module for your thesis killer project based on the provided description, including the EthicsSupervisor Agent implemented as a Reinforcement Learning monitor, the specified submodules (AI_Detector, Usage_Logger, HumanPromptChecker, AdvisorFeedbackSync, EthicalViolationAlert), and the interfaces for connecting to external tools and logging information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ad5129f"
      },
      "source": [
        "## Define the scope of the ethics module\n",
        "\n",
        "### Subtask:\n",
        "Clarify what aspects of ethics the module should address within the context of your thesis killer project, focusing on authorship tracking, AI labelling, and human-in-the-loop prompts, orchestrated by an EthicsSupervisor Agent implemented as a Reinforcement Learning monitor.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0bd7ab6"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the ethical concerns, explain the relation of authorship tracking, AI labelling, and human-in-the-loop prompts, outline the responsibilities of the EthicsSupervisor Agent, and briefly explain the contribution of the submodules, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae65fbf7"
      },
      "source": [
        "## Identify relevant ethical guidelines or frameworks\n",
        "\n",
        "### Subtask:\n",
        "Research and select appropriate ethical principles or frameworks applicable to your project's domain, considering how they relate to the functions of the defined submodules and how these can be translated into states, actions, and reward signals for the RL-based EthicsSupervisor.\n",
        "\n",
        "**Reasoning**:\n",
        "Selecting appropriate ethical guidelines is crucial for ensuring the ethics module effectively addresses the challenges identified in the project's presentation. These guidelines will inform the design and implementation of the EthicsSupervisor and its submodules, as well as the definition of states, actions, and reward signals for the Reinforcement Learning approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1689d8f"
      },
      "source": [
        "## Design the module's structure\n",
        "\n",
        "### Subtask:\n",
        "Outline the components and functionalities of the ethics module, with the EthicsSupervisor Agent implemented as a Reinforcement Learning monitor that has access to agent decisions, user responses, LLM-generated content, timing logs, and human feedback loops. Define the roles and interactions of the submodules (AI_Detector, Usage_Logger, HumanPromptChecker, AdvisorFeedbackSync, and EthicalViolationAlert) and how they will utilize interfaces to connect with tools like GPTZero, Copyleaks, or custom DetectGPT, log timestamps, usage intent, and tool confidence, and use rules/classifiers for warnings and suggestions. Design how the information from these submodules and interfaces will be used as state, action, and reward signals for the RL model.\n",
        "\n",
        "**Reasoning**:\n",
        "A well-defined structure is essential for implementing a complex module like the ethics module. Clearly outlining the roles and interactions of the EthicsSupervisor, submodules, and interfaces, and specifically designing how information will be used for the RL model, will ensure a cohesive and functional design that addresses the identified ethical challenges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fa79dc9"
      },
      "source": [
        "## Design the module's structure\n",
        "\n",
        "### Subtask:\n",
        "Outline the components and functionalities of the ethics module, with the EthicsSupervisor Agent implemented as a Reinforcement Learning monitor that has access to agent decisions, user responses, LLM-generated content, timing logs, and human feedback loops. Define the roles and interactions of the submodules (AI_Detector, Usage_Logger, HumanPromptChecker, AdvisorFeedbackSync, and EthicalViolationAlert) and how they will utilize interfaces to connect with **AI detector tools**, log timestamps, usage intent, and tool confidence, and use rules/classifiers for warnings and suggestions. Design how the information from these submodules and interfaces will be used as state, action, and reward signals for the RL model.\n",
        "\n",
        "**Reasoning**:\n",
        "A well-defined structure is essential for implementing a complex module like the ethics module. Clearly outlining the roles and interactions of the EthicsSupervisor, submodules, and interfaces, and specifically designing how information will be used for the RL model, will ensure a cohesive and functional design that addresses the identified ethical challenges."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import OpenAIServerModel\n",
        "api_key = \"AIzaSyBNqQzrD75wV8WfGsV27VHUZ9j5ts5ihMg\"   # use some free api key\n",
        "model = OpenAIServerModel(\n",
        "    model_id=\"gemini-2.0-flash\", # the model I used\n",
        "    api_base=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        "    api_key=api_key,\n",
        ")"
      ],
      "metadata": {
        "id": "JsrtVPUa1qT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import the OpenAI library\n",
        "from openai import OpenAI\n",
        "# Used to securely store your API key - uncomment if using Colab Secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get your OpenAI API key securely\n",
        "# Replace \"<YOUR_OPENAI_API_KEY>\" with your key, or use Colab Secrets\n",
        "# Or if using Colab Secrets:\n",
        "openai_api_key_secure = userdata.get('OPENAI_API_KEY')\n",
        "openai_organization = userdata.get('OPENAI_ORGANIZATION')\n",
        "openai_project = userdata.get('OPENAI_PROJECT_ID')\n",
        "\n",
        "# Set your project API key\n",
        "OpenAI.api_key = openai_api_key_secure\n",
        "# You must also set organization and project ID\n",
        "OpenAI.organization = openai_organization\n",
        "OpenAI.project = openai_project\n",
        "\n",
        "# Create the OpenAI client\n",
        "client = OpenAI(api_key= OpenAI.api_key)\n",
        "\n"
      ],
      "metadata": {
        "id": "5NDgGXE0tS15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a request to the Chat Completions endpoint\n",
        "response = client.chat.completions.create(\n",
        "  # Specify the model\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    # Assign the correct role\n",
        "    {\"role\": \"user\",\n",
        "     \"content\": \"Write a polite reply accepting an AI Engineer job offer within 20 words.\"}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Brf7k9_ytsni",
        "outputId": "94f9c3c9-5fc4-4584-8e42-49472f3587d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: Acceptance of Job Offer\n",
            "\n",
            "Dear [Hiring Manager's Name],\n",
            "\n",
            "I am thrilled to accept the AI Engineer position. Thank you for this opportunity!\n",
            "\n",
            "Best regards,  \n",
            "[Your Name]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a9d1e9e"
      },
      "source": [
        "import pandas as pd\n",
        "import time # Import time for simulating API call delay\n",
        "from openai import OpenAI # Import the OpenAI library\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata # Uncomment if using Colab Secrets\n",
        "from scipy.spatial import distance # Assuming scipy is installed\n",
        "\n",
        "class EthicsModule:\n",
        "    def __init__(self, openai_client):\n",
        "        self.usage_logs = []\n",
        "        self.usage_embeddings = [] # Initialize list to store embeddings\n",
        "        self.ai_detection_threshold = 0.7 # Simple threshold for AI detection\n",
        "        # Use the provided OpenAI client\n",
        "        self.client = openai_client\n",
        "\n",
        "\n",
        "    def log_usage(self, prompt, intent, thesis_stage=\"unknown\"):\n",
        "        \"\"\"Logs the usage of the thesis assistant with more details.\"\"\"\n",
        "        log_entry = {\n",
        "            'timestamp': pd.Timestamp.now(),\n",
        "            'prompt': prompt,\n",
        "            'intent': intent,\n",
        "            'thesis_stage': thesis_stage # Added thesis stage\n",
        "        }\n",
        "        self.usage_logs.append(log_entry)\n",
        "        print(f\"Usage logged: Timestamp={log_entry['timestamp']}, Prompt='{prompt}', Intent='{intent}', Thesis Stage='{thesis_stage}'\")\n",
        "        # Optionally generate embedding for the new log entry immediately\n",
        "        # self._generate_embedding_for_log(log_entry)\n",
        "\n",
        "\n",
        "    def _generate_embedding_for_log(self, log_entry):\n",
        "         \"\"\"Generates embedding for a single log entry and stores it.\"\"\"\n",
        "         try:\n",
        "             prompt_text = log_entry['prompt']\n",
        "             response = self.client.embeddings.create(\n",
        "                 model=\"text-embedding-3-small\", # Use the embedding model\n",
        "                 input=prompt_text\n",
        "             )\n",
        "             embedding = response.data[0].embedding\n",
        "             # Store the embedding along with a reference to the original log index\n",
        "             self.usage_embeddings.append({'embedding': embedding, 'original_index': len(self.usage_logs) - 1})\n",
        "             print(f\"Generated embedding for log entry {len(self.usage_logs) - 1}\")\n",
        "         except Exception as e:\n",
        "             print(f\"Error generating embedding for log entry: {e}\")\n",
        "\n",
        "\n",
        "    def generate_all_usage_embeddings(self):\n",
        "        \"\"\"Generates embeddings for all usage logs using OpenAI API.\"\"\"\n",
        "        print(\"Generating embeddings for all usage logs using OpenAI API...\")\n",
        "        self.usage_embeddings = [] # Clear existing embeddings\n",
        "        for i, log_entry in enumerate(self.usage_logs):\n",
        "            try:\n",
        "                prompt_text = log_entry['prompt']\n",
        "                response = self.client.embeddings.create(\n",
        "                    model=\"text-embedding-3-small\", # Use the embedding model\n",
        "                    input=prompt_text\n",
        "                )\n",
        "                embedding = response.data[0].embedding\n",
        "                # Store the embedding along with a reference to the original log index\n",
        "                self.usage_embeddings.append({'embedding': embedding, 'original_index': i})\n",
        "            except Exception as e:\n",
        "                 print(f\"Error generating embedding for log entry {i}: {e}\")\n",
        "\n",
        "        print(f\"Generated {len(self.usage_embeddings)} embeddings.\")\n",
        "\n",
        "\n",
        "    def find_similar_usage(self, query_prompt, n=3):\n",
        "        \"\"\"\n",
        "        Finds the n most similar usage logs based on prompt embedding similarity.\n",
        "\n",
        "        Args:\n",
        "            query_prompt (str): The prompt to find similar usage for.\n",
        "            n (int): The number of closest usage logs to find.\n",
        "\n",
        "        Returns:\n",
        "            list of dict: A list of dictionaries for the n most similar usage logs,\n",
        "                          each containing 'distance', 'original_log' (the full log entry).\n",
        "                          Returns an empty list if no embeddings are available.\n",
        "        \"\"\"\n",
        "        if not self.usage_embeddings:\n",
        "            print(\"No usage embeddings available to query.\")\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Generate embedding for the query prompt\n",
        "            query_response = self.client.embeddings.create(\n",
        "                model=\"text-embedding-3-small\", # Use the embedding model\n",
        "                input=query_prompt\n",
        "            )\n",
        "            query_embedding = query_response.data[0].embedding\n",
        "\n",
        "            distances = []\n",
        "            for item in self.usage_embeddings:\n",
        "                dist = distance.cosine(query_embedding, item['embedding'])\n",
        "                distances.append({\n",
        "                    \"distance\": dist,\n",
        "                    \"original_index\": item['original_index']\n",
        "                    })\n",
        "\n",
        "            distances_sorted = sorted(distances, key=lambda x: x['distance'])\n",
        "\n",
        "            # Get the original log entries for the n closest\n",
        "            similar_logs = []\n",
        "            for item in distances_sorted[0:n]:\n",
        "                original_log = self.usage_logs[item['original_index']]\n",
        "                similar_logs.append({\n",
        "                    \"distance\": item['distance'],\n",
        "                    \"original_log\": original_log\n",
        "                })\n",
        "\n",
        "            return similar_logs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during similar usage query: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "    def detect_ai(self, text):\n",
        "        \"\"\"Uses the OpenAI LLM to assess if content is AI generated.\"\"\"\n",
        "        print(\"Using OpenAI LLM for AI detection...\")\n",
        "        try:\n",
        "            # Craft a prompt for the LLM to assess AI generation\n",
        "            # This prompt might need refinement for better results\n",
        "            prompt_text = f\"Assess the likelihood that the following text was generated by an AI. Respond ONLY with a score between 0 and 1, where 1 is highly likely to be AI generated, followed by a brief explanation on a new line.\\n\\nText to assess:\\n{text}\\n\\nScore:\"\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\", # Or another suitable model\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an AI text detection assistant.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt_text}\n",
        "                ],\n",
        "                max_tokens=50 # Restrict tokens to manage cost\n",
        "            )\n",
        "\n",
        "            # Attempt to parse the score from the LLM's response\n",
        "            response_text = response.choices[0].message.content.strip()\n",
        "            print(f\"LLM Raw Response: {response_text}\") # Print raw response for debugging\n",
        "            try:\n",
        "                # Assuming the LLM starts the response with the score on the first line\n",
        "                detection_score = float(response_text.splitlines()[0])\n",
        "            except (ValueError, IndexError):\n",
        "                print(f\"Could not parse score from LLM response: '{response_text}'. Assuming a default score.\")\n",
        "                detection_score = 0.5 # Default score if parsing fails\n",
        "\n",
        "            # Simulate some processing time (optional, but good for realism)\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            is_ai_generated = detection_score > self.ai_detection_threshold\n",
        "            print(f\"AI detection score from LLM: {detection_score:.2f}. Is likely AI: {is_ai_generated}\")\n",
        "            # Return the AI detection status, score, and potentially the full LLM response\n",
        "            return is_ai_generated, detection_score, response_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during OpenAI LLM AI detection: {e}\")\n",
        "            # Fallback in case of API errors\n",
        "            return False, 0.0, f\"Error: {e}\"\n",
        "\n",
        "\n",
        "    def check_ethical_usage(self, prompt, generated_text):\n",
        "        \"\"\"Basic check for ethical usage, combining prompt analysis and AI detection.\"\"\"\n",
        "        print(\"Checking for ethical usage...\")\n",
        "\n",
        "        # Basic Human Prompt Checker logic (simplified)\n",
        "        prompt_lower = prompt.lower()\n",
        "        if \"write my entire thesis\" in prompt_lower or \"do my whole thesis\" in prompt_lower:\n",
        "            print(\"Ethical Alert: Skeptical usage detected (attempting to write entire thesis). Encourage ethical use and own writing.\")\n",
        "        elif \"generate abstract\" in prompt_lower or \"write introduction\" in prompt_lower:\n",
        "             print(\"Ethical Note: AI used for structural writing. Remember to review and rephrase carefully.\")\n",
        "        elif \"analyze this concept\" in prompt_lower or \"explain this\" in prompt_lower:\n",
        "             print(\"Ethical Usage: AI used for understanding/analysis. Good practice!\")\n",
        "        else:\n",
        "            print(\"Prompt intent: Could be ethical, further analysis needed in a complex model.\")\n",
        "\n",
        "\n",
        "        # Basic Ethical Violation Alert logic (simplified, tied to AI detection and prompt analysis)\n",
        "        is_ai, score, llm_response = self.detect_ai(generated_text)\n",
        "\n",
        "        if is_ai and (\"write my entire thesis\" in prompt_lower or \"do my whole thesis\" in prompt_lower):\n",
        "            print(\"Ethical VIOLATION Alert: High potential for academic dishonesty due to prompt and AI content.\")\n",
        "        elif is_ai:\n",
        "            print(\"Ethical Alert: Potential AI-generated content detected. Encourage rephrasing.\")\n",
        "\n",
        "        # Example of checking for over-reliance (very basic) - in a real model, this would look at usage patterns over time\n",
        "        # This basic check uses the length of usage logs and checks recent prompts for \"generate\"\n",
        "        if len(self.usage_logs) > 5 and all(\"generate\" in entry['prompt'].lower() for entry in self.usage_logs[-5:]):\n",
        "             print(\"Ethical Alert: Potential over-reliance on AI generation detected. Encourage critical thinking and original writing.\")\n",
        "\n",
        "\n",
        "# Example Usage (after creating and initializing the openai client):\n",
        "# Make sure the 'client' object is defined from a previous cell\n",
        "# ethics_module = EthicsModule(openai_client=client)\n",
        "# ethics_module.log_usage(\"help me understand this concept\", \"research\", thesis_stage=\"literature review\")\n",
        "# ethics_module.generate_all_usage_embeddings() # Generate embeddings after logging\n",
        "# similar_logs = ethics_module.find_similar_usage(\"find papers on NLP\")\n",
        "# print(\"\\nSimilar Usage Logs:\")\n",
        "# for log in similar_logs:\n",
        "#      print(f\"  - Distance: {log['distance']:.4f}, Prompt: '{log['original_log']['prompt']}'\")\n",
        "# is_ai, score, llm_response = ethics_module.detect_ai(\"The quick brown fox jumps over the lazy dog.\")\n",
        "# print(f\"Detect AI Result: Is AI: {is_ai}, Score: {score:.2f}, LLM Response: {llm_response}\")\n",
        "# is_ai, score, llm_response = ethics_module.detect_ai(\"As an AI language model, I can help with that.\")\n",
        "# print(f\"Detect AI Result: Is AI: {is_ai}, Score: {score:.2f}, LLM Response: {llm_response}\")\n",
        "# ethics_module.check_ethical_usage(\"write my entire thesis\", \"Here is a thesis.\")\n",
        "# ethics_module.check_ethical_usage(\"analyze this concept\", \"Based on my training data, this concept is...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "372fdee9",
        "outputId": "503c3974-739e-4af1-ed0b-143d2f8ef970"
      },
      "source": [
        "\n",
        "# Example Usage (after creating and initializing the openai client):\n",
        "\n",
        "if 'client' in locals():\n",
        "    # Initialize the ethics module with the created client\n",
        "    # Make sure the EthicsModule class is defined in a previous cell\n",
        "    ethics_module = EthicsModule(openai_client=client)\n",
        "\n",
        "    print(\"--- Testing log_usage ---\")\n",
        "    ethics_module.log_usage(\"Help me find papers on natural language processing\", \"research\", thesis_stage=\"literature review\")\n",
        "    ethics_module.log_usage(\"Generate an outline for my introduction\", \"writing_support\", thesis_stage=\"introduction\")\n",
        "    print(\"\\nCurrent usage logs:\")\n",
        "    display(pd.DataFrame(ethics_module.usage_logs))\n",
        "\n",
        "    print(\"\\n--- Testing detect_ai ---\")\n",
        "    # Test with human-like text (shorter)\n",
        "    is_ai_human, score_human, llm_response_human = ethics_module.detect_ai(\"The quick brown fox jumps over the lazy dog. This is a short sentence.\")\n",
        "    print(f\"Test 1 Result: Is AI: {is_ai_human}, Score: {score_human:.2f}, LLM Response: {llm_response_human}\")\n",
        "\n",
        "    # Test with text likely generated by an AI (shorter)\n",
        "    is_ai_ai, score_ai, llm_response_ai = ethics_module.detect_ai(\"As an AI language model, I can assist you.\")\n",
        "    print(f\"Test 2 Result: Is AI: {is_ai_ai}, Score: {score_ai:.2f}, LLM Response: {llm_response_ai}\")\n",
        "\n",
        "    # Test with some placeholder generated text (shorter)\n",
        "    is_ai_generated, score_generated, llm_response_generated = ethics_module.detect_ai(\"Generated text about a topic.\")\n",
        "    print(f\"Test 3 Result: Is AI: {is_ai_generated}, Score: {score_generated:.2f}, LLM Response: {llm_response_generated}\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Testing check_ethical_usage ---\")\n",
        "    # Test with an ethical prompt and seemingly human text (shorter)\n",
        "    ethics_module.check_ethical_usage(\"analyze this concept\", \"Based on my understanding, this concept is complex.\")\n",
        "\n",
        "    # Test with a skeptical prompt and seemingly AI text (shorter)\n",
        "    ethics_module.check_ethical_usage(\"write my entire thesis\", \"Here is a short thesis summary.\")\n",
        "\n",
        "    # Test with an ethical prompt and text likely flagged as AI (shorter)\n",
        "    ethics_module.check_ethical_usage(\"explain this theory\", \"Based on my training data, this theory is interesting.\")\n",
        "\n",
        "    # Test simple over-reliance check (might require more log entries to trigger)\n",
        "    print(\"\\n--- Testing potential over-reliance check (might need more logs) ---\")\n",
        "    # Add more \"generate\" prompts to usage logs to potentially trigger over-reliance alert (shorter prompts)\n",
        "    for _ in range(5):\n",
        "        ethics_module.log_usage(\"generate text\", \"writing_support\")\n",
        "    ethics_module.check_ethical_usage(\"continue writing\", \"More text.\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: 'client' object not found. Please run the cell to set up the OpenAI client first.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing log_usage ---\n",
            "Usage logged: Timestamp=2025-06-19 15:58:34.555343, Prompt='Help me find papers on natural language processing', Intent='research', Thesis Stage='literature review'\n",
            "Usage logged: Timestamp=2025-06-19 15:58:34.555446, Prompt='Generate an outline for my introduction', Intent='writing_support', Thesis Stage='introduction'\n",
            "\n",
            "Current usage logs:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                   timestamp  \\\n",
              "0 2025-06-19 15:58:34.555343   \n",
              "1 2025-06-19 15:58:34.555446   \n",
              "\n",
              "                                              prompt           intent  \\\n",
              "0  Help me find papers on natural language proces...         research   \n",
              "1            Generate an outline for my introduction  writing_support   \n",
              "\n",
              "        thesis_stage  \n",
              "0  literature review  \n",
              "1       introduction  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57228404-1a26-496e-a0f9-2213fd59a450\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>prompt</th>\n",
              "      <th>intent</th>\n",
              "      <th>thesis_stage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-06-19 15:58:34.555343</td>\n",
              "      <td>Help me find papers on natural language proces...</td>\n",
              "      <td>research</td>\n",
              "      <td>literature review</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-06-19 15:58:34.555446</td>\n",
              "      <td>Generate an outline for my introduction</td>\n",
              "      <td>writing_support</td>\n",
              "      <td>introduction</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57228404-1a26-496e-a0f9-2213fd59a450')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-57228404-1a26-496e-a0f9-2213fd59a450 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-57228404-1a26-496e-a0f9-2213fd59a450');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-873314c5-ee40-41fe-a606-643747d7275f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-873314c5-ee40-41fe-a606-643747d7275f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-873314c5-ee40-41fe-a606-643747d7275f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"ethics_module\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-06-19 15:58:34.555343\",\n        \"max\": \"2025-06-19 15:58:34.555446\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2025-06-19 15:58:34.555446\",\n          \"2025-06-19 15:58:34.555343\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Generate an outline for my introduction\",\n          \"Help me find papers on natural language processing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"intent\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"writing_support\",\n          \"research\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thesis_stage\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"introduction\",\n          \"literature review\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing detect_ai ---\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.1  \n",
            "The text is a well-known pangram and consists of simple, straightforward sentences that are common in human writing. The lack of complexity and the use of a familiar expression suggest a low likelihood of AI generation.\n",
            "AI detection score from LLM: 0.10. Is likely AI: False\n",
            "Test 1 Result: Is AI: False, Score: 0.10, LLM Response: 0.1  \n",
            "The text is a well-known pangram and consists of simple, straightforward sentences that are common in human writing. The lack of complexity and the use of a familiar expression suggest a low likelihood of AI generation.\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "This text is quite generic and could be easily produced by both an AI and a human, making it less likely to be definitively AI-generated.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "Test 2 Result: Is AI: False, Score: 0.20, LLM Response: 0.2  \n",
            "This text is quite generic and could be easily produced by both an AI and a human, making it less likely to be definitively AI-generated.\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "The text is vague and doesn't exhibit typical markers of AI generation, such as overly complex structures or lack of coherence. It seems more like a placeholder than a fully developed piece.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "Test 3 Result: Is AI: False, Score: 0.20, LLM Response: 0.2  \n",
            "The text is vague and doesn't exhibit typical markers of AI generation, such as overly complex structures or lack of coherence. It seems more like a placeholder than a fully developed piece.\n",
            "\n",
            "--- Testing check_ethical_usage ---\n",
            "Checking for ethical usage...\n",
            "Ethical Usage: AI used for understanding/analysis. Good practice!\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "The text is simple and lacks the hallmark characteristics of AI-generated content, such as elaborate phrasing or unusual sentence structure, making it more likely to be human-written.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "Checking for ethical usage...\n",
            "Ethical Alert: Skeptical usage detected (attempting to write entire thesis). Encourage ethical use and own writing.\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "The text is very brief and lacks complexity, making it difficult to determine, but it does not exhibit typical AI-produced verbosity or elaborate structure often found in AI-generated content.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "Checking for ethical usage...\n",
            "Ethical Usage: AI used for understanding/analysis. Good practice!\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.3  \n",
            "The text is simple and lacks complexity, but it could plausibly be written by a human reflecting on a theory. The phrase \"based on my training data\" is suggestive of AI, yet it’s not strong enough to definit\n",
            "AI detection score from LLM: 0.30. Is likely AI: False\n",
            "\n",
            "--- Testing potential over-reliance check (might need more logs) ---\n",
            "Usage logged: Timestamp=2025-06-19 15:58:47.393860, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Usage logged: Timestamp=2025-06-19 15:58:47.393927, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Usage logged: Timestamp=2025-06-19 15:58:47.393943, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Usage logged: Timestamp=2025-06-19 15:58:47.393956, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Usage logged: Timestamp=2025-06-19 15:58:47.393967, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Checking for ethical usage...\n",
            "Prompt intent: Could be ethical, further analysis needed in a complex model.\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.1\n",
            "\n",
            "The text is too brief and lacks complexity or context, making it more likely to be human-generated, possibly as a placeholder or casual notation.\n",
            "AI detection score from LLM: 0.10. Is likely AI: False\n",
            "Ethical Alert: Potential over-reliance on AI generation detected. Encourage critical thinking and original writing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit gymnasium stable-baselines3\n",
        "import json\n",
        "import os\n",
        "import gymnasium as gym\n",
        "import streamlit as st\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "# ===========================================================\n",
        "# PART 1 — CONFIGURATION MANAGER\n",
        "# ===========================================================\n",
        "\n",
        "class RLConfigManager:\n",
        "    \"\"\"\n",
        "    Manage centralized configuration for dynamic RL system.\n",
        "\n",
        "    Configuration Structure:\n",
        "    - state_variables: list[str]\n",
        "    - actions: list[str]\n",
        "    - reward_config: dict[str, float]\n",
        "    - action_effects: dict[str, dict[str, float]]\n",
        "    \"\"\"\n",
        "\n",
        "    CONFIG_FILE = \"rl_config.json\"\n",
        "\n",
        "    @classmethod\n",
        "    def load_config(cls):\n",
        "        \"\"\"\n",
        "        Load RL configuration from file, or create default if missing.\n",
        "\n",
        "        Returns:\n",
        "            config (dict): Loaded configuration dictionary.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(cls.CONFIG_FILE):\n",
        "            default_config = {\n",
        "                \"state_variables\": [\"embedding_drift\", \"ai_usage\", \"ethical_flags\", \"advisor_feedback\", \"timestep\"],\n",
        "                \"actions\": [\"Allow prompt\", \"Suggest reflection\", \"Ethical warning\", \"Suggest rewriting\", \"Advisor feedback reminder\", \"Disable AI feature\"],\n",
        "                \"reward_config\": {\"user_revised\": 2, \"ai_violation\": -3, \"advisor_positive\": 3, \"rewrite_accepted\": 1, \"milestone_completed\": 5, \"hallucination_detected\": -2},\n",
        "                \"action_effects\": {\"1\": {\"ai_usage\": -0.05, \"embedding_drift\": -0.05}, \"3\": {\"ethical_flags\": -0.1}, \"4\": {\"advisor_feedback\": 0.1}}\n",
        "            }\n",
        "            cls.save_config(default_config)\n",
        "        with open(cls.CONFIG_FILE, \"r\") as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    @classmethod\n",
        "    def save_config(cls, config):\n",
        "        \"\"\"\n",
        "        Save configuration back to disk.\n",
        "\n",
        "        Args:\n",
        "            config (dict): Configuration dictionary.\n",
        "        \"\"\"\n",
        "        with open(cls.CONFIG_FILE, \"w\") as f:\n",
        "            json.dump(config, f, indent=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY669UXT85Hs",
        "outputId": "cf1ccb77-c27b-4d9e-d24d-df749823648e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.35.0)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=16.8 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.8)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: watchdog>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.6.0+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.43.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.3.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.3.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.6.15)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.17.0)\n",
            "Downloading stable_baselines3-2.6.0-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable-baselines3-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: DEVELOPER DASHBOARD (Class-Based Streamlit Interface with Full Docstrings)\n",
        "# ===========================================================\n",
        "\n",
        "class DeveloperDashboard:\n",
        "    \"\"\"\n",
        "    Streamlit-based developer interface for interactively updating RL configuration.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.config = RLConfigManager.load_config()\n",
        "\n",
        "    def launch(self):\n",
        "        st.title(\"🎯 Thesis RL Developer Dashboard\")\n",
        "        self.edit_action_space()\n",
        "        self.edit_reward_shaping()\n",
        "        self.edit_action_effects()\n",
        "        self.save_button()\n",
        "\n",
        "    def edit_action_space(self):\n",
        "        \"\"\"\n",
        "        Display and edit the RL action space.\n",
        "        Developers can add new high-level intervention actions here.\n",
        "        \"\"\"\n",
        "        st.header(\"1️⃣ Manage Action Space\")\n",
        "        st.write(\"Define high-level interventions available to RL agent:\")\n",
        "\n",
        "        st.subheader(\"Current Actions:\")\n",
        "        for idx, action in enumerate(self.config[\"actions\"]):\n",
        "            st.write(f\"**{idx}:** {action}\")\n",
        "\n",
        "        new_action = st.text_input(\"Add New Action:\")\n",
        "        if st.button(\"Add Action\"):\n",
        "            if new_action.strip():\n",
        "                self.config[\"actions\"].append(new_action.strip())\n",
        "                st.success(f\"✅ Added action: '{new_action}'\")\n",
        "\n",
        "    def edit_reward_shaping(self):\n",
        "        st.header(\"2️⃣ Edit Reward Shaping\")\n",
        "        st.write(\"Adjust reward values for key supervision signals:\")\n",
        "\n",
        "        for key, val in self.config[\"reward_config\"].items():\n",
        "            new_val = st.slider(f\"Reward for {key}:\", -10, 10, val)\n",
        "            self.config[\"reward_config\"][key] = new_val\n",
        "\n",
        "    def edit_action_effects(self):\n",
        "        st.header(\"3️⃣ Define Action Effects\")\n",
        "        st.write(\"Specify which state variables are influenced by actions:\")\n",
        "\n",
        "        action_idx = st.text_input(\"Action Index (integer):\", value=\"1\")\n",
        "        variable_name = st.text_input(\"State Variable (e.g. ai_usage):\")\n",
        "        delta = st.number_input(\"Delta Change (+/-):\", step=0.01, value=0.0)\n",
        "\n",
        "        if st.button(\"Add Effect\"):\n",
        "            effects = self.config.setdefault(\"action_effects\", {})\n",
        "            action_effect = effects.setdefault(str(action_idx), {})\n",
        "            action_effect[variable_name] = delta\n",
        "            st.success(f\"✅ Effect added: Action {action_idx} → {variable_name} += {delta}\")\n",
        "\n",
        "    def save_button(self):\n",
        "        \"\"\"\n",
        "        Provide save button to persist updated configuration back to disk.\n",
        "        \"\"\"\n",
        "        if st.button(\"Save Full Configuration\"):\n",
        "            RLConfigManager.save_config(self.config)\n",
        "            st.success(\"✅ All changes saved successfully!\")"
      ],
      "metadata": {
        "id": "_m1U4vfL9Flz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 3 — DATA PREPROCESSOR (LOG TO STATE CONVERSION)\n",
        "# ===========================================================\n",
        "\n",
        "class DataPreprocessor:\n",
        "    \"\"\"\n",
        "    Convert raw usage logs into RL state vectors and reward labels.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initialize with current config structure.\n",
        "\n",
        "        Args:\n",
        "            config (dict): Loaded RL configuration.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "\n",
        "    def extract_state(self, log_entry):\n",
        "        \"\"\"\n",
        "        Convert one log entry into RL state vector.\n",
        "\n",
        "        Args:\n",
        "            log_entry (dict): Single usage log.\n",
        "\n",
        "        Returns:\n",
        "            state (np.ndarray): RL normalized state vector.\n",
        "        \"\"\"\n",
        "        state = []\n",
        "        for var in self.config[\"state_variables\"]:\n",
        "            if var == \"timestep\":\n",
        "                state.append(log_entry.get(\"deadline_ratio\", 0.0))\n",
        "            else:\n",
        "                state.append(log_entry.get(var, 0.0))\n",
        "        return np.array(state)\n",
        "\n",
        "    def compute_reward(self, log_entry):\n",
        "        \"\"\"\n",
        "        Compute shaped reward for given log event.\n",
        "\n",
        "        Returns:\n",
        "            reward (float): computed reward value.\n",
        "        \"\"\"\n",
        "        reward = 0.0\n",
        "        for key, value in self.config[\"reward_config\"].items():\n",
        "            if log_entry.get(key, False):\n",
        "                reward += value\n",
        "        return reward"
      ],
      "metadata": {
        "id": "-6xa8116-cmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# PART 4: RL ENVIRONMENT (Fully Dynamic Gym-Compatible Environment)\n",
        "# ===========================================================\n",
        "#\n",
        "# This module defines the RL environment the PPO agent interacts with.\n",
        "#\n",
        "# Fully config-driven:\n",
        "# - The state space (variables used)\n",
        "# - The action space (interventions)\n",
        "# - The reward function\n",
        "# - The state update effects per action\n",
        "#\n",
        "# -----------------------------------------------------------\n",
        "# ✅ WHY FULLY DYNAMIC STATE?\n",
        "# -----------------------------------------------------------\n",
        "# - Allows easy expansion of system complexity.\n",
        "# - Developers can add new state features via config without touching any code.\n",
        "# - Keeps RL model compatible with evolving assistant behavior.\n",
        "#\n",
        "# -----------------------------------------------------------\n",
        "# ✅ KEY CONCEPTS (NOW FULLY DYNAMIC):\n",
        "# -----------------------------------------------------------\n",
        "# - State Variables: loaded from `state_variables` in config\n",
        "# - Action Effects: loaded from `action_effects` in config\n",
        "# - Observation Space: dynamically computed based on config\n",
        "# ===========================================================\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "class EthicsSupervisorEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Fully dynamic Gym-compatible RL environment for Thesis Assistant Ethics Supervisor.\n",
        "\n",
        "    Attributes:\n",
        "        ethics_module (MockEthicsModule): simulated system state\n",
        "        config (dict): loaded RL configuration\n",
        "        state_variables (list): list of state variable names (from config)\n",
        "        actions (list): list of available action labels (from config)\n",
        "        action_effects (dict): state update deltas per action (from config)\n",
        "        observation_space (gym.Space): dynamic continuous state space\n",
        "        action_space (gym.Space): discrete action space\n",
        "        timestep (int): current simulation step count\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ethics_module, config):\n",
        "        \"\"\"\n",
        "        Initialize the environment.\n",
        "\n",
        "        Args:\n",
        "            ethics_module (MockEthicsModule): external system state simulator\n",
        "            config (dict): full RL configuration\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.ethics_module = ethics_module\n",
        "        self.config = config\n",
        "\n",
        "        self.state_variables = config[\"state_variables\"]\n",
        "        self.actions = config[\"actions\"]\n",
        "        self.action_effects = config.get(\"action_effects\", {})\n",
        "\n",
        "        # Fully dynamic observation space size:\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=1, shape=(len(self.state_variables),), dtype=np.float32\n",
        "        )\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(len(self.actions))\n",
        "        self.timestep = 0\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        \"\"\"\n",
        "        Reset environment at start of new episode.\n",
        "        \"\"\"\n",
        "        self.timestep = 0\n",
        "        return self._get_state(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute one interaction step.\n",
        "\n",
        "        Args:\n",
        "            action (int): action index from RL agent\n",
        "\n",
        "        Returns:\n",
        "            state (np.array): next normalized state\n",
        "            reward (float): reward received\n",
        "            done (bool): whether episode terminates\n",
        "            truncated (bool): always False (not used)\n",
        "            info (dict): additional info (empty)\n",
        "        \"\"\"\n",
        "        reward = self._compute_reward(action)\n",
        "        self._apply_action_effects(action)\n",
        "        self.timestep += 1\n",
        "        done = (self.timestep >= 100)\n",
        "        return self._get_state(), reward, done, False, {}\n",
        "\n",
        "    def _get_state(self):\n",
        "        \"\"\"\n",
        "        Construct normalized state vector fully dynamically.\n",
        "\n",
        "        Returns:\n",
        "            np.array: normalized state vector based on config-defined variables\n",
        "        \"\"\"\n",
        "        state = []\n",
        "        for var in self.state_variables:\n",
        "            if var == \"timestep\":\n",
        "                state.append(self.timestep / 100.0)\n",
        "            else:\n",
        "                value = getattr(self.ethics_module, var, 0.0)\n",
        "                state.append(value)\n",
        "        return np.array(state)\n",
        "\n",
        "    def _compute_reward(self, action):\n",
        "        \"\"\"\n",
        "        Compute reward for chosen action.\n",
        "        \"\"\"\n",
        "        base_reward = 1.0\n",
        "        time_scaling = 1 + 2.0 * min(1.0, self.timestep / 80.0)\n",
        "        api_cost = 0.002 * action\n",
        "        lambda_cost = 10\n",
        "        return base_reward * time_scaling - lambda_cost * api_cost\n",
        "\n",
        "    def _apply_action_effects(self, action):\n",
        "        \"\"\"\n",
        "        Dynamically apply effects to system state based on action.\n",
        "\n",
        "        Args:\n",
        "            action (int): selected action index\n",
        "        \"\"\"\n",
        "        effects = self.action_effects.get(str(action), {})\n",
        "        for variable, delta in effects.items():\n",
        "            current_value = getattr(self.ethics_module, variable, None)\n",
        "            if current_value is not None:\n",
        "                updated_value = np.clip(current_value + delta, 0.0, 1.0)\n",
        "                setattr(self.ethics_module, variable, updated_value)"
      ],
      "metadata": {
        "id": "wYBfOBpu9a5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "\n",
        "===========================================================\n",
        "\n",
        "... (entire previous system unchanged above) ...\n",
        "\n",
        "===========================================================\n",
        "\n",
        "APPENDIX — PPO SUITABILITY ANALYSIS FOR THESIS ASSISTANT\n",
        "\n",
        "===========================================================\n",
        "\n",
        "Analysis: Strengths and Limitations of PPO for Thesis Assistant RL System\n",
        "\n",
        "✅ PROS (Why PPO is suitable globally):\n",
        "\n",
        "Stable policy optimization even in high-dimensional state spaces.\n",
        "\n",
        "Supports multiple complex actions (advisory interventions, ethical warnings, etc).\n",
        "\n",
        "Optimizes long-term reward (handles delayed ethical consequences).\n",
        "\n",
        "Clipping mechanism stabilizes policy updates (critical for safe ethical behavior).\n",
        "\n",
        "Can be trained globally across many users for a general ethical baseline.\n",
        "\n",
        "\n",
        "⚠ CONS (Limitations for personalization scenario):\n",
        "\n",
        "Requires many training samples to fully converge (sample inefficient).\n",
        "\n",
        "Slow adaptation when applied directly to new individual students.\n",
        "\n",
        "May not personalize fast enough during limited thesis timeframe (6-12 months).\n",
        "\n",
        "Potential difficulty adapting to individual personality shifts quickly.\n",
        "\n",
        "PPO only indirectly receives feedback via reward — few-shot adaptation is hard.\n",
        "\n",
        "\n",
        "✅ RECOMMENDED STRATEGY:\n",
        "\n",
        "Use PPO for global pretraining across many students (shared ethical policy).\n",
        "\n",
        "Introduce a lightweight per-student adaptation layer (small fine-tuning component).\n",
        "\n",
        "Combine PPO with human-in-the-loop reward shaping for rapid personalization.\n",
        "\n",
        "Consider hybrid architecture with PPO + bandits or meta-RL elements for few-shot adjustments.\n",
        "\n",
        "\n",
        "This hybrid design balances PPO’s global stability with efficient short-term personalization needs of the thesis assistant. \"\"\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YUIZPAy9ju__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# PART 5: PPO SUPERVISOR (Reinforcement Learning Agent Controller)\n",
        "# ===========================================================\n",
        "#\n",
        "# This module manages the PPO RL agent training, saving, loading, and inference.\n",
        "#\n",
        "# - Clean separation of agent control logic from environment definition.\n",
        "# - Compatible with stable-baselines3 PPO implementation.\n",
        "# - Supports continual training and model persistence.\n",
        "#\n",
        "# -----------------------------------------------------------\n",
        "# ✅ KEY CONCEPTS:\n",
        "# -----------------------------------------------------------\n",
        "# - PPO (Proximal Policy Optimization): modern stable RL algorithm\n",
        "# - Continual training: keep refining policy incrementally\n",
        "# - Safe reloading: easily resume training from saved checkpoints\n",
        "# ===========================================================\n",
        "\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "import os\n",
        "import numpy as np # Import numpy as it's used in MockEthicsModule\n",
        "\n",
        "# MockEthicsModule is not defined in the provided cells, so I'll assume it's a placeholder\n",
        "# and create a simple mock class to avoid a NameError.\n",
        "class MockEthicsModule:\n",
        "    def __init__(self):\n",
        "        # Initialize attributes that the environment might try to access\n",
        "        self.embedding_drift = np.random.rand()\n",
        "        self.ai_usage = np.random.rand()\n",
        "        self.ethical_flags = np.random.rand()\n",
        "        self.advisor_feedback = np.random.rand()\n",
        "        # Add any other attributes that are expected by the environment\n",
        "        self.deadline_ratio = np.random.rand() # Added based on DataPreprocessor\n",
        "        # Add attributes required by DataPreprocessor.extract_state\n",
        "        self.user_revised = np.random.choice([True, False])\n",
        "        self.ai_violation = np.random.choice([True, False])\n",
        "        self.advisor_positive = np.random.choice([True, False])\n",
        "        self.rewrite_accepted = np.random.choice([True, False])\n",
        "        self.milestone_completed = np.random.choice([True, False])\n",
        "        self.hallucination_detected = np.random.choice([True, False])\n",
        "        self.prompt = \"mock prompt\"\n",
        "        self.intent = \"mock intent\"\n",
        "        self.thesis_stage = \"mock stage\"\n",
        "\n",
        "\n",
        "class EthicsSupervisorRL:\n",
        "    \"\"\"\n",
        "    PPO Supervisor class controlling RL training and inference.\n",
        "\n",
        "    Attributes:\n",
        "        ethics_module (MockEthicsModule): simulated system state.\n",
        "        env (EthicsSupervisorEnv): RL environment.\n",
        "        model (PPO): PPO policy model.\n",
        "        model_path (str): path to save/load PPO model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, model_path=\"ethics_rl_model\"):\n",
        "        \"\"\"\n",
        "        Initialize PPO agent.\n",
        "\n",
        "        Args:\n",
        "            config (dict): RL configuration dictionary.\n",
        "            model_path (str): storage path for saving/loading model.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        self.ethics_module = MockEthicsModule()\n",
        "        self.env = EthicsSupervisorEnv(self.ethics_module, config)\n",
        "        self.model_path = model_path\n",
        "\n",
        "        if os.path.exists(model_path + \".zip\"):\n",
        "            self.model = PPO.load(model_path, env=self.env)\n",
        "            print(\"Loaded pretrained RL model.\")\n",
        "        else:\n",
        "            self.model = PPO(\"MlpPolicy\", self.env, verbose=0)\n",
        "            print(\"Initialized new PPO model.\")\n",
        "\n",
        "    def train(self, timesteps=50000):\n",
        "        \"\"\"\n",
        "        Train PPO model for specified timesteps.\n",
        "\n",
        "        Args:\n",
        "            timesteps (int): number of environment steps.\n",
        "        \"\"\"\n",
        "        self.model.learn(total_timesteps=timesteps)\n",
        "        self.model.save(self.model_path)\n",
        "        print(\"Training complete and model saved.\")\n",
        "\n",
        "    def recommend_action(self):\n",
        "        \"\"\"\n",
        "        Predict next action based on current state.\n",
        "\n",
        "        Returns:\n",
        "            int: action index selected by PPO policy.\n",
        "        \"\"\"\n",
        "        state = self.env._get_state()\n",
        "        action, _ = self.model.predict(state, deterministic=True)\n",
        "        return action"
      ],
      "metadata": {
        "id": "hIj4QA7J92Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 6 — CONTINUAL RL TRAINING LOOP\n",
        "# ===========================================================\n",
        "\n",
        "class RLTrainingLoop:\n",
        "    \"\"\"\n",
        "    Orchestrates continual RL training process using log batches.\n",
        "\n",
        "    Attributes:\n",
        "        config (dict): loaded RL configuration.\n",
        "        preprocessor (DataPreprocessor): converts logs to state vectors.\n",
        "        trainer (EthicsSupervisorRL): handles PPO model training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, model_path=\"ppo_ethics_model\"):\n",
        "        \"\"\"\n",
        "        Initialize training loop components.\n",
        "\n",
        "        Args:\n",
        "            model_path (str): path to PPO model storage.\n",
        "        \"\"\"\n",
        "        self.config = RLConfigManager.load_config()\n",
        "        self.preprocessor = DataPreprocessor(self.config)\n",
        "        self.trainer = EthicsSupervisorRL(self.config, model_path) # Corrected class name\n",
        "\n",
        "    def run_training_day(self, log_batch):\n",
        "        \"\"\"\n",
        "        Process one batch of usage logs and train PPO agent.\n",
        "\n",
        "        Args:\n",
        "            log_batch (list of dict): usage logs for one training day.\n",
        "        \"\"\"\n",
        "        for log_entry in log_batch:\n",
        "            state_vector = self.preprocessor.extract_state(log_entry)\n",
        "            reward = self.preprocessor.compute_reward(log_entry)\n",
        "            print(f\"Processed Log → State: {state_vector}, Reward: {reward}\")\n",
        "        # The training method in EthicsSupervisorRL is named 'train', not 'train_step'\n",
        "        self.trainer.train(timesteps=5000)\n",
        "        # The prediction method in EthicsSupervisorRL is named 'recommend_action', not 'predict'\n",
        "        action = self.trainer.recommend_action()\n",
        "        print(f\"Recommended action after training: {action}\")"
      ],
      "metadata": {
        "id": "11LAFgGlB_1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PART 7 — RL TRAINING LAUNCHER (FINAL ENTRY POINT)\n",
        "# ===========================================================\n",
        "\n",
        "class RLTrainingLauncher:\n",
        "    \"\"\"\n",
        "    Full master system launcher for thesis RL assistant.\n",
        "\n",
        "    Supports:\n",
        "    - Developer dashboard configuration\n",
        "    - Real data training\n",
        "    - Synthetic dev training\n",
        "    - Online incremental updates\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.config = RLConfigManager.load_config()\n",
        "        self.preprocessor = DataPreprocessor(self.config)\n",
        "        # Pass the config to RLTrainingLoop initialization\n",
        "        self.training_loop = RLTrainingLoop(self.config)\n",
        "\n",
        "    def launch_dashboard(self):\n",
        "        \"\"\"Launch Streamlit-based developer interface for configuring RL system.\"\"\"\n",
        "        dashboard = DeveloperDashboard()\n",
        "        dashboard.launch()\n",
        "\n",
        "    def run_simulated_training(self, days=3, batch_size=5):\n",
        "        \"\"\"\n",
        "        Simulate RL model training using synthetic logs.\n",
        "\n",
        "        Args:\n",
        "            days (int): Number of training days to simulate.\n",
        "            batch_size (int): Number of logs to generate per day.\n",
        "        \"\"\"\n",
        "        # The simulator is not defined, so this part will still cause an error.\n",
        "        # I will add a placeholder for a mock simulator for now.\n",
        "        class MockSimulator:\n",
        "            def generate_batch(self, batch_size):\n",
        "                print(\"Generating mock log batch...\")\n",
        "                # Return a list of dictionaries with placeholder data\n",
        "                # This data should be structured to match what DataPreprocessor expects\n",
        "                mock_logs = []\n",
        "                for _ in range(batch_size):\n",
        "                    mock_logs.append({\n",
        "                        \"embedding_drift\": np.random.rand(),\n",
        "                        \"ai_usage\": np.random.rand(),\n",
        "                        \"ethical_flags\": np.random.rand(),\n",
        "                        \"advisor_feedback\": np.random.rand(),\n",
        "                        \"deadline_ratio\": np.random.rand(),\n",
        "                        \"user_revised\": np.random.choice([True, False]),\n",
        "                        \"ai_violation\": np.random.choice([True, False]),\n",
        "                        \"advisor_positive\": np.random.choice([True, False]),\n",
        "                        \"rewrite_accepted\": np.random.choice([True, False]),\n",
        "                        \"milestone_completed\": np.random.choice([True, False]),\n",
        "                        \"hallucination_detected\": np.random.choice([True, False]),\n",
        "                        \"prompt\": \"mock prompt\", # Added prompt for preprocessor\n",
        "                        \"intent\": \"mock intent\", # Added intent for preprocessor\n",
        "                        \"thesis_stage\": \"mock stage\" # Added thesis_stage for preprocessor\n",
        "                    })\n",
        "                return mock_logs\n",
        "\n",
        "        self.simulator = MockSimulator() # Initialize the mock simulator\n",
        "\n",
        "        for day in range(days):\n",
        "            print(f\"\\nSimulated Day {day + 1}\")\n",
        "            logs = self.simulator.generate_batch(batch_size=batch_size)\n",
        "            self.training_loop.run_training_day(logs)\n",
        "\n",
        "    def run_real_training(self, real_logs):\n",
        "        \"\"\"\n",
        "        Train RL model using real usage logs.\n",
        "\n",
        "        Args:\n",
        "            real_logs (list of dict): Collected usage logs to use in training.\n",
        "        \"\"\"\n",
        "        print(\"\\nTraining with Real Logs...\")\n",
        "        self.training_loop.run_training_day(real_logs)\n",
        "\n",
        "    def run_online_incremental_training(self, incremental_logs):\n",
        "        \"\"\"\n",
        "        Run online incremental updates using newly gathered data.\n",
        "\n",
        "        Args:\n",
        "            incremental_logs (list of dict): New usage logs collected for fine-tuning.\n",
        "        \"\"\"\n",
        "        print(\"\\nIncremental Online Training...\")\n",
        "        self.training_loop.run_training_day(incremental_logs)\n",
        "\n",
        "# =========================== MAIN EXECUTION ===========================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    launcher = RLTrainingLauncher()\n",
        "    print(\"RL Training System Entry Point\")\n",
        "    print(\"Modes: [dashboard] [train_simulated] [train_real] [train_online]\")\n",
        "    mode = input(\"Mode: \").strip()\n",
        "\n",
        "    if mode == \"dashboard\":\n",
        "        launcher.launch_dashboard()\n",
        "    elif mode == \"train_simulated\": # Added the missing mode\n",
        "        launcher.run_simulated_training()\n",
        "    elif mode == \"train_real\":\n",
        "        print(\"Load your real usage logs into 'real_logs' and call launcher.run_real_training(real_logs)\")\n",
        "    elif mode == \"train_online\":\n",
        "        print(\"Load new incremental logs into 'incremental_logs' and call launcher.run_online_incremental_training(incremental_logs)\")\n",
        "    else:\n",
        "        print(\"Invalid mode selected.\")"
      ],
      "metadata": {
        "id": "xHLVBuFyHeVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81c7810e-bf5c-40b4-c69f-759c92d49a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized new PPO model.\n",
            "RL Training System Entry Point\n",
            "Modes: [dashboard] [train_simulated] [train_real] [train_online]\n",
            "Mode: train_real\n",
            "Load your real usage logs into 'real_logs' and call launcher.run_real_training(real_logs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f37b37ac"
      },
      "source": [
        "# @title Sample Code: RLHF Concept for EthicsSupervisor (AI Generated Example)\n",
        "\n",
        "import numpy as np # Assuming numpy is installed\n",
        "\n",
        "# This is a conceptual outline and placeholder for the RLHF implementation.\n",
        "# A full RL implementation would require a more sophisticated library (e.g., Stable Baselines3, Ray RLLib)\n",
        "# and careful design of the environment, state space, action space, and reward function.\n",
        "\n",
        "class EthicsRLAgent:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        # Placeholder for the RL model (e.g., a simple Q-table or a neural network)\n",
        "        self.model = self._initialize_model()\n",
        "\n",
        "    def _initialize_model(self):\n",
        "        # In a real implementation, initialize your RL model here.\n",
        "        # For a simple example, a Q-table:\n",
        "        # return np.zeros((self.state_dim, self.action_dim))\n",
        "        pass # Placeholder\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        # In a real implementation, the agent chooses an action based on the current state\n",
        "        # and its learned policy (e.g., epsilon-greedy exploration).\n",
        "        print(\"EthicsRLAgent choosing action (placeholder)...\")\n",
        "        # return np.random.randint(self.action_dim) # Example: random action\n",
        "        pass # Placeholder\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        # In a real implementation, the agent updates its model based on the experience.\n",
        "        print(f\"EthicsRLAgent learning: State={state}, Action={action}, Reward={reward} (placeholder)...\")\n",
        "        pass # Placeholder\n",
        "\n",
        "\n",
        "# Conceptual mapping of ethical events to state features, actions, and rewards:\n",
        "\n",
        "# --- Conceptual State Representation ---\n",
        "# The state would be a numerical representation derived from the EthicsModule's information:\n",
        "# Example State Features (needs careful design and scaling):\n",
        "# - AI detection score (from detect_ai)\n",
        "# - Likelihood of skeptical prompt (from HumanPromptChecker logic)\n",
        "# - Presence of unaddressed advisor feedback (from AdvisorFeedbackSync)\n",
        "# - Recency and frequency of LLM usage (from Usage_Logger data/embeddings)\n",
        "# - Results from embedding similarity queries (from find_similar_usage)\n",
        "# - Flags for specific ethical violations (from EthicalViolationAlert logic)\n",
        "# - User engagement with previous prompts/alerts\n",
        "\n",
        "# --- Conceptual Action Space ---\n",
        "# The actions the EthicsRLAgent can take:\n",
        "# Example Actions:\n",
        "# 0: Allow interaction to proceed without intervention\n",
        "# 1: Trigger a reflection prompt (via HumanPromptChecker)\n",
        "# 2: Issue an ethical warning/alert (via EthicalViolationAlert)\n",
        "# 3: Suggest rephrasing AI-generated content\n",
        "# 4: Recommend consulting advisor feedback (via AdvisorFeedbackSync)\n",
        "# 5: Temporarily restrict certain LLM functionalities\n",
        "\n",
        "# --- Conceptual Reward Function ---\n",
        "# Rewards signal desirable ethical outcomes:\n",
        "# Example Reward Mapping (based on user feedback and ethical guidelines):\n",
        "# - Positive reward (+): User rephrases AI content, user incorporates advisor feedback, user engages with reflection prompt, ethical prompt used, progress aligns with plan.\n",
        "# - Negative reward (-): User ignores AI flag, user uses skeptical prompt, user disregards advisor feedback, over-reliance detected, hallucination detected.\n",
        "# - Higher weight for rewards/penalties related to advisor feedback and academic integrity violations.\n",
        "\n",
        "# --- Conceptual Feedback Loop Integration (e.g., in LangGraph) ---\n",
        "# The EthicsSupervisor (as the RL Agent) observes interactions, chooses an action,\n",
        "# and receives a reward signal to learn.\n",
        "#\n",
        "# 1. User interaction/Agent action triggers state update in EthicsSupervisor.\n",
        "# 2. EthicsRLAgent observes state and chooses an action (e.g., trigger prompt, allow).\n",
        "# 3. Action is executed (e.g., prompt displayed, interaction continues).\n",
        "# 4. User response/Subsequent events provide feedback.\n",
        "# 5. Reward is calculated based on feedback and ethical outcomes.\n",
        "# 6. EthicsRLAgent uses (state, action, reward, next_state) to learn/update its policy.\n",
        "\n",
        "\n",
        "# Example of how you might initialize the agent (requires defining state_dim and action_dim):\n",
        "# state_dimension = ... # Define based on your state representation\n",
        "# action_dimension = ... # Number of possible actions\n",
        "# ethics_rl_agent = EthicsRLAgent(state_dim=state_dimension, action_dim=action_dimension)\n",
        "\n",
        "# Example of how the agent might be used in a simplified loop:\n",
        "# current_state = ... # Get initial state from EthicsModule/submodules\n",
        "# done = False\n",
        "# while not done:\n",
        "#     action = ethics_rl_agent.choose_action(current_state)\n",
        "#     # Execute the chosen action...\n",
        "#     # Observe next_state and calculate reward...\n",
        "#     # ethics_rl_agent.learn(current_state, action, reward, next_state, done)\n",
        "#     # current_state = next_state\n",
        "#     # Check if task is done...\n",
        "#     pass # Placeholder for the loop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18bd052c"
      },
      "source": [
        "## Proposed RL Agent State Structure\n",
        "\n",
        "This outlines a potential structure for the State that the Reinforcement Learning agent (overseeing project evolution and ethics) would observe. This state combines information from the Ethics Module with broader thesis progress details.\n",
        "\n",
        "The state would likely be represented as a numerical vector or a structured object that the RL model can process.\n",
        "\n",
        "**Components of the State:**\n",
        "\n",
        "1.  **Ethical State Features (from Ethics Module):**\n",
        "    *   **AI Detection Score:** The score from the `detect_ai` function for the most recent generated content (e.g., a value between 0 and 1).\n",
        "    *   **Prompt Classification:** A categorical or numerical representation of the last prompt's ethical classification (e.g., 0 for ethical, 1 for structural, 2 for skeptical/dangerous).\n",
        "    *   **Usage Frequency:** Metrics on recent LLM usage (e.g., number of LLM interactions in the last hour/day, proportion of \"generate\" prompts).\n",
        "    *   **Embedding Similarity:** The similarity score from `find_similar_usage` when querying the current prompt against past usage logs (e.g., the distance to the most similar ethical/skeptical past interaction).\n",
        "    *   **Ethical Alert Status:** Flags indicating if any ethical violations or warnings are currently active (e.g., binary flags for over-reliance alert, academic dishonesty alert).\n",
        "    *   **Human Engagement:** Metrics on user interaction with previous ethical interventions (e.g., did the user rephrase AI content, did they engage with a reflection prompt).\n",
        "\n",
        "2.  **Thesis Progress Features:**\n",
        "    *   **Current Thesis Stage:** A categorical or numerical representation of the current stage of the thesis (e.g., 0 for planning, 1 for literature review, 2 for methodology, 3 for writing, etc.).\n",
        "    *   **Task Completion:** Percentage of planned tasks completed for the current stage or overall project.\n",
        "    *   **Time-based Metrics:** Time spent on the project recently, time remaining until deadlines.\n",
        "    *   **Advisor Feedback Status:** A flag or metric indicating the presence and recency of unaddressed advisor feedback.\n",
        "\n",
        "3.  **Performance Features:**\n",
        "    *   **Work Quality Score:** A metric representing the quality of recent thesis work (this would be challenging to define and might require human evaluation or proxy metrics).\n",
        "    *   **Progress Rate:** A measure of how quickly tasks are being completed or milestones are being reached.\n",
        "\n",
        "**Combining the State:**\n",
        "\n",
        "These individual features would be combined into a single state representation that the RL agent's model can process. For a neural network-based RL model, this would typically be a flattened numerical vector. Categorical features would need to be appropriately encoded (e.g., one-hot encoding).\n",
        "\n",
        "**Next Steps for Implementation (for later):**\n",
        "\n",
        "*   Define the specific numerical or categorical representation for each state feature.\n",
        "*   Develop the logic within the thesis assistant to collect and compile this information into the state vector at each time step.\n",
        "*   Ensure the Ethics Module submodules (Usage_Logger, AI_Detector, etc.) are providing the necessary data points in a format that can be easily integrated into the state."
      ]
    }
  ]
}