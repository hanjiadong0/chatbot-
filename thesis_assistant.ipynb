{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPt7EukAqexOiuDpzBDEN+T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanjiadong0/chatbot-/blob/RL/thesis_assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s04Q8trlh9nJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8008a577"
      },
      "source": [
        "# Task\n",
        "Program the ethics module for the thesis assistant project based on the provided structure, focusing on authorship tracking, AI labelling, and human-in-the-loop prompts, orchestrated by an EthicsSupervisor Agent and utilizing the defined submodules (AI_Detector, Usage_Logger, HumanPromptChecker, AdvisorFeedbackSync, EthicalViolationAlert) and interfaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d2bc5dd"
      },
      "source": [
        "## Define the scope of the ethics module\n",
        "\n",
        "### Subtask:\n",
        "Clarify what aspects of ethics the module should address within the context of your thesis killer project, focusing on authorship tracking, AI labelling, and human-in-the-loop prompts, orchestrated by an EthicsSupervisor Agent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bdc5a79"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the ethical concerns, explain the relation of authorship tracking, AI labelling, and human-in-the-loop prompts, outline the responsibilities of the EthicsSupervisor Agent, and briefly explain the contribution of the submodules, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b709aa3a"
      },
      "source": [
        "# Task\n",
        "Program the ethics module for your thesis killer project based on the provided description, including the EthicsSupervisor Agent implemented as a Reinforcement Learning monitor, the specified submodules (AI_Detector, Usage_Logger, HumanPromptChecker, AdvisorFeedbackSync, EthicalViolationAlert), and the interfaces for connecting to external tools and logging information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ad5129f"
      },
      "source": [
        "## Define the scope of the ethics module\n",
        "\n",
        "### Subtask:\n",
        "Clarify what aspects of ethics the module should address within the context of your thesis killer project, focusing on authorship tracking, AI labelling, and human-in-the-loop prompts, orchestrated by an EthicsSupervisor Agent implemented as a Reinforcement Learning monitor.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0bd7ab6"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the ethical concerns, explain the relation of authorship tracking, AI labelling, and human-in-the-loop prompts, outline the responsibilities of the EthicsSupervisor Agent, and briefly explain the contribution of the submodules, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae65fbf7"
      },
      "source": [
        "## Identify relevant ethical guidelines or frameworks\n",
        "\n",
        "### Subtask:\n",
        "Research and select appropriate ethical principles or frameworks applicable to your project's domain, considering how they relate to the functions of the defined submodules and how these can be translated into states, actions, and reward signals for the RL-based EthicsSupervisor.\n",
        "\n",
        "**Reasoning**:\n",
        "Selecting appropriate ethical guidelines is crucial for ensuring the ethics module effectively addresses the challenges identified in the project's presentation. These guidelines will inform the design and implementation of the EthicsSupervisor and its submodules, as well as the definition of states, actions, and reward signals for the Reinforcement Learning approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1689d8f"
      },
      "source": [
        "## Design the module's structure\n",
        "\n",
        "### Subtask:\n",
        "Outline the components and functionalities of the ethics module, with the EthicsSupervisor Agent implemented as a Reinforcement Learning monitor that has access to agent decisions, user responses, LLM-generated content, timing logs, and human feedback loops. Define the roles and interactions of the submodules (AI_Detector, Usage_Logger, HumanPromptChecker, AdvisorFeedbackSync, and EthicalViolationAlert) and how they will utilize interfaces to connect with tools like GPTZero, Copyleaks, or custom DetectGPT, log timestamps, usage intent, and tool confidence, and use rules/classifiers for warnings and suggestions. Design how the information from these submodules and interfaces will be used as state, action, and reward signals for the RL model.\n",
        "\n",
        "**Reasoning**:\n",
        "A well-defined structure is essential for implementing a complex module like the ethics module. Clearly outlining the roles and interactions of the EthicsSupervisor, submodules, and interfaces, and specifically designing how information will be used for the RL model, will ensure a cohesive and functional design that addresses the identified ethical challenges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fa79dc9"
      },
      "source": [
        "## Design the module's structure\n",
        "\n",
        "### Subtask:\n",
        "Outline the components and functionalities of the ethics module, with the EthicsSupervisor Agent implemented as a Reinforcement Learning monitor that has access to agent decisions, user responses, LLM-generated content, timing logs, and human feedback loops. Define the roles and interactions of the submodules (AI_Detector, Usage_Logger, HumanPromptChecker, AdvisorFeedbackSync, and EthicalViolationAlert) and how they will utilize interfaces to connect with AI detector tools, log timestamps, usage intent, and tool confidence, and use rules/classifiers for warnings and suggestions. Design how the information from these submodules and interfaces will be used as state, action, and reward signals for the RL model.\n",
        "\n",
        "**Reasoning**:\n",
        "A well-defined structure is essential for implementing a complex module like the ethics module. Clearly outlining the roles and interactions of the EthicsSupervisor, submodules, and interfaces, and specifically designing how information will be used for the RL model, will ensure a cohesive and functional design that addresses the identified ethical challenges."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import OpenAIServerModel\n",
        "api_key = \"AIzaSyBNqQzrD75wV8WfGsV27VHUZ9j5ts5ihMg\"   # use some free api key\n",
        "model = OpenAIServerModel(\n",
        "    model_id=\"gemini-2.0-flash\", # the model I used\n",
        "    api_base=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        "    api_key=api_key,\n",
        ")"
      ],
      "metadata": {
        "id": "JsrtVPUa1qT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import the OpenAI library\n",
        "from openai import OpenAI\n",
        "# Used to securely store your API key - uncomment if using Colab Secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get your OpenAI API key securely\n",
        "# Replace \"<YOUR_OPENAI_API_KEY>\" with your key, or use Colab Secrets\n",
        "# Or if using Colab Secrets:\n",
        "openai_api_key_secure = userdata.get('OPENAI_API_KEY')\n",
        "openai_organization = userdata.get('OPENAI_ORGANIZATION')\n",
        "openai_project = userdata.get('OPENAI_PROJECT_ID')\n",
        "\n",
        "# Set your project API key\n",
        "OpenAI.api_key = openai_api_key_secure\n",
        "# You must also set organization and project ID\n",
        "OpenAI.organization = openai_organization\n",
        "OpenAI.project = openai_project\n",
        "\n",
        "# Create the OpenAI client\n",
        "client = OpenAI(api_key= OpenAI.api_key)\n",
        "\n"
      ],
      "metadata": {
        "id": "5NDgGXE0tS15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a request to the Chat Completions endpoint\n",
        "response = client.chat.completions.create(\n",
        "  # Specify the model\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    # Assign the correct role\n",
        "    {\"role\": \"user\",\n",
        "     \"content\": \"Write a polite reply accepting an AI Engineer job offer within 20 words.\"}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Brf7k9_ytsni",
        "outputId": "94f9c3c9-5fc4-4584-8e42-49472f3587d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: Acceptance of Job Offer\n",
            "\n",
            "Dear [Hiring Manager's Name],\n",
            "\n",
            "I am thrilled to accept the AI Engineer position. Thank you for this opportunity!\n",
            "\n",
            "Best regards,  \n",
            "[Your Name]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a9d1e9e"
      },
      "source": [
        "import pandas as pd\n",
        "import time # Import time for simulating API call delay\n",
        "from openai import OpenAI # Import the OpenAI library\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata # Uncomment if using Colab Secrets\n",
        "from scipy.spatial import distance # Assuming scipy is installed\n",
        "\n",
        "class EthicsModule:\n",
        "    def __init__(self, openai_client):\n",
        "        self.usage_logs = []\n",
        "        self.usage_embeddings = [] # Initialize list to store embeddings\n",
        "        self.ai_detection_threshold = 0.7 # Simple threshold for AI detection\n",
        "        # Use the provided OpenAI client\n",
        "        self.client = openai_client\n",
        "\n",
        "\n",
        "    def log_usage(self, prompt, intent, thesis_stage=\"unknown\"):\n",
        "        \"\"\"Logs the usage of the thesis assistant with more details.\"\"\"\n",
        "        log_entry = {\n",
        "            'timestamp': pd.Timestamp.now(),\n",
        "            'prompt': prompt,\n",
        "            'intent': intent,\n",
        "            'thesis_stage': thesis_stage # Added thesis stage\n",
        "        }\n",
        "        self.usage_logs.append(log_entry)\n",
        "        print(f\"Usage logged: Timestamp={log_entry['timestamp']}, Prompt='{prompt}', Intent='{intent}', Thesis Stage='{thesis_stage}'\")\n",
        "        # Optionally generate embedding for the new log entry immediately\n",
        "        # self._generate_embedding_for_log(log_entry)\n",
        "\n",
        "\n",
        "    def _generate_embedding_for_log(self, log_entry):\n",
        "         \"\"\"Generates embedding for a single log entry and stores it.\"\"\"\n",
        "         try:\n",
        "             prompt_text = log_entry['prompt']\n",
        "             response = self.client.embeddings.create(\n",
        "                 model=\"text-embedding-3-small\", # Use the embedding model\n",
        "                 input=prompt_text\n",
        "             )\n",
        "             embedding = response.data[0].embedding\n",
        "             # Store the embedding along with a reference to the original log index\n",
        "             self.usage_embeddings.append({'embedding': embedding, 'original_index': len(self.usage_logs) - 1})\n",
        "             print(f\"Generated embedding for log entry {len(self.usage_logs) - 1}\")\n",
        "         except Exception as e:\n",
        "             print(f\"Error generating embedding for log entry: {e}\")\n",
        "\n",
        "\n",
        "    def generate_all_usage_embeddings(self):\n",
        "        \"\"\"Generates embeddings for all usage logs using OpenAI API.\"\"\"\n",
        "        print(\"Generating embeddings for all usage logs using OpenAI API...\")\n",
        "        self.usage_embeddings = [] # Clear existing embeddings\n",
        "        for i, log_entry in enumerate(self.usage_logs):\n",
        "            try:\n",
        "                prompt_text = log_entry['prompt']\n",
        "                response = self.client.embeddings.create(\n",
        "                    model=\"text-embedding-3-small\", # Use the embedding model\n",
        "                    input=prompt_text\n",
        "                )\n",
        "                embedding = response.data[0].embedding\n",
        "                # Store the embedding along with a reference to the original log index\n",
        "                self.usage_embeddings.append({'embedding': embedding, 'original_index': i})\n",
        "            except Exception as e:\n",
        "                 print(f\"Error generating embedding for log entry {i}: {e}\")\n",
        "\n",
        "        print(f\"Generated {len(self.usage_embeddings)} embeddings.\")\n",
        "\n",
        "\n",
        "    def find_similar_usage(self, query_prompt, n=3):\n",
        "        \"\"\"\n",
        "        Finds the n most similar usage logs based on prompt embedding similarity.\n",
        "\n",
        "        Args:\n",
        "            query_prompt (str): The prompt to find similar usage for.\n",
        "            n (int): The number of closest usage logs to find.\n",
        "\n",
        "        Returns:\n",
        "            list of dict: A list of dictionaries for the n most similar usage logs,\n",
        "                          each containing 'distance', 'original_log' (the full log entry).\n",
        "                          Returns an empty list if no embeddings are available.\n",
        "        \"\"\"\n",
        "        if not self.usage_embeddings:\n",
        "            print(\"No usage embeddings available to query.\")\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Generate embedding for the query prompt\n",
        "            query_response = self.client.embeddings.create(\n",
        "                model=\"text-embedding-3-small\", # Use the embedding model\n",
        "                input=query_prompt\n",
        "            )\n",
        "            query_embedding = query_response.data[0].embedding\n",
        "\n",
        "            distances = []\n",
        "            for item in self.usage_embeddings:\n",
        "                dist = distance.cosine(query_embedding, item['embedding'])\n",
        "                distances.append({\n",
        "                    \"distance\": dist,\n",
        "                    \"original_index\": item['original_index']\n",
        "                    })\n",
        "\n",
        "            distances_sorted = sorted(distances, key=lambda x: x['distance'])\n",
        "\n",
        "            # Get the original log entries for the n closest\n",
        "            similar_logs = []\n",
        "            for item in distances_sorted[0:n]:\n",
        "                original_log = self.usage_logs[item['original_index']]\n",
        "                similar_logs.append({\n",
        "                    \"distance\": item['distance'],\n",
        "                    \"original_log\": original_log\n",
        "                })\n",
        "\n",
        "            return similar_logs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during similar usage query: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "    def detect_ai(self, text):\n",
        "        \"\"\"Uses the OpenAI LLM to assess if content is AI generated.\"\"\"\n",
        "        print(\"Using OpenAI LLM for AI detection...\")\n",
        "        try:\n",
        "            # Craft a prompt for the LLM to assess AI generation\n",
        "            # This prompt might need refinement for better results\n",
        "            prompt_text = f\"Assess the likelihood that the following text was generated by an AI. Respond ONLY with a score between 0 and 1, where 1 is highly likely to be AI generated, followed by a brief explanation on a new line.\\n\\nText to assess:\\n{text}\\n\\nScore:\"\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\", # Or another suitable model\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an AI text detection assistant.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt_text}\n",
        "                ],\n",
        "                max_tokens=50 # Restrict tokens to manage cost\n",
        "            )\n",
        "\n",
        "            # Attempt to parse the score from the LLM's response\n",
        "            response_text = response.choices[0].message.content.strip()\n",
        "            print(f\"LLM Raw Response: {response_text}\") # Print raw response for debugging\n",
        "            try:\n",
        "                # Assuming the LLM starts the response with the score on the first line\n",
        "                detection_score = float(response_text.splitlines()[0])\n",
        "            except (ValueError, IndexError):\n",
        "                print(f\"Could not parse score from LLM response: '{response_text}'. Assuming a default score.\")\n",
        "                detection_score = 0.5 # Default score if parsing fails\n",
        "\n",
        "            # Simulate some processing time (optional, but good for realism)\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            is_ai_generated = detection_score > self.ai_detection_threshold\n",
        "            print(f\"AI detection score from LLM: {detection_score:.2f}. Is likely AI: {is_ai_generated}\")\n",
        "            # Return the AI detection status, score, and potentially the full LLM response\n",
        "            return is_ai_generated, detection_score, response_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during OpenAI LLM AI detection: {e}\")\n",
        "            # Fallback in case of API errors\n",
        "            return False, 0.0, f\"Error: {e}\"\n",
        "\n",
        "\n",
        "    def check_ethical_usage(self, prompt, generated_text):\n",
        "        \"\"\"Basic check for ethical usage, combining prompt analysis and AI detection.\"\"\"\n",
        "        print(\"Checking for ethical usage...\")\n",
        "\n",
        "        # Basic Human Prompt Checker logic (simplified)\n",
        "        prompt_lower = prompt.lower()\n",
        "        if \"write my entire thesis\" in prompt_lower or \"do my whole thesis\" in prompt_lower:\n",
        "            print(\"Ethical Alert: Skeptical usage detected (attempting to write entire thesis). Encourage ethical use and own writing.\")\n",
        "        elif \"generate abstract\" in prompt_lower or \"write introduction\" in prompt_lower:\n",
        "             print(\"Ethical Note: AI used for structural writing. Remember to review and rephrase carefully.\")\n",
        "        elif \"analyze this concept\" in prompt_lower or \"explain this\" in prompt_lower:\n",
        "             print(\"Ethical Usage: AI used for understanding/analysis. Good practice!\")\n",
        "        else:\n",
        "            print(\"Prompt intent: Could be ethical, further analysis needed in a complex model.\")\n",
        "\n",
        "\n",
        "        # Basic Ethical Violation Alert logic (simplified, tied to AI detection and prompt analysis)\n",
        "        is_ai, score, llm_response = self.detect_ai(generated_text)\n",
        "\n",
        "        if is_ai and (\"write my entire thesis\" in prompt_lower or \"do my whole thesis\" in prompt_lower):\n",
        "            print(\"Ethical VIOLATION Alert: High potential for academic dishonesty due to prompt and AI content.\")\n",
        "        elif is_ai:\n",
        "            print(\"Ethical Alert: Potential AI-generated content detected. Encourage rephrasing.\")\n",
        "\n",
        "        # Example of checking for over-reliance (very basic) - in a real model, this would look at usage patterns over time\n",
        "        # This basic check uses the length of usage logs and checks recent prompts for \"generate\"\n",
        "        if len(self.usage_logs) > 5 and all(\"generate\" in entry['prompt'].lower() for entry in self.usage_logs[-5:]):\n",
        "             print(\"Ethical Alert: Potential over-reliance on AI generation detected. Encourage critical thinking and original writing.\")\n",
        "\n",
        "\n",
        "# Example Usage (after creating and initializing the openai client):\n",
        "# Make sure the 'client' object is defined from a previous cell\n",
        "# ethics_module = EthicsModule(openai_client=client)\n",
        "# ethics_module.log_usage(\"help me understand this concept\", \"research\", thesis_stage=\"literature review\")\n",
        "# ethics_module.generate_all_usage_embeddings() # Generate embeddings after logging\n",
        "# similar_logs = ethics_module.find_similar_usage(\"find papers on NLP\")\n",
        "# print(\"\\nSimilar Usage Logs:\")\n",
        "# for log in similar_logs:\n",
        "#      print(f\"  - Distance: {log['distance']:.4f}, Prompt: '{log['original_log']['prompt']}'\")\n",
        "# is_ai, score, llm_response = ethics_module.detect_ai(\"The quick brown fox jumps over the lazy dog.\")\n",
        "# print(f\"Detect AI Result: Is AI: {is_ai}, Score: {score:.2f}, LLM Response: {llm_response}\")\n",
        "# is_ai, score, llm_response = ethics_module.detect_ai(\"As an AI language model, I can help with that.\")\n",
        "# print(f\"Detect AI Result: Is AI: {is_ai}, Score: {score:.2f}, LLM Response: {llm_response}\")\n",
        "# ethics_module.check_ethical_usage(\"write my entire thesis\", \"Here is a thesis.\")\n",
        "# ethics_module.check_ethical_usage(\"analyze this concept\", \"Based on my training data, this concept is...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "372fdee9",
        "outputId": "503c3974-739e-4af1-ed0b-143d2f8ef970"
      },
      "source": [
        "\n",
        "# Example Usage (after creating and initializing the openai client):\n",
        "\n",
        "if 'client' in locals():\n",
        "    # Initialize the ethics module with the created client\n",
        "    # Make sure the EthicsModule class is defined in a previous cell\n",
        "    ethics_module = EthicsModule(openai_client=client)\n",
        "\n",
        "    print(\"--- Testing log_usage ---\")\n",
        "    ethics_module.log_usage(\"Help me find papers on natural language processing\", \"research\", thesis_stage=\"literature review\")\n",
        "    ethics_module.log_usage(\"Generate an outline for my introduction\", \"writing_support\", thesis_stage=\"introduction\")\n",
        "    print(\"\\nCurrent usage logs:\")\n",
        "    display(pd.DataFrame(ethics_module.usage_logs))\n",
        "\n",
        "    print(\"\\n--- Testing detect_ai ---\")\n",
        "    # Test with human-like text (shorter)\n",
        "    is_ai_human, score_human, llm_response_human = ethics_module.detect_ai(\"The quick brown fox jumps over the lazy dog. This is a short sentence.\")\n",
        "    print(f\"Test 1 Result: Is AI: {is_ai_human}, Score: {score_human:.2f}, LLM Response: {llm_response_human}\")\n",
        "\n",
        "    # Test with text likely generated by an AI (shorter)\n",
        "    is_ai_ai, score_ai, llm_response_ai = ethics_module.detect_ai(\"As an AI language model, I can assist you.\")\n",
        "    print(f\"Test 2 Result: Is AI: {is_ai_ai}, Score: {score_ai:.2f}, LLM Response: {llm_response_ai}\")\n",
        "\n",
        "    # Test with some placeholder generated text (shorter)\n",
        "    is_ai_generated, score_generated, llm_response_generated = ethics_module.detect_ai(\"Generated text about a topic.\")\n",
        "    print(f\"Test 3 Result: Is AI: {is_ai_generated}, Score: {score_generated:.2f}, LLM Response: {llm_response_generated}\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Testing check_ethical_usage ---\")\n",
        "    # Test with an ethical prompt and seemingly human text (shorter)\n",
        "    ethics_module.check_ethical_usage(\"analyze this concept\", \"Based on my understanding, this concept is complex.\")\n",
        "\n",
        "    # Test with a skeptical prompt and seemingly AI text (shorter)\n",
        "    ethics_module.check_ethical_usage(\"write my entire thesis\", \"Here is a short thesis summary.\")\n",
        "\n",
        "    # Test with an ethical prompt and text likely flagged as AI (shorter)\n",
        "    ethics_module.check_ethical_usage(\"explain this theory\", \"Based on my training data, this theory is interesting.\")\n",
        "\n",
        "    # Test simple over-reliance check (might require more log entries to trigger)\n",
        "    print(\"\\n--- Testing potential over-reliance check (might need more logs) ---\")\n",
        "    # Add more \"generate\" prompts to usage logs to potentially trigger over-reliance alert (shorter prompts)\n",
        "    for _ in range(5):\n",
        "        ethics_module.log_usage(\"generate text\", \"writing_support\")\n",
        "    ethics_module.check_ethical_usage(\"continue writing\", \"More text.\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: 'client' object not found. Please run the cell to set up the OpenAI client first.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing log_usage ---\n",
            "Usage logged: Timestamp=2025-06-19 15:58:34.555343, Prompt='Help me find papers on natural language processing', Intent='research', Thesis Stage='literature review'\n",
            "Usage logged: Timestamp=2025-06-19 15:58:34.555446, Prompt='Generate an outline for my introduction', Intent='writing_support', Thesis Stage='introduction'\n",
            "\n",
            "Current usage logs:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                   timestamp  \\\n",
              "0 2025-06-19 15:58:34.555343   \n",
              "1 2025-06-19 15:58:34.555446   \n",
              "\n",
              "                                              prompt           intent  \\\n",
              "0  Help me find papers on natural language proces...         research   \n",
              "1            Generate an outline for my introduction  writing_support   \n",
              "\n",
              "        thesis_stage  \n",
              "0  literature review  \n",
              "1       introduction  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57228404-1a26-496e-a0f9-2213fd59a450\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>prompt</th>\n",
              "      <th>intent</th>\n",
              "      <th>thesis_stage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-06-19 15:58:34.555343</td>\n",
              "      <td>Help me find papers on natural language proces...</td>\n",
              "      <td>research</td>\n",
              "      <td>literature review</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-06-19 15:58:34.555446</td>\n",
              "      <td>Generate an outline for my introduction</td>\n",
              "      <td>writing_support</td>\n",
              "      <td>introduction</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57228404-1a26-496e-a0f9-2213fd59a450')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-57228404-1a26-496e-a0f9-2213fd59a450 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-57228404-1a26-496e-a0f9-2213fd59a450');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-873314c5-ee40-41fe-a606-643747d7275f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-873314c5-ee40-41fe-a606-643747d7275f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-873314c5-ee40-41fe-a606-643747d7275f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"ethics_module\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-06-19 15:58:34.555343\",\n        \"max\": \"2025-06-19 15:58:34.555446\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2025-06-19 15:58:34.555446\",\n          \"2025-06-19 15:58:34.555343\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Generate an outline for my introduction\",\n          \"Help me find papers on natural language processing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"intent\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"writing_support\",\n          \"research\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thesis_stage\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"introduction\",\n          \"literature review\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing detect_ai ---\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.1  \n",
            "The text is a well-known pangram and consists of simple, straightforward sentences that are common in human writing. The lack of complexity and the use of a familiar expression suggest a low likelihood of AI generation.\n",
            "AI detection score from LLM: 0.10. Is likely AI: False\n",
            "Test 1 Result: Is AI: False, Score: 0.10, LLM Response: 0.1  \n",
            "The text is a well-known pangram and consists of simple, straightforward sentences that are common in human writing. The lack of complexity and the use of a familiar expression suggest a low likelihood of AI generation.\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "This text is quite generic and could be easily produced by both an AI and a human, making it less likely to be definitively AI-generated.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "Test 2 Result: Is AI: False, Score: 0.20, LLM Response: 0.2  \n",
            "This text is quite generic and could be easily produced by both an AI and a human, making it less likely to be definitively AI-generated.\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "The text is vague and doesn't exhibit typical markers of AI generation, such as overly complex structures or lack of coherence. It seems more like a placeholder than a fully developed piece.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "Test 3 Result: Is AI: False, Score: 0.20, LLM Response: 0.2  \n",
            "The text is vague and doesn't exhibit typical markers of AI generation, such as overly complex structures or lack of coherence. It seems more like a placeholder than a fully developed piece.\n",
            "\n",
            "--- Testing check_ethical_usage ---\n",
            "Checking for ethical usage...\n",
            "Ethical Usage: AI used for understanding/analysis. Good practice!\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "The text is simple and lacks the hallmark characteristics of AI-generated content, such as elaborate phrasing or unusual sentence structure, making it more likely to be human-written.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "Checking for ethical usage...\n",
            "Ethical Alert: Skeptical usage detected (attempting to write entire thesis). Encourage ethical use and own writing.\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.2  \n",
            "The text is very brief and lacks complexity, making it difficult to determine, but it does not exhibit typical AI-produced verbosity or elaborate structure often found in AI-generated content.\n",
            "AI detection score from LLM: 0.20. Is likely AI: False\n",
            "Checking for ethical usage...\n",
            "Ethical Usage: AI used for understanding/analysis. Good practice!\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.3  \n",
            "The text is simple and lacks complexity, but it could plausibly be written by a human reflecting on a theory. The phrase \"based on my training data\" is suggestive of AI, yet it’s not strong enough to definit\n",
            "AI detection score from LLM: 0.30. Is likely AI: False\n",
            "\n",
            "--- Testing potential over-reliance check (might need more logs) ---\n",
            "Usage logged: Timestamp=2025-06-19 15:58:47.393860, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Usage logged: Timestamp=2025-06-19 15:58:47.393927, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Usage logged: Timestamp=2025-06-19 15:58:47.393943, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Usage logged: Timestamp=2025-06-19 15:58:47.393956, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Usage logged: Timestamp=2025-06-19 15:58:47.393967, Prompt='generate text', Intent='writing_support', Thesis Stage='unknown'\n",
            "Checking for ethical usage...\n",
            "Prompt intent: Could be ethical, further analysis needed in a complex model.\n",
            "Using OpenAI LLM for AI detection...\n",
            "LLM Raw Response: 0.1\n",
            "\n",
            "The text is too brief and lacks complexity or context, making it more likely to be human-generated, possibly as a placeholder or casual notation.\n",
            "AI detection score from LLM: 0.10. Is likely AI: False\n",
            "Ethical Alert: Potential over-reliance on AI generation detected. Encourage critical thinking and original writing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18bd052c"
      },
      "source": [
        "## Proposed RL Agent State Structure\n",
        "\n",
        "This outlines a potential structure for the State that the Reinforcement Learning agent (overseeing project evolution and ethics) would observe. This state combines information from the Ethics Module with broader thesis progress details.\n",
        "\n",
        "The state would likely be represented as a numerical vector or a structured object that the RL model can process.\n",
        "\n",
        "**Components of the State:**\n",
        "\n",
        "1.  **Ethical State Features (from Ethics Module):**\n",
        "    *   **AI Detection Score:** The score from the `detect_ai` function for the most recent generated content (e.g., a value between 0 and 1).\n",
        "    *   **Prompt Classification:** A categorical or numerical representation of the last prompt's ethical classification (e.g., 0 for ethical, 1 for structural, 2 for skeptical/dangerous).\n",
        "    *   **Usage Frequency:** Metrics on recent LLM usage (e.g., number of LLM interactions in the last hour/day, proportion of \"generate\" prompts).\n",
        "    *   **Embedding Similarity:** The similarity score from `find_similar_usage` when querying the current prompt against past usage logs (e.g., the distance to the most similar ethical/skeptical past interaction).\n",
        "    *   **Ethical Alert Status:** Flags indicating if any ethical violations or warnings are currently active (e.g., binary flags for over-reliance alert, academic dishonesty alert).\n",
        "    *   **Human Engagement:** Metrics on user interaction with previous ethical interventions (e.g., did the user rephrase AI content, did they engage with a reflection prompt).\n",
        "\n",
        "2.  **Thesis Progress Features:**\n",
        "    *   **Current Thesis Stage:** A categorical or numerical representation of the current stage of the thesis (e.g., 0 for planning, 1 for literature review, 2 for methodology, 3 for writing, etc.).\n",
        "    *   **Task Completion:** Percentage of planned tasks completed for the current stage or overall project.\n",
        "    *   **Time-based Metrics:** Time spent on the project recently, time remaining until deadlines.\n",
        "    *   **Advisor Feedback Status:** A flag or metric indicating the presence and recency of unaddressed advisor feedback.\n",
        "\n",
        "3.  **Performance Features:**\n",
        "    *   **Work Quality Score:** A metric representing the quality of recent thesis work (this would be challenging to define and might require human evaluation or proxy metrics).\n",
        "    *   **Progress Rate:** A measure of how quickly tasks are being completed or milestones are being reached.\n",
        "\n",
        "**Combining the State:**\n",
        "\n",
        "These individual features would be combined into a single state representation that the RL agent's model can process. For a neural network-based RL model, this would typically be a flattened numerical vector. Categorical features would need to be appropriately encoded (e.g., one-hot encoding).\n",
        "\n",
        "**Next Steps for Implementation (for later):**\n",
        "\n",
        "*   Define the specific numerical or categorical representation for each state feature.\n",
        "*   Develop the logic within the thesis assistant to collect and compile this information into the state vector at each time step.\n",
        "*   Ensure the Ethics Module submodules (Usage_Logger, AI_Detector, etc.) are providing the necessary data points in a format that can be easily integrated into the state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXkiYjcX9aRp",
        "outputId": "28563520-d819-45fb-eedd-3342f1aca44d"
      },
      "source": [
        "!pip install streamlit gymnasium stable-baselines3\n",
        "!pip install numpy # Ensure numpy is installed if not already\n",
        "!pip install pandas # Ensure pandas is installed if not already\n",
        "!pip install scipy # Ensure scipy is installed if not already"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.1.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.6.0-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable-baselines3-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Configuration Manager\n",
        "\n",
        "The `RLConfigManager` class is responsible for managing the centralized configuration of the Reinforcement Learning system for the Thesis Assistant's ethics module. This configuration is dynamic, allowing developers to define and modify key aspects of the RL environment and agent behavior without directly altering the core code.\n",
        "\n",
        "**Purpose:**\n",
        "The primary purpose of this class is to provide a persistent and easily modifiable way to store the settings that govern the RL agent's learning and decision-making process. This includes defining the observable state variables, the available actions the agent can take, the reward values associated with different events, and how each action influences the state.\n",
        "\n",
        "**Key Components:**\n",
        "- `CONFIG_FILE`: A class variable specifying the name of the JSON file used for storing the configuration (`rl_config.json`).\n",
        "- `load_config()`: A class method that loads the configuration from the `CONFIG_FILE`. If the file does not exist, it creates a default configuration and saves it.\n",
        "- `save_config(config)`: A class method that saves a given configuration dictionary to the `CONFIG_FILE`.\n",
        "\n",
        "**How to Use:**\n",
        "- To get the current configuration, call `RLConfigManager.load_config()`. This will return a dictionary containing the settings.\n",
        "- To update the configuration, modify the dictionary obtained from `load_config()` and then call `RLConfigManager.save_config(updated_config)`.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **Developer Dashboard:** The `DeveloperDashboard` uses `RLConfigManager` to load and save the configuration, allowing developers to interactively modify the settings via a Streamlit interface.\n",
        "- **Data Preprocessor:** The `DataPreprocessor` uses the configuration (specifically, `state_variables` and `reward_config`) to determine how to convert raw usage logs into state vectors and compute rewards.\n",
        "- **RL Environment:** The `EthicsSupervisorEnv` is built dynamically based on the configuration loaded by `RLConfigManager`, defining its observation space, action space, and state transition logic (`action_effects`).\n",
        "- **RL Training Loop and Trainer:** These components implicitly rely on the configuration loaded by `RLConfigManager` via the environment and preprocessor.\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "The configuration is stored in a JSON file (`rl_config.json`) and represented in Python as a dictionary with the following structure:\n",
        "- \"state_variables\": A list of strings, where each string is the name of a variable that constitutes the state observed by the RL agent.\n",
        "- \"actions\": A list of strings, where each string is a human-readable label for an action the RL agent can take. The index of the action in this list is the action ID used by the RL model.\n",
        "- \"reward_config\": A dictionary mapping event names (as found in usage logs) to numerical reward values.\n",
        "- \"action_effects\": A nested dictionary defining how actions change state variables. The outer keys are string representations of action indices, and the inner dictionaries map state variable names to the delta value (change) applied to that variable when the action is taken.\n"
      ],
      "metadata": {
        "id": "lGMt9HgODAiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 1 — CONFIGURATION MANAGER\n",
        "# ===========================================================\n",
        "import json\n",
        "import os\n",
        "import gymnasium as gym\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import random\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "class RLConfigManager:\n",
        "    \"\"\"\n",
        "    Manages centralized configuration for the dynamic Reinforcement Learning system.\n",
        "\n",
        "    This class handles loading and saving configuration settings for the RL environment,\n",
        "    including state variables, action space definitions, reward shaping values,\n",
        "    and the effects of actions on state variables.\n",
        "    \"\"\"\n",
        "\n",
        "    CONFIG_FILE = \"rl_config.json\"\n",
        "\n",
        "    @classmethod\n",
        "    def load_config(cls):\n",
        "        \"\"\"\n",
        "        Load RL configuration from a JSON file.\n",
        "\n",
        "        If the configuration file does not exist, a default configuration is created\n",
        "        and saved to the file.\n",
        "\n",
        "        Returns:\n",
        "            config (dict): The loaded configuration dictionary.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(cls.CONFIG_FILE):\n",
        "            # Define a default configuration if the file is not found\n",
        "            default_config = {\n",
        "                \"state_variables\": [\"embedding_drift\", \"ai_usage\", \"ethical_flags\", \"advisor_feedback\", \"timestep\"],\n",
        "                \"actions\": [\"Allow prompt\", \"Suggest reflection\", \"Ethical warning\", \"Suggest rewriting\", \"Advisor feedback reminder\", \"Disable AI feature\"],\n",
        "                \"reward_config\": {\"user_revised\": 2, \"ai_violation\": -3, \"advisor_positive\": 3, \"rewrite_accepted\": 1, \"milestone_completed\": 5, \"hallucination_detected\": -2},\n",
        "                \"action_effects\": {\"1\": {\"ai_usage\": -0.05, \"embedding_drift\": -0.05}, \"3\": {\"ethical_flags\": -0.1}, \"4\": {\"advisor_feedback\": 0.1}}\n",
        "            }\n",
        "            cls.save_config(default_config)\n",
        "        # Load the configuration from the JSON file\n",
        "        with open(cls.CONFIG_FILE, \"r\") as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    @classmethod\n",
        "    def save_config(cls, config):\n",
        "        \"\"\"\n",
        "        Save the current configuration to the JSON file.\n",
        "\n",
        "        Args:\n",
        "            config (dict): The configuration dictionary to save.\n",
        "        \"\"\"\n",
        "        with open(cls.CONFIG_FILE, \"w\") as f:\n",
        "            json.dump(config, f, indent=2) # Save with indentation for readability\n"
      ],
      "metadata": {
        "id": "aHkvncRKH_ue"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Developer Dashboard\n",
        "\n",
        "The `DeveloperDashboard` class provides a graphical user interface (GUI) built with Streamlit, allowing developers to interactively configure the Reinforcement Learning system. This dashboard simplifies the process of modifying the state space, action space, reward shaping, and action effects without directly editing the configuration JSON file.\n",
        "\n",
        "**Purpose:**\n",
        "The main purpose is to offer a user-friendly way for developers to experiment with and tune the RL agent's behavior and the environment's dynamics. This is crucial during the development and testing phases of the RL ethics supervisor.\n",
        "\n",
        "**Key Components:**\n",
        "- `__init__()`: Initializes the dashboard by loading the current configuration using `RLConfigManager.load_config()`.\n",
        "- `launch()`: The main method to launch the Streamlit application. It sets the title and calls methods to display and edit different parts of the configuration.\n",
        "- `edit_action_space()`: Displays the current actions and provides an input field and button to add new actions to the configuration.\n",
        "- `edit_reward_shaping()`: Displays sliders for each item in the `reward_config`, allowing developers to adjust the reward values.\n",
        "- `edit_action_effects()`: Provides input fields to define how a specific action (identified by index) affects a specific state variable, allowing developers to add these effects to the `action_effects` dictionary in the configuration.\n",
        "- `save_button()`: Displays a button that, when clicked, saves the currently modified configuration back to the `rl_config.json` file using `RLConfigManager.save_config()`.\n",
        "\n",
        "**How to Use:**\n",
        "- To run the dashboard, execute the `launch()` method of an instance of `DeveloperDashboard`. Note that Streamlit applications are typically run from the command line using `streamlit run your_script_name.py`. In a notebook environment, you might need specific integrations or run the relevant cell and access the output via a provided URL.\n",
        "- Use the input fields, sliders, and buttons in the web interface to modify the configuration settings.\n",
        "- Click the \"Save Full Configuration\" button to persist the changes.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **Configuration Manager:** The `DeveloperDashboard` directly interacts with `RLConfigManager` to load the initial configuration when launched and to save the updated configuration.\n",
        "- **RL System Components:** The changes made through the dashboard directly influence how the `DataPreprocessor`, `EthicsSupervisorEnv`, and the RL agent (`EthicsSupervisorRL`) behave when they load the updated configuration.\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "The dashboard works directly with the dictionary structure managed by `RLConfigManager`. Changes made in the UI are reflected in the `self.config` dictionary within the `DeveloperDashboard` instance before being saved."
      ],
      "metadata": {
        "id": "gr6FY9KeDlO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: DEVELOPER DASHBOARD (Class-Based Streamlit Interface with Full Docstrings)\n",
        "# ===========================================================\n",
        "\n",
        "class DeveloperDashboard:\n",
        "    \"\"\"\n",
        "    Streamlit-based developer interface for interactively updating RL configuration.\n",
        "\n",
        "    This dashboard allows developers to view and modify the action space, reward shaping,\n",
        "    and action effects defined in the RL configuration file.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the dashboard by loading the current configuration.\n",
        "        \"\"\"\n",
        "        self.config = RLConfigManager.load_config()\n",
        "\n",
        "    def launch(self):\n",
        "        \"\"\"\n",
        "        Launch the Streamlit dashboard interface.\n",
        "        \"\"\"\n",
        "        st.title(\"🎯 Thesis RL Developer Dashboard\")\n",
        "        self.edit_action_space()\n",
        "        self.edit_reward_shaping()\n",
        "        self.edit_action_effects()\n",
        "        self.save_button()\n",
        "\n",
        "    def edit_action_space(self):\n",
        "        \"\"\"\n",
        "        Display and edit the RL action space.\n",
        "\n",
        "        Developers can add new high-level intervention actions here.\n",
        "        \"\"\"\n",
        "        st.header(\"1️⃣ Manage Action Space\")\n",
        "        st.write(\"Define high-level interventions available to RL agent:\")\n",
        "\n",
        "        st.subheader(\"Current Actions:\")\n",
        "        # Display current actions with their indices\n",
        "        for idx, action in enumerate(self.config[\"actions\"]):\n",
        "            st.write(f\"**{idx}:** {action}\")\n",
        "\n",
        "        new_action = st.text_input(\"Add New Action:\")\n",
        "        if st.button(\"Add Action\"):\n",
        "            if new_action.strip(): # Ensure the input is not empty or just whitespace\n",
        "                self.config[\"actions\"].append(new_action.strip())\n",
        "                st.success(f\"✅ Added action: '{new_action}'\")\n",
        "\n",
        "    def edit_reward_shaping(self):\n",
        "        \"\"\"\n",
        "        Display and edit the reward shaping values.\n",
        "\n",
        "        Developers can adjust the reward values associated with key supervision signals.\n",
        "        \"\"\"\n",
        "        st.header(\"2️⃣ Edit Reward Shaping\")\n",
        "        st.write(\"Adjust reward values for key supervision signals:\")\n",
        "\n",
        "        # Create sliders for each reward configuration item\n",
        "        for key, val in self.config[\"reward_config\"].items():\n",
        "            new_val = st.slider(f\"Reward for {key}:\", -10, 10, val)\n",
        "            self.config[\"reward_config\"][key] = new_val\n",
        "\n",
        "    def edit_action_effects(self):\n",
        "        \"\"\"\n",
        "        Display and define the effects of actions on state variables.\n",
        "\n",
        "        Developers can specify how choosing a particular action changes the values\n",
        "        of specific state variables.\n",
        "        \"\"\"\n",
        "        st.header(\"3️⃣ Define Action Effects\")\n",
        "        st.write(\"Specify which state variables are influenced by actions:\")\n",
        "\n",
        "        action_idx = st.text_input(\"Action Index (integer):\", value=\"1\")\n",
        "        variable_name = st.text_input(\"State Variable (e.g. ai_usage):\")\n",
        "        delta = st.number_input(\"Delta Change (+/-):\", step=0.01, value=0.0)\n",
        "\n",
        "        if st.button(\"Add Effect\"):\n",
        "            # Add the defined effect to the configuration\n",
        "            effects = self.config.setdefault(\"action_effects\", {}) # Get or create action_effects dictionary\n",
        "            action_effect = effects.setdefault(str(action_idx), {}) # Get or create effects for the specific action index\n",
        "            action_effect[variable_name] = delta\n",
        "            st.success(f\"✅ Effect added: Action {action_idx} → {variable_name} += {delta}\")\n",
        "\n",
        "    def save_button(self):\n",
        "        \"\"\"\n",
        "        Provide a save button to persist updated configuration back to disk.\n",
        "        \"\"\"\n",
        "        if st.button(\"Save Full Configuration\"):\n",
        "            RLConfigManager.save_config(self.config)\n",
        "            st.success(\"✅ All changes saved successfully!\")\n"
      ],
      "metadata": {
        "id": "woYe1lCLIASQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Data Preprocessor\n",
        "\n",
        "The `DataPreprocessor` class is a crucial component of the RL training pipeline. Its role is to bridge the gap between the raw usage logs generated by the thesis assistant and the structured input required by the Reinforcement Learning environment and agent.\n",
        "\n",
        "**Purpose:**\n",
        "The main purpose is to transform detailed log entries, which capture various events and state information from a user's interaction with the assistant, into a standardized numerical state vector that the RL agent can observe and a corresponding reward signal based on the events in the log.\n",
        "\n",
        "**Key Components:**\n",
        "- `__init__(config)`: Initializes the preprocessor with the current RL configuration, which includes definitions of state variables and reward mapping.\n",
        "- `extract_state(log_entry)`: Takes a single log entry (a dictionary) and converts it into a normalized NumPy array representing the state vector. It selects the relevant information from the log entry based on the `state_variables` defined in the configuration.\n",
        "- `compute_reward(log_entry)`: Calculates the reward associated with a log entry. It looks for specific event keys within the log entry (as defined in the `reward_config` in the configuration) and sums up the corresponding reward values.\n",
        "\n",
        "**How to Use:**\n",
        "- Create an instance of `DataPreprocessor`, passing the loaded RL configuration: `preprocessor = DataPreprocessor(config)`.\n",
        "- For each raw usage log entry (a dictionary), call `preprocessor.extract_state(log_entry)` to get the state vector and `preprocessor.compute_reward(log_entry)` to get the reward.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **Configuration Manager:** The `DataPreprocessor` relies heavily on the configuration loaded by `RLConfigManager` to know which log attributes correspond to state variables and how to map events to reward values.\n",
        "- **RL Training Loop:** The `RLTrainingLoop` uses the `DataPreprocessor` to process batches of logs before feeding the resulting state and reward information (or using it to update the environment's state and compute rewards in a more interactive simulation) to the RL agent for training.\n",
        "- **Simulators (Synthetic Cohort/Student):** The simulator classes generate log entries that are in a format expected by the `DataPreprocessor`.\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "- It processes input in the form of dictionaries (log entries).\n",
        "- It outputs a NumPy array for the state and a float for the reward.\n",
        "- It uses the `state_variables` and `reward_config` from the RL configuration dictionary to perform the conversion and computation. The keys in `reward_config` are expected to potentially appear as boolean flags or other relevant values in the input `log_entry` dictionaries."
      ],
      "metadata": {
        "id": "tv9IOabADzxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 3 — DATA PREPROCESSOR (LOG TO STATE CONVERSION)\n",
        "# ===========================================================\n",
        "\n",
        "class DataPreprocessor:\n",
        "    \"\"\"\n",
        "    Converts raw usage logs into RL state vectors and reward labels.\n",
        "\n",
        "    This class is responsible for transforming the detailed log entries\n",
        "    from the thesis assistant's usage into a format (state vectors and rewards)\n",
        "    that the Reinforcement Learning agent can understand and use for training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initialize with the current RL configuration structure.\n",
        "\n",
        "        Args:\n",
        "            config (dict): Loaded RL configuration dictionary.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "\n",
        "    def extract_state(self, log_entry):\n",
        "        \"\"\"\n",
        "        Convert a single log entry into an RL state vector.\n",
        "\n",
        "        The state vector is constructed based on the state variables defined\n",
        "        in the loaded configuration. Values are normalized where appropriate.\n",
        "\n",
        "        Args:\n",
        "            log_entry (dict): A single usage log entry.\n",
        "\n",
        "        Returns:\n",
        "            state (np.ndarray): The normalized RL state vector.\n",
        "        \"\"\"\n",
        "        state = []\n",
        "        # Iterate through state variables defined in the config\n",
        "        for var in self.config[\"state_variables\"]:\n",
        "            if var == \"timestep\":\n",
        "                # For timestep, use deadline_ratio from the log entry and normalize (assuming max timestep is 100 for normalization)\n",
        "                state.append(log_entry.get(\"deadline_ratio\", 0.0))\n",
        "            else:\n",
        "                # For other variables, get the value directly from the log entry (defaulting to 0.0 if not present)\n",
        "                value = log_entry.get(var, 0.0)\n",
        "                # Ensure the value is a float for consistency\n",
        "                state.append(float(value))\n",
        "        return np.array(state, dtype=np.float32) # Ensure float32 dtype for compatibility with RL libraries\n",
        "\n",
        "\n",
        "    def compute_reward(self, log_entry):\n",
        "        \"\"\"\n",
        "        Compute the shaped reward for a given log event.\n",
        "\n",
        "        The reward is calculated based on the 'reward_config' in the loaded\n",
        "        configuration and the presence of specific keys (representing events)\n",
        "        in the log entry.\n",
        "\n",
        "        Args:\n",
        "            log_entry (dict): A single usage log entry.\n",
        "\n",
        "        Returns:\n",
        "            reward (float): The computed reward value.\n",
        "        \"\"\"\n",
        "        reward = 0.0\n",
        "        # Iterate through the reward configuration items\n",
        "        for key, value in self.config[\"reward_config\"].items():\n",
        "            # Check if the key exists in the log entry and its value is True (for boolean events)\n",
        "            # This assumes the reward_config keys correspond to boolean flags in the log entry\n",
        "            if key in log_entry and log_entry.get(key) is True:\n",
        "                reward += value\n",
        "        return reward"
      ],
      "metadata": {
        "id": "wRlBWM72IBBr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: RL Environment (EthicsSupervisorEnv)\n",
        "\n",
        "The `EthicsSupervisorEnv` class is a custom Reinforcement Learning environment built using the Gymnasium library. It is designed to simulate the interaction between the Thesis Assistant's ethics module and a student's thesis writing process, allowing an RL agent to learn optimal intervention policies. A key feature is its dynamic nature, where the state space, action space, rewards, and state transitions are defined by the loaded configuration.\n",
        "\n",
        "**Purpose:**\n",
        "To provide a simulated environment where the RL agent (the Ethics Supervisor) can learn through trial and error. The environment presents states representing the current situation (ethical flags, AI usage, progress, etc.) and provides feedback (rewards) based on the agent's chosen actions and the resulting changes in the simulated student's state.\n",
        "\n",
        "**Key Components:**\n",
        "- `__init__(ethics_module, config)`: Initializes the environment. It takes a simulated or actual `ethics_module` object (which holds the state variables) and the RL configuration. It dynamically defines the `observation_space` and `action_space` based on the configuration.\n",
        "- `reset(seed=None)`: Resets the environment to an initial state at the beginning of a new simulation episode. It also resets the internal timestep counter.\n",
        "- `step(action)`: Executes one step in the environment based on the `action` taken by the RL agent. It computes a reward, applies the effects of the action to the state (using `_apply_action_effects`), increments the timestep, and determines if the episode is done.\n",
        "- `_get_state()`: Constructs the current state vector observed by the agent. It gathers the values of the state variables from the `ethics_module` object based on the configuration and normalizes them.\n",
        "- `_compute_reward(action)`: Calculates the reward signal for the current step. The provided implementation is a placeholder that considers the timestep and a simple cost related to the action index. In a more complete system, this would incorporate ethical outcomes, user feedback, advisor input, etc.\n",
        "- `_apply_action_effects(action)`: Updates the state variables in the `ethics_module` object based on the effects defined for the taken action in the configuration.\n",
        "\n",
        "**How to Use:**\n",
        "- Initialize the environment: `env = EthicsSupervisorEnv(mock_ethics_module_instance, config)`. The `mock_ethics_module_instance` should be an object (like `MockEthicsModule`) that holds the current values of the state variables defined in the config.\n",
        "- Call `env.reset()` to start a new episode.\n",
        "- In a training or inference loop, get an action from the RL agent and call `env.step(action)` to advance the simulation. The `step` method returns the next state, reward, and episode status.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **Mock Ethics Module:** The environment directly reads state variable values from and writes updated values to an instance of `MockEthicsModule` (or a similar object representing the system state).\n",
        "- **Configuration Manager:** The environment's fundamental structure (state and action spaces, action effects) is determined by the configuration loaded by `RLConfigManager`.\n",
        "- **PPO Supervisor (RL Agent):** The `EthicsSupervisorRL` class interacts with the `EthicsSupervisorEnv` to train the PPO model by calling `reset()` and `step()` and receiving state and reward information.\n",
        "- **Data Preprocessor:** While not directly used by the environment during a `step`, the state variables and reward structure defined in the configuration used by the environment are consistent with what the `DataPreprocessor` expects when processing raw logs.\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "- The environment's state is represented as a NumPy array (`observation_space`).\n",
        "- Actions are discrete integers (`action_space`).\n",
        "- It uses the `state_variables`, `actions`, and `action_effects` dictionaries from the RL configuration to define its behavior.\n",
        "- The `_compute_reward` method is a placeholder and would ideally use the `reward_config` from the configuration and events from the `ethics_module` state."
      ],
      "metadata": {
        "id": "nWuTD8KREBAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 4: RL ENVIRONMENT (Fully Dynamic Gym-Compatible Environment)\n",
        "# ===========================================================\n",
        "#\n",
        "# This module defines the RL environment the PPO agent interacts with.\n",
        "#\n",
        "# - Fully config-driven:\n",
        "#   - The state space (variables used)\n",
        "#   - The action space (interventions)\n",
        "#   - The reward function\n",
        "#   - The state update effects per action\n",
        "#\n",
        "# -----------------------------------------------------------\n",
        "# ✅ WHY FULLY DYNAMIC STATE?\n",
        "# -----------------------------------------------------------\n",
        "# - Allows easy expansion of system complexity.\n",
        "# - Developers can add new state features via config without touching any code.\n",
        "# - Keeps RL model compatible with evolving assistant behavior.\n",
        "#\n",
        "# -----------------------------------------------------------\n",
        "# ✅ KEY CONCEPTS (NOW FULLY DYNAMIC):\n",
        "# -----------------------------------------------------------\n",
        "# - State Variables: loaded from `state_variables` in config\n",
        "# - Action Effects: loaded from `action_effects` in config\n",
        "# - Observation Space: dynamically computed based on config\n",
        "# ===========================================================\n",
        "\n",
        "class EthicsSupervisorEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Fully dynamic Gym-compatible RL environment for the Thesis Assistant Ethics Supervisor.\n",
        "\n",
        "    This environment simulates the state of a student's thesis progress and ethical\n",
        "    interactions, allowing an RL agent to learn intervention policies. The environment's\n",
        "    structure (state space, action space, rewards, and transitions) is dynamically\n",
        "    defined by the provided configuration dictionary.\n",
        "\n",
        "    Attributes:\n",
        "        ethics_module (MockEthicsModule): A simulated or actual system state object\n",
        "                                           that holds the current values of the state variables.\n",
        "        config (dict): The loaded RL configuration dictionary.\n",
        "        state_variables (list): List of state variable names defined in the config.\n",
        "        actions (list): List of available action labels defined in the config.\n",
        "        action_effects (dict): Dictionary specifying how actions affect state variables (from config).\n",
        "        observation_space (gym.spaces.Box): Dynamically computed continuous state space.\n",
        "        action_space (gym.spaces.Discrete): Discrete action space based on the number of actions.\n",
        "        timestep (int): The current simulation step count within an episode.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ethics_module, config):\n",
        "        \"\"\"\n",
        "        Initialize the environment.\n",
        "\n",
        "        Args:\n",
        "            ethics_module (MockEthicsModule): An external system state simulator\n",
        "                                               (or actual system interface).\n",
        "            config (dict): The full RL configuration dictionary.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.ethics_module = ethics_module\n",
        "        self.config = config\n",
        "\n",
        "        self.state_variables = config[\"state_variables\"]\n",
        "        self.actions = config[\"actions\"]\n",
        "        self.action_effects = config.get(\"action_effects\", {})\n",
        "\n",
        "        # Fully dynamic observation space size based on the number of state variables\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=1, shape=(len(self.state_variables),), dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Discrete action space based on the number of defined actions\n",
        "        self.action_space = gym.spaces.Discrete(len(self.actions))\n",
        "        self.timestep = 0\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        \"\"\"\n",
        "        Reset the environment at the start of a new episode.\n",
        "\n",
        "        Args:\n",
        "            seed (int, optional): Seed for random number generation. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the initial state and an info dictionary.\n",
        "                   (state (np.array), info (dict))\n",
        "        \"\"\"\n",
        "        super().reset(seed=seed) # Call the parent reset method\n",
        "        self.timestep = 0\n",
        "        # Reset the ethics module state for a new episode (assuming ethics_module has a reset method)\n",
        "        if hasattr(self.ethics_module, 'reset'):\n",
        "             self.ethics_module.reset()\n",
        "        return self._get_state(), {} # Return the initial state and an empty info dict\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute one interaction step in the environment.\n",
        "\n",
        "        The agent selects an action, which may affect the environment's state.\n",
        "        A reward is computed, and the environment transitions to the next state.\n",
        "\n",
        "        Args:\n",
        "            action (int): The action index selected by the RL agent.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the next state, the reward, a 'done' flag,\n",
        "                   a 'truncated' flag, and an info dictionary.\n",
        "                   (state (np.array), reward (float), done (bool), truncated (bool), info (dict))\n",
        "        \"\"\"\n",
        "        # Compute the reward for the chosen action\n",
        "        reward = self._compute_reward(action)\n",
        "        # Apply the effects of the action to the environment's state\n",
        "        self._apply_action_effects(action)\n",
        "        self.timestep += 1\n",
        "        # Determine if the episode is finished (e.g., based on timestep limit)\n",
        "        done = (self.timestep >= 100) # Example termination condition\n",
        "        return self._get_state(), reward, done, False, {} # truncated is False, info is empty dict\n",
        "\n",
        "    def _get_state(self):\n",
        "        \"\"\"\n",
        "        Construct the normalized state vector fully dynamically.\n",
        "\n",
        "        This method gathers the current values of the state variables from the\n",
        "        `ethics_module` object based on the configuration and formats them\n",
        "        into a normalized NumPy array.\n",
        "\n",
        "        Returns:\n",
        "            np.array: The normalized state vector based on config-defined variables.\n",
        "        \"\"\"\n",
        "        state = []\n",
        "        # Iterate through the state variables defined in the configuration\n",
        "        for var in self.state_variables:\n",
        "            if var == \"timestep\":\n",
        "                # Normalize timestep (assuming maximum 100 steps for normalization)\n",
        "                state.append(self.timestep / 100.0)\n",
        "            else:\n",
        "                # Get the value from the ethics_module object, defaulting to 0.0 if the attribute doesn't exist\n",
        "                value = getattr(self.ethics_module, var, 0.0)\n",
        "                state.append(value)\n",
        "        return np.array(state, dtype=np.float32) # Ensure float32 dtype for compatibility with RL libraries\n",
        "\n",
        "    def _compute_reward(self, action):\n",
        "        \"\"\"\n",
        "        Compute the reward for the chosen action.\n",
        "\n",
        "        This is a placeholder reward function. In a real system, this would\n",
        "        be more complex, potentially incorporating feedback from the user,\n",
        "        advisor, and ethical violation signals.\n",
        "\n",
        "        Args:\n",
        "            action (int): The selected action index.\n",
        "\n",
        "        Returns:\n",
        "            float: The computed reward value.\n",
        "        \"\"\"\n",
        "        base_reward = 1.0\n",
        "        # Scale reward based on timestep (encouraging progress over time)\n",
        "        time_scaling = 1 + 2.0 * min(1.0, self.timestep / 80.0)\n",
        "        # Simple API cost based on action index (assuming higher indices are more \"costly\" actions)\n",
        "        api_cost = 0.002 * action\n",
        "        lambda_cost = 10 # Weight for the API cost\n",
        "        return base_reward * time_scaling - lambda_cost * api_cost\n",
        "\n",
        "    def _apply_action_effects(self, action):\n",
        "        \"\"\"\n",
        "        Dynamically apply the effects of the selected action to the system state.\n",
        "\n",
        "        Based on the 'action_effects' defined in the configuration, this method\n",
        "        updates the corresponding state variables in the `ethics_module` object.\n",
        "\n",
        "        Args:\n",
        "            action (int): The selected action index.\n",
        "        \"\"\"\n",
        "        # Get the effects defined for the chosen action (if any)\n",
        "        effects = self.action_effects.get(str(action), {})\n",
        "        # Iterate through the variables and their corresponding delta changes for this action\n",
        "        for variable, delta in effects.items():\n",
        "            current_value = getattr(self.ethics_module, variable, None)\n",
        "            if current_value is not None:\n",
        "                # Update the state variable, clipping the value between 0.0 and 1.0\n",
        "                updated_value = np.clip(current_value + delta, 0.0, 1.0)\n",
        "                setattr(self.ethics_module, variable, updated_value)\n"
      ],
      "metadata": {
        "id": "M4UvshTCIBv3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "055bb835"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because I am still incorrectly using `code_block` for markdown content. I will create a markdown cell to document the PPO Supervisor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: PPO Supervisor (EthicsSupervisorRL)\n",
        "\n",
        "The `EthicsSupervisorRL` class is responsible for managing the Proximal Policy Optimization (PPO) agent, which serves as the core Reinforcement Learning component of the ethics module. This class handles the initialization, training, saving, loading, and action recommendation (inference) for the PPO model.\n",
        "\n",
        "**Purpose:**\n",
        "To implement and control the RL agent that learns an optimal policy for intervening in the thesis writing process to promote ethical behavior and positive outcomes. It trains the agent by interacting with the `EthicsSupervisorEnv` and provides action recommendations based on the current state.\n",
        "\n",
        "**Key Components:**\n",
        "- `__init__(config, model_path=\"ethics_rl_model\")`: Initializes the PPO supervisor. It creates an instance of the `MockEthicsModule` (to represent the system state), initializes the `EthicsSupervisorEnv` using the provided configuration, and either loads a pre-trained PPO model from `model_path` or initializes a new PPO model if no saved model is found.\n",
        "- `train(timesteps=50000)`: Trains the PPO model for a specified number of environment interaction steps. It calls the `learn()` method of the Stable Baselines3 PPO model, which handles the data collection (interacting with the environment), policy optimization, and value function updates. After training, it saves the updated model.\n",
        "- `recommend_action()`: Takes the current state from the environment, feeds it to the trained PPO model's policy, and returns the recommended action index. This is the method used during online operation to get the agent's decision.\n",
        "\n",
        "**How to Use:**\n",
        "- Initialize the supervisor: `rl_supervisor = EthicsSupervisorRL(config, model_path=\"my_model\")`. This will either load an existing model or create a new one.\n",
        "- To train the model, call `rl_supervisor.train(timesteps=100000)`.\n",
        "- To get an action recommendation based on the current environment state, call `action = rl_supervisor.recommend_action()`.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **Configuration Manager:** The supervisor uses the configuration (loaded indirectly via the environment initialization) to define the RL problem (state/action spaces).\n",
        "- **RL Environment:** The supervisor interacts directly with the `EthicsSupervisorEnv` during training (calling `env.step()`) and inference (getting the state via `env._get_state()`).\n",
        "- **Mock Ethics Module:** The supervisor initializes an instance of `MockEthicsModule` and passes it to the environment. The environment then uses this object to manage the state.\n",
        "- **RL Training Loop:** The `RLTrainingLoop` uses the `EthicsSupervisorRL` instance to perform the actual training (`trainer.train()`) and potentially get action recommendations (`trainer.recommend_action()`) within its training orchestration.\n",
        "- **Synthetic Pretrainer:** The `SyntheticRLPretrainer` uses the `RLTrainingLoop`, which in turn uses `EthicsSupervisorRL`, to train the model on synthetic data.\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "- It manages a `stable_baselines3.PPO` model.\n",
        "- It interacts with the environment using NumPy arrays for states and integers for actions.\n",
        "- The PPO model's policy and value function are learned from the experience collected by interacting with the `EthicsSupervisorEnv`, which is configured using the dictionary loaded by `RLConfigManager`.\n",
        "\n",
        "\n",
        "===========================================================\n",
        "\n",
        "APPENDIX — PPO SUITABILITY ANALYSIS FOR THESIS ASSISTANT\n",
        "\n",
        "===========================================================\n",
        "\n",
        "Analysis: Strengths and Limitations of PPO for Thesis Assistant RL System\n",
        "\n",
        "✅ PROS (Why PPO is suitable globally):\n",
        "\n",
        "Stable policy optimization even in high-dimensional state spaces.\n",
        "\n",
        "Supports multiple complex actions (advisory interventions, ethical warnings, etc).\n",
        "\n",
        "Optimizes long-term reward (handles delayed ethical consequences).\n",
        "\n",
        "Clipping mechanism stabilizes policy updates (critical for safe ethical behavior).\n",
        "\n",
        "Can be trained globally across many users for a general ethical baseline.\n",
        "\n",
        "\n",
        "⚠ CONS (Limitations for personalization scenario):\n",
        "\n",
        "Requires many training samples to fully converge (sample inefficient).\n",
        "\n",
        "Slow adaptation when applied directly to new individual students.\n",
        "\n",
        "May not personalize fast enough during limited thesis timeframe (6-12 months).\n",
        "\n",
        "Potential difficulty adapting to individual personality shifts quickly.\n",
        "\n",
        "PPO only indirectly receives feedback via reward — few-shot adaptation is hard.\n",
        "\n",
        "\n",
        "✅ RECOMMENDED STRATEGY:\n",
        "\n",
        "Use PPO for global pretraining across many students (shared ethical policy).\n",
        "\n",
        "Introduce a lightweight per-student adaptation layer (small fine-tuning component).\n",
        "\n",
        "Combine PPO with human-in-the-loop reward shaping for rapid personalization.\n",
        "\n",
        "Consider hybrid architecture with PPO + bandits or meta-RL elements for few-shot adjustments.\n",
        "\n",
        "\n",
        "This hybrid design balances PPO’s global stability with efficient short-term personalization needs of the thesis assistant. \"\"\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QrXdtqPRETV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# PART 5: PPO SUPERVISOR (Reinforcement Learning Agent Controller)\n",
        "# ===========================================================\n",
        "#\n",
        "# This module manages the PPO RL agent training, saving, loading, and inference.\n",
        "#\n",
        "# - Clean separation of agent control logic from environment definition.\n",
        "# - Compatible with stable-baselines3 PPO implementation.\n",
        "# - Supports continual training and model persistence.\n",
        "#\n",
        "# -----------------------------------------------------------\n",
        "# ✅ KEY CONCEPTS:\n",
        "# -----------------------------------------------------------\n",
        "# - PPO (Proximal Policy Optimization): modern stable RL algorithm\n",
        "# - Continual training: keep refining policy incrementally\n",
        "# - Safe reloading: easily resume training from saved checkpoints\n",
        "# ===========================================================\n",
        "\n",
        "class MockEthicsModule:\n",
        "    \"\"\"\n",
        "    A mock class simulating the state of the ethics module and thesis progress.\n",
        "\n",
        "    This class is used by the RL environment to represent the system state.\n",
        "    It includes attributes that correspond to the state variables defined\n",
        "    in the RL configuration.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the mock ethics module with random initial states.\n",
        "        \"\"\"\n",
        "        # Initialize attributes that the environment might try to access\n",
        "        self.embedding_drift = np.random.rand()\n",
        "        self.ai_usage = np.random.rand()\n",
        "        self.ethical_flags = np.random.rand()\n",
        "        self.advisor_feedback = np.random.rand()\n",
        "        self.deadline_ratio = np.random.rand() # Represents progress towards deadline (0.0 to 1.0)\n",
        "        # Add attributes required by DataPreprocessor.extract_state (used for reward calculation)\n",
        "        self.user_revised = random.choice([True, False])\n",
        "        self.ai_violation = random.choice([True, False])\n",
        "        self.advisor_positive = random.choice([True, False])\n",
        "        self.rewrite_accepted = random.choice([True, False])\n",
        "        self.milestone_completed = random.choice([True, False])\n",
        "        self.hallucination_detected = random.choice([True, False])\n",
        "        self.prompt = \"mock prompt\" # Placeholder for prompt text\n",
        "        self.intent = \"mock intent\" # Placeholder for intent\n",
        "        self.thesis_stage = \"mock stage\" # Placeholder for thesis stage\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the mock ethics module state for a new episode.\n",
        "        \"\"\"\n",
        "        self.embedding_drift = np.random.rand()\n",
        "        self.ai_usage = np.random.rand()\n",
        "        self.ethical_flags = np.random.rand()\n",
        "        self.advisor_feedback = np.random.rand()\n",
        "        self.deadline_ratio = np.random.rand()\n",
        "        self.user_revised = random.choice([True, False])\n",
        "        self.ai_violation = random.choice([True, False])\n",
        "        self.advisor_positive = random.choice([True, False])\n",
        "        self.rewrite_accepted = random.choice([True, False])\n",
        "        self.milestone_completed = random.choice([True, False])\n",
        "        self.hallucination_detected = random.choice([True, False])\n",
        "        self.prompt = \"mock prompt\"\n",
        "        self.intent = \"mock intent\"\n",
        "        self.thesis_stage = \"mock stage\"\n",
        "\n",
        "\n",
        "class EthicsSupervisorRL:\n",
        "    \"\"\"\n",
        "    PPO Supervisor class controlling RL training and inference for the Ethics Supervisor.\n",
        "\n",
        "    This class initializes, trains, saves, and loads the PPO model that acts\n",
        "     as the RL agent for the ethics module. It interacts with the\n",
        "    `EthicsSupervisorEnv` to learn the optimal intervention policy.\n",
        "\n",
        "    Attributes:\n",
        "        ethics_module (MockEthicsModule): Simulated system state object.\n",
        "        env (EthicsSupervisorEnv): The RL environment instance.\n",
        "        model (PPO): The Stable Baselines3 PPO policy model.\n",
        "        model_path (str): The file path for saving and loading the PPO model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, model_path=\"ethics_rl_model\"):\n",
        "        \"\"\"\n",
        "        Initialize the PPO agent.\n",
        "\n",
        "        Args:\n",
        "            config (dict): The RL configuration dictionary.\n",
        "            model_path (str): The storage path for saving/loading the PPO model.\n",
        "                              Defaults to \"ethics_rl_model\".\n",
        "        \"\"\"\n",
        "\n",
        "        self.ethics_module = MockEthicsModule() # Initialize the mock ethics module\n",
        "        self.env = EthicsSupervisorEnv(self.ethics_module, config) # Initialize the RL environment\n",
        "        self.model_path = model_path\n",
        "\n",
        "        # Load an existing model if available, otherwise initialize a new one\n",
        "        if os.path.exists(model_path + \".zip\"):\n",
        "            self.model = PPO.load(model_path, env=self.env)\n",
        "            print(\"Loaded pretrained RL model.\")\n",
        "        else:\n",
        "            # Initialize a new PPO model with an MlpPolicy\n",
        "            self.model = PPO(\"MlpPolicy\", self.env, verbose=0) # verbose=0 to suppress training output\n",
        "            print(\"Initialized new PPO model.\")\n",
        "\n",
        "    def train(self, timesteps=50000):\n",
        "        \"\"\"\n",
        "        Train the PPO model for a specified number of timesteps.\n",
        "\n",
        "        The model interacts with the environment to collect experience and update\n",
        "        its policy.\n",
        "\n",
        "        Args:\n",
        "            timesteps (int): The total number of environment steps to train for.\n",
        "                             Defaults to 50000.\n",
        "        \"\"\"\n",
        "        self.model.learn(total_timesteps=timesteps)\n",
        "        self.model.save(self.model_path) # Save the model after training\n",
        "        print(\"Training complete and model saved.\")\n",
        "\n",
        "    def recommend_action(self):\n",
        "        \"\"\"\n",
        "        Predict the next action based on the current state of the environment.\n",
        "\n",
        "        This method uses the trained PPO policy to select an action given the\n",
        "        current observation from the environment.\n",
        "\n",
        "        Returns:\n",
        "            int: The action index selected by the PPO policy.\n",
        "        \"\"\"\n",
        "        state = self.env._get_state() # Get the current state from the environment\n",
        "        # Predict the action using the trained model in deterministic mode\n",
        "        action, _ = self.model.predict(state, deterministic=True)\n",
        "        # The action is returned as a NumPy array, so extract the scalar value\n",
        "        return int(action)\n",
        "\n"
      ],
      "metadata": {
        "id": "joWEdcaAICiM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Continual RL Training Loop\n",
        "\n",
        "The `RLTrainingLoop` class is designed to orchestrate the process of continually training the Reinforcement Learning agent using batches of usage logs. It acts as a bridge between the raw data (logs) and the RL training process managed by the `EthicsSupervisorRL`.\n",
        "\n",
        "**Purpose:**\n",
        "To facilitate the training of the RL agent using collected usage data. It takes batches of logs, processes them into states and rewards using the `DataPreprocessor`, and then triggers the training of the PPO agent managed by the `EthicsSupervisorRL`. This allows for updating the agent's policy as new data becomes available.\n",
        "\n",
        "**Key Components:**\n",
        "- `__init__(config, model_path=\"ppo_ethics_model\")`: Initializes the training loop. It loads the configuration, initializes a `DataPreprocessor` instance, and initializes an `EthicsSupervisorRL` instance (which handles the PPO model).\n",
        "- `run_training_day(log_batch)`: The main method for processing a batch of logs. It iterates through each log entry in the `log_batch`, uses the `DataPreprocessor` to extract the state and compute the reward, prints this information, and then calls the `train()` method of the `EthicsSupervisorRL` instance to update the PPO model using the processed data. It also demonstrates getting a recommended action after training.\n",
        "\n",
        "**How to Use:**\n",
        "- Initialize the training loop: `training_loop = RLTrainingLoop(config)`.\n",
        "- Provide a batch of usage logs (a list of dictionaries) to the `run_training_day()` method: `training_loop.run_training_day(my_log_batch)`.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **Configuration Manager:** The training loop loads the configuration via `RLConfigManager` during its initialization.\n",
        "- **Data Preprocessor:** It uses the `DataPreprocessor` instance to convert raw log entries into state vectors and reward values.\n",
        "- **PPO Supervisor (RL Agent):** It uses the `EthicsSupervisorRL` instance to perform the actual training of the PPO model using the processed log data.\n",
        "- **Synthetic Pretrainer and RL Training Launcher:** These classes utilize the `RLTrainingLoop` to execute the training process, providing it with batches of logs (either synthetic or real).\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "- It takes a list of dictionaries (log entries) as input for training.\n",
        "- It uses the `DataPreprocessor` to work with state vectors (NumPy arrays) and reward values (floats).\n",
        "- The training process itself is managed by the `EthicsSupervisorRL` class, which interacts with the `EthicsSupervisorEnv` based on the configuration."
      ],
      "metadata": {
        "id": "WUK7NhspIN8Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "559dcaef"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because I am still incorrectly using `code_block` for markdown content. I will create a markdown cell to document the Continual RL Training Loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iiNne3wXIDt-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7jIbaC1_aCt"
      },
      "source": [
        "\n",
        "# PART 6 — CONTINUAL RL TRAINING LOOP\n",
        "# ===========================================================\n",
        "\n",
        "class RLTrainingLoop:\n",
        "    \"\"\"\n",
        "    Orchestrates the continual RL training process using batches of usage logs.\n",
        "\n",
        "    This class manages the flow of data from usage logs to the RL training process.\n",
        "    It uses the `DataPreprocessor` to convert logs into states and rewards and\n",
        "    the `EthicsSupervisorRL` to train the PPO agent.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, model_path=\"ppo_ethics_model\"):\n",
        "        \"\"\"\n",
        "        Initialize training loop components.\n",
        "\n",
        "        Args:\n",
        "            config (dict): The loaded RL configuration.\n",
        "            model_path (str): The path to the PPO model storage. Defaults to \"ppo_ethics_model\".\n",
        "        \"\"\"\n",
        "        self.config = RLConfigManager.load_config() # Load configuration\n",
        "        self.preprocessor = DataPreprocessor(self.config) # Initialize data preprocessor\n",
        "        # Initialize the RL trainer, passing the config and model path\n",
        "        self.trainer = EthicsSupervisorRL(self.config, model_path)\n",
        "\n",
        "    def run_training_day(self, log_batch):\n",
        "        \"\"\"\n",
        "        Process one batch of usage logs and train the PPO agent.\n",
        "\n",
        "        This method iterates through the provided log batch, processes each log\n",
        "        into a state and reward, and then uses these to train the RL agent.\n",
        "        Note: In a true online RL setting, training would occur more frequently\n",
        "        and interactively with the environment. This simulates batch training\n",
        "        from collected logs.\n",
        "\n",
        "        Args:\n",
        "            log_batch (list of dict): A list of usage logs for one training day or batch.\n",
        "        \"\"\"\n",
        "        print(f\"Processing batch of {len(log_batch)} logs for training...\")\n",
        "        # In a real RL loop, you would accumulate experiences (state, action, reward, next_state, done)\n",
        "        # from interacting with the environment, and then train the agent on these experiences.\n",
        "        # For this simulated training loop, we will process each log entry and call trainer.train\n",
        "        # with a small number of timesteps based on the batch size. This is a simplification\n",
        "        # and not a standard RL training loop, but demonstrates the integration points.\n",
        "        for log_entry in log_batch:\n",
        "            state_vector = self.preprocessor.extract_state(log_entry)\n",
        "            reward = self.preprocessor.compute_reward(log_entry)\n",
        "            # In a real scenario, you would step the environment with an action and get the next state and reward\n",
        "            # For this simulated training loop, we'll just print the processed info\n",
        "            print(f\"Processed Log → State: {state_vector}, Reward: {reward}\")\n",
        "\n",
        "        # The training method in EthicsSupervisorRL is named 'train'\n",
        "        # In a real RL loop, you would accumulate experiences and then train\n",
        "        # For this simulation, we'll call train after processing the batch, using the batch size as timesteps\n",
        "        # A more realistic approach would involve running episodes in the environment\n",
        "        # and training on the collected trajectories.\n",
        "        self.trainer.train(timesteps=len(log_batch)) # Train for the number of logs in the batch\n",
        "        # The prediction method in EthicsSupervisorRL is named 'recommend_action'\n",
        "        # This is just an example of how to use the trained model after training the batch\n",
        "        action = self.trainer.recommend_action()\n",
        "        print(f\"Recommended action after training on batch: {action}\")\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 7: Synthetic Thesis Student Simulator (ThesisStudentSimulator)\n",
        "\n",
        "The `ThesisStudentSimulator` class provides a way to generate synthetic data that mimics the progression and ethical interactions of a thesis student using the assistant. This simulator is essential for generating datasets to pre-train and test the Reinforcement Learning ethics supervisor in a controlled environment.\n",
        "\n",
        "**Purpose:**\n",
        "To create realistic (though simplified) sequences of events and state changes that a thesis student might experience over the course of their project. This synthetic data includes simulated metrics like AI usage, ethical flags, advisor feedback, and progress towards the deadline, which are used to train the RL agent.\n",
        "\n",
        "**Key Components:**\n",
        "- `__init__(student_type=\"Stable Performer\")`: Initializes the simulator for a specific type of student (e.g., \"Conservative\", \"Aggressive\", \"Struggling\", \"Stable Performer\"). Each student type has different tendencies influencing how their state variables evolve.\n",
        "- `evolve_one_step()`: Simulates one step in the student's thesis journey. It updates the state variables based on the student type and progress towards the deadline and generates a dictionary representing a single log entry for this step.\n",
        "- `grade_final_outcome(last_state)`: A static method that calculates a final grade and reward for a simulated thesis trajectory based on the state of the simulator at the end of the trajectory. This provides a terminal reward signal for the simulation.\n",
        "- `generate_full_trajectory_with_grading(student_type, trajectory_length=30)`: A static method that runs a full simulation trajectory for a specified student type and length, collecting all log entries and returning the list of logs along with the final grade and terminal reward.\n",
        "\n",
        "**How to Use:**\n",
        "- To simulate a single student's journey, create an instance: `student_sim = ThesisStudentSimulator(\"Struggling\")` and repeatedly call `student_sim.evolve_one_step()` to get step-by-step logs.\n",
        "- To generate a complete trajectory and its grading, call the static method: `logs, grade, reward = ThesisStudentSimulator.generate_full_trajectory_with_grading(\"Aggressive\", trajectory_length=50)`.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **Data Preprocessor:** The `evolve_one_step()` method generates log entries in a dictionary format that is compatible with the input expected by the `DataPreprocessor`.\n",
        "- **Thesis Cohort Simulator:** The `ThesisCohortSimulator` uses the `generate_full_trajectory_with_grading` method to create datasets for multiple students.\n",
        "- **Synthetic RL Pretrainer:** The `SyntheticRLPretrainer` utilizes the data generated by the simulator (via the `ThesisCohortSimulator`) to train the RL agent.\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "- The simulator maintains internal state variables (e.g., `ai_usage`, `ethical_flags`) as numerical values, typically floats between 0.0 and 1.0.\n",
        "- It generates output as dictionaries, where each dictionary represents a single usage log entry containing the state variables and simulated event flags.\n",
        "- The behavior is influenced by the `student_type` string."
      ],
      "metadata": {
        "id": "S_v_cF3uITia"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhLUsrnO_dDW"
      },
      "source": [
        "# PART 7 — SYNTHETIC THESIS STUDENT SIMULATOR WITH OUTCOME GRADING\n",
        "# ===========================================================\n",
        "\n",
        "class ThesisStudentSimulator:\n",
        "    \"\"\"\n",
        "    Simulates the progress and ethical behavior of a synthetic thesis student.\n",
        "\n",
        "    This class generates synthetic usage logs and simulates changes in state\n",
        "    variables (like AI usage, ethical flags, etc.) over time, based on\n",
        "    predefined student types. It also includes a method to grade the final\n",
        "    outcome of a simulated thesis trajectory.\n",
        "    \"\"\"\n",
        "    def __init__(self, student_type=\"Stable Performer\"):\n",
        "        \"\"\"\n",
        "        Initialize the simulator for a specific type of student.\n",
        "\n",
        "        Args:\n",
        "            student_type (str): The type of student to simulate\n",
        "                                 (\"Conservative\", \"Aggressive\", \"Struggling\",\n",
        "                                  \"Stable Performer\"). Defaults to \"Stable Performer\".\n",
        "        \"\"\"\n",
        "        self.student_type = student_type\n",
        "        # Initialize state variables\n",
        "        self.embedding_drift = 0.2\n",
        "        self.ai_usage = 0.3\n",
        "        self.ethical_flags = 0.05\n",
        "        self.advisor_feedback = 0.6\n",
        "        self.deadline_ratio = 0.0 # Represents progress towards deadline (0.0 to 1.0)\n",
        "\n",
        "    def evolve_one_step(self):\n",
        "        \"\"\"\n",
        "        Simulate one step of the student's thesis progress and generate a log entry.\n",
        "\n",
        "        State variables evolve based on the student type and time progression.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary representing a single usage log entry with updated state.\n",
        "        \"\"\"\n",
        "        # Simulate progress towards the deadline\n",
        "        self.deadline_ratio = min(self.deadline_ratio + 0.03, 1.0) # Increment deadline ratio\n",
        "\n",
        "        # Simulate changes in state variables based on student type\n",
        "        if self.student_type == \"Conservative\":\n",
        "            self.ai_usage += np.random.normal(0.01, 0.02)\n",
        "            self.ethical_flags += np.random.normal(0.0, 0.01)\n",
        "            self.advisor_feedback += np.random.normal(0.02, 0.05)\n",
        "            self.embedding_drift += np.random.normal(0.01, 0.02)\n",
        "        elif self.student_type == \"Aggressive\":\n",
        "            self.ai_usage += np.random.normal(0.05, 0.05)\n",
        "            self.ethical_flags += np.random.normal(0.02, 0.03)\n",
        "            self.advisor_feedback += np.random.normal(-0.02, 0.05)\n",
        "            self.embedding_drift += np.random.normal(0.03, 0.05)\n",
        "        elif self.student_type == \"Struggling\":\n",
        "            self.ai_usage += np.random.normal(0.03, 0.03)\n",
        "            self.ethical_flags += np.random.normal(0.05, 0.05)\n",
        "            self.advisor_feedback += np.random.normal(-0.03, 0.05)\n",
        "            self.embedding_drift += np.random.normal(0.04, 0.05)\n",
        "        elif self.student_type == \"Stable Performer\":\n",
        "            self.ai_usage += np.random.normal(0.02, 0.02)\n",
        "            self.ethical_flags += np.random.normal(0.01, 0.01)\n",
        "            self.advisor_feedback += np.random.normal(0.03, 0.04)\n",
        "            self.embedding_drift += np.random.normal(0.02, 0.02)\n",
        "\n",
        "        # Increase ethical flags towards the end of the project (simulating pressure)\n",
        "        if self.deadline_ratio > 0.8:\n",
        "            self.ethical_flags += 0.02\n",
        "\n",
        "        # Simulate some convergence towards target values as the deadline approaches\n",
        "        convergence_factor = self.deadline_ratio\n",
        "        self.ai_usage += (0.5 - self.ai_usage) * 0.1 * convergence_factor\n",
        "        self.ethical_flags += (0.1 - self.ethical_flags) * 0.1 * convergence_factor\n",
        "        self.advisor_feedback += (0.8 - self.advisor_feedback) * 0.1 * convergence_factor\n",
        "        self.embedding_drift += (0.3 - self.embedding_drift) * 0.05 * convergence_factor\n",
        "\n",
        "        # Clip state variables to remain within the [0, 1] range\n",
        "        self.ai_usage = np.clip(self.ai_usage, 0, 1.0)\n",
        "        self.ethical_flags = np.clip(self.ethical_flags, 0, 1.0)\n",
        "        self.advisor_feedback = np.clip(self.advisor_feedback, 0, 1.0)\n",
        "        self.embedding_drift = np.clip(self.embedding_drift, 0, 1.0)\n",
        "\n",
        "        # Create a log entry with the current state and some simulated events (for reward calculation)\n",
        "        log_entry = {\n",
        "            \"embedding_drift\": self.embedding_drift,\n",
        "            \"ai_usage\": self.ai_usage,\n",
        "            \"ethical_flags\": self.ethical_flags,\n",
        "            \"advisor_feedback\": self.advisor_feedback,\n",
        "            \"deadline_ratio\": self.deadline_ratio,\n",
        "            # Simulate boolean event flags based on probabilities or state\n",
        "            \"user_revised\": random.random() < 0.6, # Probability of user revising content\n",
        "            \"ai_violation\": random.random() < self.ethical_flags, # Higher ethical flags increase chance of violation\n",
        "            \"advisor_positive\": random.random() < self.advisor_feedback, # Higher feedback increases chance of positive advisor event\n",
        "            \"rewrite_accepted\": random.random() < 0.7, # Probability of rewrite suggestion being accepted\n",
        "            \"milestone_completed\": random.random() < 0.4, # Probability of completing a milestone\n",
        "            \"hallucination_detected\": random.random() < 0.1 # Probability of detecting a hallucination\n",
        "        }\n",
        "        return log_entry\n",
        "\n",
        "    @staticmethod\n",
        "    def grade_final_outcome(last_state):\n",
        "        \"\"\"\n",
        "        Grades the final outcome of a simulated thesis based on the last state.\n",
        "\n",
        "        This is a simplified grading function for simulation purposes.\n",
        "\n",
        "        Args:\n",
        "            last_state (dict): The final state of the simulator after a trajectory.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the grade (\"Excellent\", \"Acceptable\", \"Failed\")\n",
        "                   and a corresponding numerical reward.\n",
        "        \"\"\"\n",
        "        # Calculate penalties and bonuses based on the final state values\n",
        "        ai_penalty = (last_state[\"ai_usage\"] - 0.5) * 0.5 # Penalty if AI usage is high relative to 0.5\n",
        "        ethics_penalty = last_state[\"ethical_flags\"] * 1.5 # Penalty for ethical flags\n",
        "        advisor_bonus = last_state[\"advisor_feedback\"] * 2.0 # Bonus for positive advisor feedback\n",
        "        embedding_penalty = last_state[\"embedding_drift\"] * 0.3 # Penalty for high embedding drift\n",
        "        # Calculate total score\n",
        "        total_score = advisor_bonus - ethics_penalty - ai_penalty - embedding_penalty\n",
        "\n",
        "        # Assign grade and reward based on the total score\n",
        "        if total_score > 1.2:\n",
        "            return \"Excellent\", 5.0\n",
        "        elif total_score > 0:\n",
        "            return \"Acceptable\", 2.0\n",
        "        else:\n",
        "            return \"Failed\", -5.0\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_full_trajectory_with_grading(student_type, trajectory_length=30):\n",
        "        \"\"\"\n",
        "        Generates a full simulated thesis trajectory for a student and grades it.\n",
        "\n",
        "        Args:\n",
        "            student_type (str): The type of student to simulate.\n",
        "            trajectory_length (int): The number of steps in the simulation trajectory.\n",
        "                                     Defaults to 30.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                   - logs (list of dict): The list of log entries generated during the trajectory.\n",
        "                   - grade (str): The final grade (\"Excellent\", \"Acceptable\", \"Failed\").\n",
        "                   - reward (float): The terminal reward associated with the final grade.\n",
        "        \"\"\"\n",
        "        student = ThesisStudentSimulator(student_type)\n",
        "        logs = []\n",
        "        # Evolve the student's state for the specified trajectory length\n",
        "        for _ in range(trajectory_length):\n",
        "            logs.append(student.evolve_one_step())\n",
        "        # Grade the final outcome based on the last state in the trajectory\n",
        "        grade, terminal_reward = ThesisStudentSimulator.grade_final_outcome(logs[-1])\n",
        "        return logs, grade, terminal_reward\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 8: Multi-Student Synthetic Cohort Generator (ThesisCohortSimulator)\n",
        "\n",
        "The `ThesisCohortSimulator` class is designed to generate a dataset of simulated thesis trajectories for an entire cohort of diverse synthetic students. This aggregated dataset is crucial for the initial pre-training of the Reinforcement Learning ethics supervisor, providing a broad range of scenarios and behaviors.\n",
        "\n",
        "**Purpose:**\n",
        "To efficiently create a large, varied dataset of synthetic student interactions and outcomes. This dataset is used to train the RL agent to develop a generalizable ethical policy across different student types before any potential per-student fine-tuning.\n",
        "\n",
        "**Key Components:**\n",
        "- `STUDENT_TYPES`: A class attribute list defining the different types of students that can be simulated (\"Conservative\", \"Aggressive\", \"Struggling\", \"Stable Performer\").\n",
        "- `generate_cohort_dataset(num_students=100, trajectory_length=30)`: A static method that is the primary function of this class. It generates the specified number of synthetic students, each with a randomly assigned type, runs a full simulation trajectory for each using the `ThesisStudentSimulator`, and collects all the resulting logs, final grades, and terminal rewards into a single dataset. It also prints a summary of the final grades distribution within the generated cohort.\n",
        "\n",
        "**How to Use:**\n",
        "- To generate a dataset for a cohort of 100 students with trajectories of 30 steps each, simply call the static method: `cohort_data = ThesisCohortSimulator.generate_cohort_dataset(num_students=100, trajectory_length=30)`. The returned `cohort_data` is a list where each element is a dictionary containing the student type, their full trajectory of logs, their final grade, and the terminal reward.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **Thesis Student Simulator:** The `ThesisCohortSimulator` relies heavily on the `ThesisStudentSimulator.generate_full_trajectory_with_grading` static method to produce individual student trajectories and outcomes.\n",
        "- **Synthetic RL Pretrainer:** The `SyntheticRLPretrainer` class uses the `generate_cohort_dataset` method to obtain the large pool of synthetic logs required for pre-training the RL agent. It then flattens the trajectories from this dataset into a single list of logs for the training process.\n",
        "- **RLTrainingLoop:** Although not directly interacted with by this class, the dataset generated here is ultimately fed into the `RLTrainingLoop` by the `SyntheticRLPretrainer`.\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "- The class uses the predefined `STUDENT_TYPES` list.\n",
        "- The output is a list of dictionaries, each representing a simulated student with their complete `trajectory` (a list of log dictionaries), their `grade` (string), and `final_reward` (float).\n"
      ],
      "metadata": {
        "id": "KWUxbeEDIcAf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "5d8623a5",
        "outputId": "a5a65c59-d865-44a1-a5af-dbd2f73ac743"
      },
      "source": [
        "# ===========================================================\n",
        "# PART 8 — MULTI-STUDENT SYNTHETIC COHORT GENERATOR\n",
        "# ===========================================================\n",
        "\n",
        "class ThesisCohortSimulator:\n",
        "    \"\"\"\n",
        "    Generates a dataset of simulated thesis trajectories for a cohort of students.\n",
        "\n",
        "    This class uses the `ThesisStudentSimulator` to create trajectories for\n",
        "    multiple students of different types, providing a dataset for training\n",
        "    and evaluating the RL agent.\n",
        "    \"\"\"\n",
        "    STUDENT_TYPES = [\"Conservative\", \"Aggressive\", \"Struggling\", \"Stable Performer\"]\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_cohort_dataset(num_students=100, trajectory_length=30):\n",
        "        \"\"\"\n",
        "        Generates a dataset of simulated thesis trajectories for a cohort.\n",
        "\n",
        "        Args:\n",
        "            num_students (int): The number of students to simulate. Defaults to 100.\n",
        "            trajectory_length (int): The number of steps in each student's trajectory.\n",
        "                                     Defaults to 30.\n",
        "\n",
        "        Returns:\n",
        "            list of dict: A list of dictionaries, where each dictionary represents\n",
        "                          a student and contains their trajectory, final grade,\n",
        "                          and terminal reward.\n",
        "        \"\"\"\n",
        "        dataset = []\n",
        "        grade_summary = {\"Excellent\": 0, \"Acceptable\": 0, \"Failed\": 0}\n",
        "        # Generate trajectories for the specified number of students\n",
        "        for _ in range(num_students):\n",
        "            # Randomly select a student type\n",
        "            student_type = random.choice(ThesisCohortSimulator.STUDENT_TYPES)\n",
        "            # Generate a full trajectory and grade for the student\n",
        "            logs, grade, terminal_reward = ThesisStudentSimulator.generate_full_trajectory_with_grading(\n",
        "                student_type, trajectory_length)\n",
        "            dataset.append({\n",
        "                \"student_type\": student_type,\n",
        "                \"trajectory\": logs,\n",
        "                \"grade\": grade,\n",
        "                \"final_reward\": terminal_reward\n",
        "            })\n",
        "            grade_summary[grade] += 1 # Count the grades for summary\n",
        "\n",
        "        # Print a summary of the generated cohort grades\n",
        "        print(\"Cohort Generation Complete:\")\n",
        "        for grade, count in grade_summary.items():\n",
        "            print(f\"  {grade}: {count} students\")\n",
        "        return dataset\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-17-649839036.py, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-17-649839036.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    The `ThesisCohortSimulator` class is designed to generate a dataset of simulated thesis trajectories for an entire cohort of diverse synthetic students. This aggregated dataset is crucial for the initial pre-training of the Reinforcement Learning ethics supervisor, providing a broad range of scenarios and behaviors.\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81e3dee2"
      },
      "source": [
        "## Part 9: Synthetic RL Pretraining Pipeline (SyntheticRLPretrainer)\n",
        "\n",
        "The `SyntheticRLPretrainer` class orchestrates the process of pre-training the Reinforcement Learning ethics supervisor using a large synthetic dataset generated by the `ThesisCohortSimulator`. This step is typically done before applying the RL agent to real student data to provide it with an initial understanding of the environment and ethical considerations.\n",
        "\n",
        "**Purpose:**\n",
        "To automate the generation of a comprehensive synthetic dataset and use it to train the `EthicsSupervisorRL` agent, establishing a foundational policy for ethical guidance.\n",
        "\n",
        "**Key Components:**\n",
        "- `__init__(config, model_path=\"ppo_ethics_model\")`: Initializes the pretrainer. It takes the RL configuration and the desired model path as input, and internally initializes an `RLTrainingLoop` instance, which in turn manages the `EthicsSupervisorRL` agent.\n",
        "- `run_synthetic_pretraining(num_students=100, trajectory_length=30)`: The main method to trigger the pretraining process. It first calls the `ThesisCohortSimulator.generate_cohort_dataset` method to get the synthetic data, then flattens the trajectories from all students into a single list of logs, and finally passes this aggregated list of logs to the `RLTrainingLoop.run_training_day` method to train the RL agent.\n",
        "\n",
        "**How to Use:**\n",
        "- To run the synthetic pretraining with default settings (100 students, 30 steps per trajectory), after initializing the `RLTrainingLauncher` (which initializes the `SyntheticRLPretrainer`), you would call `launcher.run_synthetic_full_pretraining()`. If you are using the `SyntheticRLPretrainer` directly, you would initialize it with a config and then call `pretrainer.run_synthetic_pretraining(num_students=200, trajectory_length=40)` to specify different parameters.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **RLConfigManager:** The pretrainer is initialized with the configuration loaded by `RLConfigManager`, which is then passed down to the `RLTrainingLoop` and `EthicsSupervisorRL`.\n",
        "- **ThesisCohortSimulator:** It directly calls the `ThesisCohortSimulator.generate_cohort_dataset` static method to obtain the synthetic training data.\n",
        "- **RLTrainingLoop:** It uses an instance of `RLTrainingLoop` to handle the actual process of feeding logs to the `DataPreprocessor` and training the `EthicsSupervisorRL` agent on this data.\n",
        "- **EthicsSupervisorRL:** The training of the PPO agent is managed by the `RLTrainingLoop` instance held within the pretrainer.\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "- It works with the list of dictionaries returned by the `ThesisCohortSimulator`, processing the `trajectory` lists within that dataset.\n",
        "- The training parameters (like the number of students and trajectory length) are passed as arguments to the `run_synthetic_pretraining` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3mkrlya_k3O"
      },
      "source": [
        "# ===========================================================\n",
        "# PART 9 — SYNTHETIC RL PRETRAINING PIPELINE\n",
        "# ===========================================================\n",
        "\n",
        "class SyntheticRLPretrainer:\n",
        "    \"\"\"\n",
        "    Manages the pretraining of the RL agent using synthetic thesis student data.\n",
        "\n",
        "    This class uses the `ThesisCohortSimulator` to generate a large dataset\n",
        "    of synthetic logs and then trains the RL agent (`EthicsSupervisorRL`)\n",
        "    using this data via the `RLTrainingLoop`.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, model_path=\"ppo_ethics_model\"):\n",
        "        \"\"\"\n",
        "        Initialize the synthetic pretrainer.\n",
        "\n",
        "        Args:\n",
        "            config (dict): The loaded RL configuration.\n",
        "            model_path (str): The path to the PPO model storage. Defaults to \"ppo_ethics_model\".\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        # Initialize the training loop with the config and model path\n",
        "        self.training_loop = RLTrainingLoop(config, model_path)\n",
        "\n",
        "    def run_synthetic_pretraining(self, num_students=100, trajectory_length=30):\n",
        "        \"\"\"\n",
        "        Runs the full synthetic pretraining pipeline.\n",
        "\n",
        "        Generates a synthetic cohort dataset and trains the RL agent on the\n",
        "        collected trajectories.\n",
        "\n",
        "        Args:\n",
        "            num_students (int): The number of synthetic students to generate data for.\n",
        "                                Defaults to 100.\n",
        "            trajectory_length (int): The length of each student's trajectory.\n",
        "                                     Defaults to 30.\n",
        "        \"\"\"\n",
        "        print(\"\\nGenerating synthetic cohort dataset for pretraining...\")\n",
        "        # Generate the synthetic dataset\n",
        "        dataset = ThesisCohortSimulator.generate_cohort_dataset(num_students, trajectory_length)\n",
        "        # Flatten the trajectories from all students into a single list of logs\n",
        "        all_logs = []\n",
        "        for student in dataset:\n",
        "            all_logs.extend(student[\"trajectory\"])\n",
        "\n",
        "        print(f\"Total synthetic logs for PPO training: {len(all_logs)}\")\n",
        "        # Run the training loop on the collected synthetic logs\n",
        "        self.training_loop.run_training_day(all_logs)\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1acaeb77"
      },
      "source": [
        "## Part 10: Final Launcher (RLTrainingLauncher)\n",
        "\n",
        "The `RLTrainingLauncher` class serves as the main entry point and master system launcher for the entire thesis RL assistant training and configuration system. It brings together all the previously defined components and provides different modes of operation for development, simulation, and training with real or synthetic data.\n",
        "\n",
        "**Purpose:**\n",
        "To provide a single interface for initializing the RL system components, launching the developer dashboard, running simulated training, handling real data training, managing online incremental updates, and executing the full synthetic pretraining pipeline.\n",
        "\n",
        "**Key Components:**\n",
        "- `__init__()`: Initializes the launcher by loading the RL configuration using `RLConfigManager`, initializing the `DataPreprocessor`, creating an instance of the `RLTrainingLoop` (which includes the `EthicsSupervisorRL` agent), and initializing the `SyntheticRLPretrainer`.\n",
        "- `launch_dashboard()`: Launches the Streamlit-based `DeveloperDashboard` for interactive configuration of the RL system.\n",
        "- `run_simulated_training(days=3, batch_size=5)`: Runs a step-by-step simulation of training using a `MockSimulator` to generate small batches of logs over several simulated \"days\". This mode is useful for basic testing and debugging of the training loop.\n",
        "- `run_real_training(real_logs)`: Takes a list of actual collected usage logs (`real_logs`) and feeds them into the `RLTrainingLoop` for training the RL agent on real-world data.\n",
        "- `run_online_incremental_training(incremental_logs)`: Designed for online learning scenarios. It takes a list of newly collected `incremental_logs` and uses the `RLTrainingLoop` to fine-tune the existing RL model with this new data.\n",
        "- `run_synthetic_full_pretraining(num_students=100, trajectory_length=30)`: Triggers the full synthetic pretraining pipeline by calling the `run_synthetic_pretraining` method of the `SyntheticRLPretrainer` instance. This generates a large synthetic dataset and trains the RL agent on it.\n",
        "\n",
        "**How to Use:**\n",
        "- Instantiate the launcher: `launcher = RLTrainingLauncher()`.\n",
        "- Select a mode of operation based on user input or script logic:\n",
        "    - `launcher.launch_dashboard()`: To start the configuration dashboard (requires Streamlit).\n",
        "    - `launcher.run_simulated_training(days=5, batch_size=10)`: To run a short simulated training session.\n",
        "    - `launcher.run_real_training(my_real_logs)`: To train with your collected real logs.\n",
        "    - `launcher.run_online_incremental_training(new_logs)`: To perform incremental online updates with new data.\n",
        "    - `launcher.run_synthetic_full_pretraining(num_students=200, trajectory_length=50)`: To run the comprehensive synthetic pretraining.\n",
        "- The `if __name__ == \"__main__\":` block provides a command-line-like interface to select the mode when the script is run directly.\n",
        "\n",
        "**Interaction with Other Components:**\n",
        "- **RLConfigManager:** Used during initialization to load the system configuration.\n",
        "- **DataPreprocessor:** An instance is held and used by the `RLTrainingLoop` for processing logs.\n",
        "- **RLTrainingLoop:** An instance is held and used by the launcher to perform training with both simulated and real/incremental logs.\n",
        "- **SyntheticRLPretrainer:** An instance is held and used to execute the full synthetic pretraining pipeline.\n",
        "- **DeveloperDashboard:** An instance is created and launched when the 'dashboard' mode is selected.\n",
        "- **MockSimulator:** An internal mock class used specifically by `run_simulated_training` to generate synthetic logs for that mode.\n",
        "\n",
        "**Data Structures and Configuration:**\n",
        "- Relies on the dictionary structure of the RL configuration loaded by `RLConfigManager`.\n",
        "- Processes lists of log dictionaries, as generated by the simulators or collected from real usage.\n",
        "- Uses numerical parameters (like `days`, `batch_size`, `num_students`, `trajectory_length`) to control the simulation and training processes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d228b936",
        "outputId": "8320f7c3-2e2d-4878-b5a8-65cc30e888bf"
      },
      "source": [
        "# ===========================================================\n",
        "# PART 10 — FINAL LAUNCHER\n",
        "# ===========================================================\n",
        "\n",
        "class RLTrainingLauncher:\n",
        "    \"\"\"\n",
        "    The main entry point and master system launcher for the thesis RL assistant training.\n",
        "\n",
        "    This class orchestrates the different training modes (dashboard, simulated,\n",
        "    real data, online, synthetic full pretraining) and initializes the\n",
        "    necessary components (`RLConfigManager`, `DataPreprocessor`,\n",
        "    `RLTrainingLoop`, `SyntheticRLPretrainer`).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the launcher by loading configuration and components.\n",
        "        \"\"\"\n",
        "        # Load the RL configuration\n",
        "        self.config = RLConfigManager.load_config()\n",
        "        # Initialize the data preprocessor\n",
        "        self.preprocessor = DataPreprocessor(self.config)\n",
        "        # Initialize the RL training loop (this also initializes the RL agent)\n",
        "        self.training_loop = RLTrainingLoop(self.config)\n",
        "        # Initialize the synthetic pretrainer\n",
        "        self.pretrainer = SyntheticRLPretrainer(self.config)\n",
        "\n",
        "    def launch_dashboard(self):\n",
        "        \"\"\"\n",
        "        Launch the Streamlit-based developer interface for configuring the RL system.\n",
        "        \"\"\"\n",
        "        dashboard = DeveloperDashboard() # Initialize the DeveloperDashboard\n",
        "        dashboard.launch() # Launch the dashboard\n",
        "\n",
        "    def run_simulated_training(self, days=3, batch_size=5):\n",
        "        \"\"\"\n",
        "        Simulate RL model training using synthetic logs generated step-by-step.\n",
        "\n",
        "        Args:\n",
        "            days (int): Number of training days to simulate. Defaults to 3.\n",
        "            batch_size (int): Number of logs to generate per day. Defaults to 5.\n",
        "        \"\"\"\n",
        "        print(\"\\nRunning Simulated Step-by-Step Training...\")\n",
        "        class MockSimulator:\n",
        "            \"\"\"\n",
        "            A mock simulator to generate synthetic log batches for step-by-step training.\n",
        "            \"\"\"\n",
        "            def generate_batch(self, batch_size):\n",
        "                \"\"\"\n",
        "                Generates a batch of mock log entries.\n",
        "\n",
        "                Args:\n",
        "                    batch_size (int): The number of logs to generate in the batch.\n",
        "\n",
        "                Returns:\n",
        "                    list of dict: A list of mock log entries.\n",
        "                \"\"\"\n",
        "                print(\"Generating mock log batch...\")\n",
        "                mock_logs = []\n",
        "                for _ in range(batch_size):\n",
        "                    # Generate placeholder data structured to match DataPreprocessor expectations\n",
        "                    mock_logs.append({\n",
        "                        \"embedding_drift\": np.random.rand(),\n",
        "                        \"ai_usage\": np.random.rand(),\n",
        "                        \"ethical_flags\": np.random.rand(),\n",
        "                        \"advisor_feedback\": np.random.rand(),\n",
        "                        \"deadline_ratio\": np.random.rand(),\n",
        "                        \"user_revised\": random.random() < 0.6,\n",
        "                        \"ai_violation\": random.random() < 0.1,\n",
        "                        \"advisor_positive\": random.random() < 0.8,\n",
        "                        \"rewrite_accepted\": random.random() < 0.7,\n",
        "                        \"milestone_completed\": random.random() < 0.4,\n",
        "                        \"hallucination_detected\": random.random() < 0.1,\n",
        "                        \"prompt\": \"mock prompt\",\n",
        "                        \"intent\": \"mock intent\",\n",
        "                        \"thesis_stage\": \"mock stage\"\n",
        "                    })\n",
        "                return mock_logs\n",
        "\n",
        "        self.simulator = MockSimulator() # Initialize the mock simulator\n",
        "\n",
        "        # Run simulation for the specified number of days\n",
        "        for day in range(days):\n",
        "            print(f\"\\nSimulated Day {day + 1}\")\n",
        "            logs = self.simulator.generate_batch(batch_size=batch_size)\n",
        "            # Run the training loop on the generated batch of logs\n",
        "            self.training_loop.run_training_day(logs)\n",
        "\n",
        "    def run_real_training(self, real_logs):\n",
        "        \"\"\"\n",
        "        Train the RL model using real usage logs.\n",
        "\n",
        "        Args:\n",
        "            real_logs (list of dict): Collected real usage logs to use in training.\n",
        "        \"\"\"\n",
        "        print(\"\\nTraining with Real Logs...\")\n",
        "        # Run the training loop on the provided real logs\n",
        "        self.training_loop.run_training_day(real_logs)\n",
        "\n",
        "    def run_online_incremental_training(self, incremental_logs):\n",
        "        \"\"\"\n",
        "        Run online incremental updates using newly gathered data.\n",
        "\n",
        "        This method simulates receiving new logs incrementally and using them\n",
        "        to fine-tune the already trained RL model.\n",
        "\n",
        "        Args:\n",
        "            incremental_logs (list of dict): New usage logs collected for fine-tuning.\n",
        "        \"\"\"\n",
        "        print(\"\\nIncremental Online Training...\")\n",
        "        # Run the training loop on the incremental logs for fine-tuning\n",
        "        self.training_loop.run_training_day(incremental_logs)\n",
        "\n",
        "    def run_synthetic_full_pretraining(self, num_students=100, trajectory_length=30):\n",
        "        \"\"\"\n",
        "        Runs the full synthetic pretraining pipeline using a cohort simulator.\n",
        "\n",
        "        Args:\n",
        "            num_students (int): The number of synthetic students to generate data for.\n",
        "                                Defaults to 100.\n",
        "            trajectory_length (int): The length of each student's trajectory.\n",
        "                                     Defaults to 30.\n",
        "        \"\"\"\n",
        "        print(\"\\nRunning Full Synthetic PPO Pretraining...\")\n",
        "        # Use the pretrainer component to run the synthetic pretraining\n",
        "        self.pretrainer.run_synthetic_pretraining(num_students, trajectory_length)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    launcher = RLTrainingLauncher() # Initialize the main launcher\n",
        "    print(\"RL Training System Entry Point\")\n",
        "    print(\"Modes: [dashboard] [train_simulated] [train_real] [train_online] [train_synthetic_full]\")\n",
        "    # Use a default mode for automated execution, or keep input() for interactive use\n",
        "    mode = \"train_synthetic_full\" # Set a default mode for testing\n",
        "    # mode = input(\"Mode: \").strip() # Uncomment for interactive mode\n",
        "\n",
        "    # Execute the selected mode\n",
        "    if mode == \"dashboard\":\n",
        "         # DeveloperDashboard is defined in this cell, so it can be launched directly.\n",
        "         # Note: Running Streamlit in a standard Jupyter cell might require specific setup\n",
        "         # or will just print a message indicating how to launch it externally.\n",
        "         launcher.launch_dashboard()\n",
        "    elif mode == \"train_simulated\":\n",
        "        launcher.run_simulated_training()\n",
        "    elif mode == \"train_real\":\n",
        "        print(\"Load your real usage logs into 'real_logs' and call launcher.run_real_training(real_logs)\")\n",
        "        # Example usage (commented out):\n",
        "        # real_logs = [...] # Load your real logs here\n",
        "        # launcher.run_real_training(real_logs)\n",
        "    elif mode == \"train_online\":\n",
        "        print(\"Load new incremental logs into 'incremental_logs' and call launcher.run_online_incremental_training(incremental_logs)\")\n",
        "        # Example usage (commented out):\n",
        "        # incremental_logs = [...] # Load your new logs here\n",
        "        # launcher.run_online_incremental_training(incremental_logs)\n",
        "    elif mode == \"train_synthetic_full\":\n",
        "        launcher.run_synthetic_full_pretraining()\n",
        "    else:\n",
        "        print(\"Invalid mode selected.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained RL model.\n",
            "Loaded pretrained RL model.\n",
            "RL Training System Entry Point\n",
            "Modes: [dashboard] [train_simulated] [train_real] [train_online] [train_synthetic_full]\n",
            "\n",
            "Running Full Synthetic PPO Pretraining...\n",
            "\n",
            "Generating synthetic cohort dataset for pretraining...\n",
            "Cohort Generation Complete:\n",
            "  Excellent: 39 students\n",
            "  Acceptable: 22 students\n",
            "  Failed: 39 students\n",
            "Total synthetic logs for PPO training: 3000\n",
            "Processing batch of 3000 logs for training...\n",
            "Processed Log → State: [0.27322635 0.36155182 0.0954284  0.5354182  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.3586745  0.41065982 0.16034892 0.45685008 0.06      ], Reward: 7.0\n",
            "Processed Log → State: [0.38287598 0.43088248 0.29124442 0.51992434 0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.52129114 0.4669372  0.34268674 0.5472633  0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.5729136  0.49924767 0.42285064 0.46380413 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.6103397  0.5092298  0.5118455  0.38049847 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.5676142  0.59878105 0.49899983 0.38910308 0.21      ], Reward: 0.0\n",
            "Processed Log → State: [0.6716991  0.6216688  0.64162654 0.43758872 0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.69376194 0.636545   0.70452213 0.46641356 0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.70802903 0.7425084  0.6920447  0.43488985 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.69173515 0.79208714 0.73994726 0.49710754 0.33      ], Reward: 7.0\n",
            "Processed Log → State: [0.7071757 0.8107922 0.7967481 0.4644563 0.36     ], Reward: 2.0\n",
            "Processed Log → State: [0.7510601  0.8283203  0.8570011  0.45397845 0.39      ], Reward: 5.0\n",
            "Processed Log → State: [0.7376516  0.8343903  0.83808887 0.36044517 0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.8085646  0.83698815 0.85744286 0.33295307 0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.814297   0.90018564 0.8771251  0.28037724 0.48      ], Reward: 6.0\n",
            "Processed Log → State: [0.8416896  0.90466744 0.87284136 0.30949354 0.51      ], Reward: -2.0\n",
            "Processed Log → State: [0.88556445 0.89809537 0.93253624 0.3746615  0.54      ], Reward: 5.0\n",
            "Processed Log → State: [0.89321977 0.9379786  0.98456377 0.4590451  0.57      ], Reward: 3.0\n",
            "Processed Log → State: [1.        0.9319673 1.        0.5109934 0.6      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.97748685 1.         0.511352   0.63      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.99884    1.         0.60855347 0.66      ], Reward: 8.0\n",
            "Processed Log → State: [1.        0.9565414 0.954947  0.6663955 0.69     ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.95313835 0.9157815  0.62916905 0.72      ], Reward: 6.0\n",
            "Processed Log → State: [0.99964267 0.955162   0.98878795 0.5923733  0.75      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.93041027 0.9636399  0.5668694  0.78      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.9040865  0.9230895  0.62466514 0.81      ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.91119826 0.9292935  0.5522903  0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.9990191 0.914024  0.927242  0.5245982 0.87     ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.914965   0.85907495 0.56543773 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.23212369 0.32427242 0.07546718 0.6256797  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.25425628 0.30859295 0.08028968 0.64800984 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.28171915 0.35331255 0.08985926 0.6368255  0.09      ], Reward: 0.0\n",
            "Processed Log → State: [0.31365156 0.4081918  0.09322912 0.7174371  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.35664964 0.3909669  0.11922695 0.70486027 0.15      ], Reward: 5.0\n",
            "Processed Log → State: [0.38697043 0.44209594 0.12535958 0.7144633  0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.43738252 0.44893512 0.13988118 0.73079324 0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.44789165 0.50070214 0.15625268 0.7580457  0.24      ], Reward: 0.0\n",
            "Processed Log → State: [0.4709217  0.5540383  0.17984454 0.7744227  0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.46366087 0.57310367 0.17656891 0.7956178  0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.4840117  0.5853192  0.18004838 0.8744311  0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.5063027  0.6330341  0.18603332 0.91131955 0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.5117758  0.65613145 0.20108645 0.9312931  0.39      ], Reward: 5.0\n",
            "Processed Log → State: [0.54333484 0.62958384 0.21497224 0.9495012  0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.56034625 0.6256282  0.22413538 0.99757785 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.5868605  0.5838612  0.23546314 1.         0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.6223093  0.5987725  0.24109632 0.9331469  0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.65855944 0.5792366  0.25327498 0.88338834 0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.71828747 0.5770017  0.24165864 0.9173318  0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.6988618  0.5750775  0.24872354 0.9356628  0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.694339   0.55456734 0.24956049 0.958571   0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.72154754 0.56859547 0.24467824 1.         0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.7010334  0.58869624 0.24352369 1.         0.69      ], Reward: 8.0\n",
            "Processed Log → State: [0.69356024 0.59754366 0.242062   1.         0.72      ], Reward: 5.0\n",
            "Processed Log → State: [0.7085807  0.60852385 0.23425765 0.9995412  0.75      ], Reward: 6.0\n",
            "Processed Log → State: [0.70736957 0.6275134  0.22634378 1.         0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.7121991  0.6187131  0.23345885 1.         0.81      ], Reward: 0.0\n",
            "Processed Log → State: [0.7167125  0.61062723 0.23838465 1.         0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.7248357 0.5978444 0.2628308 1.        0.87     ], Reward: 3.0\n",
            "Processed Log → State: [0.7082298  0.61798394 0.2909892  1.         0.9       ], Reward: 6.0\n",
            "Processed Log → State: [0.24791968 0.30254716 0.04858349 0.70712584 0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.26980594 0.31767774 0.06402051 0.74267334 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.3167023  0.34854332 0.07827473 0.78141445 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.34054556 0.40491393 0.08933679 0.77364194 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.3413309  0.4244119  0.09708808 0.7960646  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.32250333 0.45329785 0.11775398 0.8115139  0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.32608044 0.46939808 0.13889892 0.8573949  0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.32873002 0.47834194 0.1509298  0.86861265 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.33724776 0.5026463  0.15927413 0.8472966  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.36254224 0.5444444  0.17196211 0.909447   0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.36430502 0.58402956 0.17526913 0.97736555 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.37419534 0.61984426 0.18677749 1.         0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.41892982 0.6354188  0.17606547 0.9823892  0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.42193627 0.6473475  0.19373569 0.9973341  0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.4129056  0.67146045 0.20467323 0.9648981  0.45      ], Reward: 5.0\n",
            "Processed Log → State: [0.41654626 0.67506146 0.22063985 0.97148365 0.48      ], Reward: 2.0\n",
            "Processed Log → State: [0.42334318 0.698444   0.23573704 1.         0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.4559067  0.71360594 0.24089499 0.9332965  0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.4516639  0.71463746 0.234954   0.9091674  0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.45966947 0.68438894 0.24209905 0.94229275 0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.46344113 0.6907957  0.2426594  0.95572054 0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.5161158  0.6976182  0.23993589 0.9732563  0.66      ], Reward: 0.0\n",
            "Processed Log → State: [0.5283385  0.7014185  0.22467685 0.9894924  0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.53325254 0.6862005  0.2231191  0.9836901  0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.5425612  0.6845649  0.21347901 1.         0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.5303177  0.65608674 0.20922226 1.         0.78      ], Reward: 7.0\n",
            "Processed Log → State: [0.5142626  0.6440807  0.23531108 1.         0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.5435451  0.65558165 0.25844526 1.         0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.5468479 0.6427508 0.2749013 1.        0.87     ], Reward: 1.0\n",
            "Processed Log → State: [0.53588045 0.69037586 0.28840885 0.9780634  0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.17766918 0.32437503 0.04412824 0.6400762  0.03      ], Reward: 7.0\n",
            "Processed Log → State: [0.17561515 0.3023179  0.04495023 0.5671716  0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.1944211  0.28280872 0.04489559 0.6136078  0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.20711508 0.30510265 0.03320411 0.63497263 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.22648603 0.32183382 0.02672828 0.6062746  0.15      ], Reward: 0.0\n",
            "Processed Log → State: [0.27581993 0.33079028 0.02256176 0.59987587 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.28767782 0.3609666  0.02288396 0.63116205 0.21      ], Reward: 6.0\n",
            "Processed Log → State: [2.7171874e-01 3.7688774e-01 1.4598653e-04 6.4515263e-01 2.3999999e-01], Reward: 8.0\n",
            "Processed Log → State: [0.27848536 0.33051404 0.01931977 0.61687493 0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.3033702  0.3377562  0.02317834 0.61013734 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.33607712 0.35650834 0.01271998 0.5563762  0.33      ], Reward: -1.0\n",
            "Processed Log → State: [0.33609286 0.3588673  0.01566977 0.6637292  0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.33799076 0.37158763 0.01939608 0.6480225  0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.34440216 0.41218144 0.0112544  0.7104334  0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.33107886 0.47090518 0.02856044 0.72182167 0.45      ], Reward: 0.0\n",
            "Processed Log → State: [0.323211   0.46650395 0.0330191  0.78807086 0.48      ], Reward: 5.0\n",
            "Processed Log → State: [0.344165   0.5128346  0.04025521 0.78100437 0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.33969808 0.5369813  0.02933045 0.72001565 0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.38151908 0.5351771  0.0332317  0.74436605 0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.40062684 0.54864043 0.04750235 0.70353264 0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.39084327 0.57664686 0.04548893 0.80839247 0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.40164348 0.6175242  0.05648362 0.75186586 0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.40295896 0.63091385 0.05580482 0.76343626 0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.4222583  0.6576971  0.04929873 0.7565529  0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.41942587 0.65353584 0.03905727 0.75196534 0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.4235741  0.6620385  0.02674679 0.76308423 0.78      ], Reward: 7.0\n",
            "Processed Log → State: [0.43639916 0.688459   0.04792697 0.78199416 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.39862925 0.68599343 0.05729334 0.8665822  0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.43052337 0.6945045  0.06630145 0.89228874 0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.45971957 0.6976237  0.09484433 0.8470192  0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.22611298 0.3939155  0.12641634 0.5202898  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.16758363 0.3647988  0.10807929 0.48157176 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.18634795 0.4126546  0.06251774 0.48600402 0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.26258323 0.39798826 0.12410194 0.39987576 0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.2638057  0.39257255 0.11862461 0.30568993 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.2781139  0.398247   0.27003348 0.18146233 0.18      ], Reward: 5.0\n",
            "Processed Log → State: [0.31104633 0.42983997 0.33329433 0.19029061 0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.30526197 0.44366315 0.29064435 0.1237238  0.24      ], Reward: -1.0\n",
            "Processed Log → State: [0.36732    0.42708427 0.26300624 0.07684234 0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.47360864 0.4628183  0.29727632 0.         0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.52336174 0.5148405  0.33385682 0.         0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.63433546 0.5162766  0.42688465 0.02957477 0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.6595737  0.55800885 0.36994058 0.08940223 0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.6323985  0.5828569  0.41784984 0.11724312 0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.6640445  0.5432064  0.47934148 0.15306255 0.45      ], Reward: 6.0\n",
            "Processed Log → State: [0.6984761  0.5787624  0.45620683 0.14783531 0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.6885127  0.6351996  0.5127433  0.20806578 0.51      ], Reward: 5.0\n",
            "Processed Log → State: [0.7860058  0.6043997  0.4837763  0.15937497 0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.72846717 0.5665068  0.5043893  0.18150896 0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.6996182  0.5798895  0.54602695 0.15105635 0.6       ], Reward: 0.0\n",
            "Processed Log → State: [0.82833755 0.608598   0.521456   0.13579382 0.63      ], Reward: 2.0\n",
            "Processed Log → State: [0.7789458  0.6026887  0.49626905 0.11082339 0.66      ], Reward: 7.0\n",
            "Processed Log → State: [0.7106137  0.6346611  0.46405894 0.1334325  0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.72860336 0.667672   0.46920487 0.10806142 0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.83127636 0.65118146 0.47440246 0.15801015 0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.80161315 0.6075437  0.49955183 0.19260892 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.88749975 0.62054694 0.49260607 0.23682496 0.81      ], Reward: 6.0\n",
            "Processed Log → State: [0.9221251  0.6667574  0.6094842  0.26526928 0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.9543944  0.6619821  0.62996876 0.27354383 0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.9709669  0.67673445 0.5957109  0.2657625  0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.23200269 0.3393148  0.18198161 0.56621027 0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.21690631 0.34131306 0.22517638 0.51726806 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.28875643 0.39141643 0.31549463 0.43512285 0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.28271675 0.44196832 0.3540053  0.38635606 0.12      ], Reward: -1.0\n",
            "Processed Log → State: [0.32583368 0.47708818 0.34843403 0.31656814 0.15      ], Reward: 2.0\n",
            "Processed Log → State: [0.38227934 0.4936591  0.37429252 0.3945752  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.47593936 0.4725678  0.48147598 0.34106058 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.56332254 0.45445538 0.5405326  0.29267493 0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.6336967  0.5252632  0.5140658  0.29119858 0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.69055516 0.5175928  0.55557734 0.33548796 0.3       ], Reward: 7.0\n",
            "Processed Log → State: [0.69390893 0.5062306  0.6748678  0.28851008 0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.71506864 0.5066475  0.7423429  0.32214248 0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.77930087 0.58345497 0.7817341  0.28416598 0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.79618436 0.6107711  0.7619248  0.32327822 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.8330851  0.6222969  0.7685212  0.28617564 0.45      ], Reward: 7.0\n",
            "Processed Log → State: [0.8262924  0.6188273  0.707056   0.29659608 0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.8149564  0.67321503 0.67461073 0.30803838 0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.931812   0.6995232  0.71102726 0.27752993 0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.99914235 0.7355516  0.7225748  0.24229848 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.74245346 0.7203123  0.31636593 0.6       ], Reward: 2.0\n",
            "Processed Log → State: [1.         0.7834469  0.7156543  0.31427783 0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.98039967 0.78235734 0.6895823  0.27029523 0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.99077255 0.7920211  0.7394617  0.30054736 0.69      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.80988103 0.72135335 0.23893417 0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.9752602  0.7746418  0.78736097 0.24469802 0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.9787458  0.7842175  0.75018626 0.17820184 0.78      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.8114363  0.82678604 0.2048171  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.9841602  0.82572067 0.8445321  0.25954393 0.84      ], Reward: 4.0\n",
            "Processed Log → State: [1.         0.8488872  0.79891336 0.24637756 0.87      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.87262833 0.7786098  0.18831335 0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.1502135  0.3331155  0.07069381 0.57677585 0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.1303556 0.3758796 0.16746   0.5736611 0.06     ], Reward: 6.0\n",
            "Processed Log → State: [0.15873136 0.38105676 0.17478976 0.6032862  0.09      ], Reward: 2.0\n",
            "Processed Log → State: [0.250037   0.41282272 0.19917917 0.6664131  0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.27526465 0.50159866 0.19704244 0.6570194  0.15      ], Reward: 6.0\n",
            "Processed Log → State: [0.29753423 0.54970014 0.16453782 0.65981185 0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.46621606 0.7056088  0.22370198 0.6907421  0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.49177518 0.63825905 0.27492175 0.73642343 0.24      ], Reward: 5.0\n",
            "Processed Log → State: [0.48986998 0.65017384 0.2860974  0.6786395  0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.49974078 0.6194477  0.296683   0.54084533 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.5924635  0.7199588  0.24356635 0.4974701  0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.68701094 0.7319115  0.28992218 0.5434561  0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.6958447  0.73588604 0.32890317 0.45898166 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.73937684 0.811049   0.29290375 0.43272826 0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.7430423  0.8869816  0.32907653 0.43773085 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.7127893  0.8393203  0.33772886 0.41126236 0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.70118463 0.8474575  0.36600646 0.34612435 0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.75735766 0.8582499  0.43224722 0.32160932 0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.71470624 0.84833187 0.39471942 0.3759468  0.57      ], Reward: 0.0\n",
            "Processed Log → State: [0.75192875 0.88854116 0.3901245  0.4112091  0.6       ], Reward: 6.0\n",
            "Processed Log → State: [0.7470173  0.91643435 0.3751572  0.401796   0.63      ], Reward: 7.0\n",
            "Processed Log → State: [0.7747828  0.9320584  0.3958933  0.36851907 0.66      ], Reward: 5.0\n",
            "Processed Log → State: [0.70135    1.         0.42939192 0.3527253  0.69      ], Reward: 8.0\n",
            "Processed Log → State: [0.7199801  0.9903087  0.4354332  0.30547422 0.72      ], Reward: 0.0\n",
            "Processed Log → State: [0.69494444 0.89469147 0.4316291  0.34092176 0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.6439918  0.8973118  0.42071027 0.3990707  0.78      ], Reward: 5.0\n",
            "Processed Log → State: [0.7668622  0.9112633  0.4434651  0.41078275 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.7614582  1.         0.46017644 0.49807903 0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.7580262  1.         0.4481693  0.54321796 0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.79816264 1.         0.45996803 0.6394356  0.9       ], Reward: 5.0\n",
            "Processed Log → State: [0.20447578 0.2884319  0.05027658 0.6740891  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.22031224 0.30201846 0.04631355 0.69724673 0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.23067775 0.3273825  0.0504753  0.7530189  0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.2623349  0.36824822 0.07631362 0.8078849  0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.2885304  0.40059388 0.08080762 0.8081064  0.15      ], Reward: 2.0\n",
            "Processed Log → State: [0.3053602  0.4162668  0.10264097 0.83552545 0.18      ], Reward: 0.0\n",
            "Processed Log → State: [0.35197607 0.43744642 0.11514625 0.8330282  0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.34388083 0.4709109  0.13718656 0.862043   0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.36109936 0.5207846  0.15298618 0.9010946  0.27      ], Reward: 7.0\n",
            "Processed Log → State: [0.4129376  0.566732   0.15983249 0.9230568  0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.40574083 0.5677413  0.17614764 0.8736251  0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.40917063 0.5800147  0.18065052 0.8947023  0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.41524273 0.6087049  0.19647795 0.9364636  0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.43100733 0.61559016 0.20714648 0.96092796 0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.44789422 0.6407414  0.20935547 0.9922258  0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.4673603 0.6686154 0.2234652 1.        0.48     ], Reward: 8.0\n",
            "Processed Log → State: [0.47040352 0.6688549  0.22544295 0.9649219  0.51      ], Reward: 2.0\n",
            "Processed Log → State: [0.498193   0.656804   0.25502217 0.983125   0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.50228417 0.68252134 0.26426113 0.9790756  0.57      ], Reward: 5.0\n",
            "Processed Log → State: [0.5150762  0.73093385 0.25877273 0.9894261  0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.55592    0.72141755 0.26873136 0.96840554 0.63      ], Reward: 5.0\n",
            "Processed Log → State: [0.5500849  0.7504142  0.27172098 1.         0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.570976   0.68882006 0.26459417 1.         0.69      ], Reward: 8.0\n",
            "Processed Log → State: [0.55908626 0.69160837 0.25099608 1.         0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.56552714 0.6892645  0.25174683 1.         0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.5789778  0.67528385 0.2536497  1.         0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.5974168  0.70130175 0.25705862 0.99495363 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.63029206 0.6970451  0.28036943 1.         0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.6070867  0.6968374  0.28224266 0.9594348  0.87      ], Reward: 5.0\n",
            "Processed Log → State: [0.62596154 0.7286133  0.28803343 1.         0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.20271926 0.31134653 0.05642253 0.6483223  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.22020814 0.33618328 0.06926688 0.6423668  0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.2510784  0.3566832  0.06657977 0.74491477 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.30563176 0.33729857 0.05293646 0.8655455  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.32970276 0.37671337 0.03811859 0.83946323 0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.3749439 0.4036212 0.027142  0.8874811 0.18     ], Reward: 1.0\n",
            "Processed Log → State: [0.38473195 0.396883   0.04778397 0.94736934 0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.35907695 0.4011516  0.0500265  0.99120176 0.24      ], Reward: 0.0\n",
            "Processed Log → State: [0.39791724 0.3829164  0.05879415 1.         0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.4301366  0.388406   0.05382917 1.         0.3       ], Reward: 2.0\n",
            "Processed Log → State: [0.44955543 0.4243117  0.06911077 1.         0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.45946407 0.4462564  0.0751242  1.         0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.4429454  0.43683863 0.09119505 0.9911345  0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.45376608 0.41101006 0.09274072 1.         0.42      ], Reward: 5.0\n",
            "Processed Log → State: [0.44664294 0.41454062 0.08309122 0.96897745 0.45      ], Reward: 5.0\n",
            "Processed Log → State: [0.42363143 0.44392368 0.08397894 0.996987   0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.4265486  0.46200424 0.08148102 0.99621844 0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.467168   0.47041464 0.087707   0.96731865 0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.50043213 0.47069272 0.07909956 0.9518507  0.57      ], Reward: 2.0\n",
            "Processed Log → State: [0.47941503 0.48739892 0.0816037  0.9871044  0.6       ], Reward: 0.0\n",
            "Processed Log → State: [0.50527877 0.44175082 0.10281642 1.         0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.5600944  0.46039912 0.09932674 1.         0.66      ], Reward: 7.0\n",
            "Processed Log → State: [0.5632918  0.44882786 0.09632754 0.9824868  0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.55534005 0.48081335 0.08187127 0.9233375  0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.57508934 0.50608027 0.09784971 0.9210836  0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.5877848  0.5384898  0.10592172 0.88577056 0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.6071329  0.520197   0.13480178 0.7944728  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.63387203 0.53911996 0.15690014 0.8772177  0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.65185577 0.5225655  0.16750824 0.8571929  0.87      ], Reward: 2.0\n",
            "Processed Log → State: [0.64085126 0.5411927  0.16856077 0.8307335  0.9       ], Reward: 6.0\n",
            "Processed Log → State: [0.2165255  0.32352537 0.04715841 0.671898   0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.22409785 0.31366968 0.03043812 0.67375064 0.06      ], Reward: 5.0\n",
            "Processed Log → State: [0.24674693 0.32913265 0.02148765 0.76666254 0.09      ], Reward: 0.0\n",
            "Processed Log → State: [0.27409235 0.34397537 0.03781985 0.83248067 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.28272152 0.3350121  0.02935728 0.80833375 0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.2855765  0.37396315 0.01844068 0.8325622  0.18      ], Reward: 2.0\n",
            "Processed Log → State: [0.2879508  0.3796125  0.02051203 0.8342953  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.30453724 0.3930153  0.03017282 0.76234376 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.32212225 0.41022703 0.02809204 0.7655732  0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.32901022 0.42679128 0.03623694 0.76317036 0.3       ], Reward: 7.0\n",
            "Processed Log → State: [0.290962   0.43640396 0.03697856 0.75180864 0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.29396424 0.43240717 0.04356532 0.75279    0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.35802191 0.4437572  0.05473826 0.769367   0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.34691685 0.4266062  0.07361581 0.8201774  0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.3614405  0.44784254 0.07906675 0.8499393  0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.38944176 0.47403255 0.08760099 0.8482208  0.48      ], Reward: -2.0\n",
            "Processed Log → State: [0.3828633  0.48551375 0.10242856 0.980019   0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.3890156  0.49970627 0.09244867 1.         0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.40816745 0.52795595 0.0825824  1.         0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.43405798 0.53153706 0.09450108 1.         0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.42753586 0.52878904 0.09400257 1.         0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.41449034 0.5196668  0.10241294 1.         0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.41458204 0.5376403  0.1144514  1.         0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.4207239  0.556903   0.09630433 0.98567146 0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.41663888 0.5551742  0.08604011 0.9848586  0.75      ], Reward: 6.0\n",
            "Processed Log → State: [0.4120008  0.5739314  0.08829807 0.937806   0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.40471795 0.58312696 0.10246442 0.9236042  0.81      ], Reward: 5.0\n",
            "Processed Log → State: [0.42053854 0.5724648  0.10193729 0.8815376  0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.43482274 0.5773468  0.12618783 0.8710863  0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.41285288 0.5772136  0.12930705 0.7535077  0.9       ], Reward: 6.0\n",
            "Processed Log → State: [0.20207432 0.43238497 0.07409015 0.53561646 0.03      ], Reward: 7.0\n",
            "Processed Log → State: [0.24082495 0.4540865  0.10089581 0.5552254  0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.18570326 0.5025803  0.11865769 0.47902912 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.21148436 0.50998515 0.16708668 0.47825775 0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.16269639 0.64174163 0.16807479 0.38963264 0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.2391022  0.7110811  0.1410303  0.32646114 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.28448233 0.6327564  0.14927569 0.32746813 0.21      ], Reward: 0.0\n",
            "Processed Log → State: [0.2353758  0.7141632  0.19639476 0.4514147  0.24      ], Reward: 7.0\n",
            "Processed Log → State: [0.275548   0.7339202  0.2095315  0.50026196 0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.3370508  0.82771665 0.2832761  0.5141762  0.3       ], Reward: 7.0\n",
            "Processed Log → State: [0.38635802 0.824304   0.23287407 0.53957444 0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.43054065 0.82331574 0.2470201  0.5693776  0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.5876686  0.8288233  0.21517348 0.5197282  0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.66297907 0.8662182  0.17320529 0.5173994  0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.67579705 0.9982443  0.19843225 0.5520169  0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.77952665 0.9867376  0.21334898 0.5286641  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.8365061  1.         0.2700587  0.58720607 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.79760873 1.         0.20624503 0.5141507  0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.8235872  1.         0.24831438 0.59210503 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.8668689  1.         0.24918675 0.60454506 0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.8847304  1.         0.27408013 0.6710563  0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.796452   1.         0.35238615 0.75277996 0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.87896025 1.         0.37356716 0.74171275 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.9362701  0.9392505  0.38137698 0.6941806  0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.96968377 0.95875883 0.30304593 0.68596935 0.75      ], Reward: 6.0\n",
            "Processed Log → State: [0.91282725 1.         0.31320924 0.6370472  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.8678902  1.         0.34407562 0.65216553 0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.84079677 0.9873942  0.36435398 0.6576554  0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.86476684 1.         0.3524434  0.64763474 0.87      ], Reward: 5.0\n",
            "Processed Log → State: [0.8624139  0.98388946 0.42175695 0.6087684  0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.20594382 0.37253913 0.08355656 0.53912276 0.03      ], Reward: 2.0\n",
            "Processed Log → State: [0.2597942  0.40817872 0.08072774 0.5089007  0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.31849986 0.46648762 0.04796524 0.4966831  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.33865872 0.5201199  0.15778677 0.47469115 0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.41506904 0.5912147  0.14529423 0.47001886 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.41687003 0.61482614 0.15107998 0.3402745  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.42853427 0.62854505 0.18861772 0.32117933 0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.43001795 0.62051845 0.16496202 0.2937833  0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.43695286 0.66391623 0.1760463  0.22225222 0.27      ], Reward: 0.0\n",
            "Processed Log → State: [0.43754688 0.70084465 0.18220143 0.24118958 0.3       ], Reward: 7.0\n",
            "Processed Log → State: [0.45912397 0.70562804 0.19137277 0.2973909  0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.43497726 0.7436062  0.24348561 0.33247524 0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.46896604 0.728451   0.27495256 0.36681455 0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.43332553 0.68467164 0.30325916 0.42252186 0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.5317009  0.7004693  0.32799956 0.4396004  0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.5391101  0.6655891  0.34128764 0.48419416 0.48      ], Reward: 2.0\n",
            "Processed Log → State: [0.5992739  0.7069081  0.36684805 0.49898908 0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.5875549  0.7908711  0.36651087 0.477674   0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.64502156 0.8694749  0.45682088 0.52681607 0.57      ], Reward: 5.0\n",
            "Processed Log → State: [0.7046945  0.89626724 0.50948477 0.5397082  0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.789585   0.87700194 0.48069292 0.48634014 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.83148295 0.8358235  0.38326052 0.54759085 0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.90315586 0.86810666 0.3576709  0.62313867 0.69      ], Reward: 8.0\n",
            "Processed Log → State: [0.99668616 0.8736116  0.40917566 0.64807385 0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.9211448  0.8809384  0.3789537  0.73149204 0.75      ], Reward: 4.0\n",
            "Processed Log → State: [0.8645845  0.8830917  0.35190356 0.6850003  0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.90239924 0.8370082  0.41439852 0.6101095  0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.9682769  0.84299135 0.56221116 0.6543472  0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.8961124  0.88977545 0.6249848  0.6348755  0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.90400374 0.8289671  0.70392346 0.5655869  0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.18272945 0.4018921  0.05871565 0.55734    0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.24932705 0.48525026 0.07066972 0.5131026  0.06      ], Reward: 0.0\n",
            "Processed Log → State: [0.2509327  0.54125726 0.05521637 0.52819836 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.22263172 0.6023353  0.11881532 0.4693823  0.12      ], Reward: 5.0\n",
            "Processed Log → State: [0.3046365  0.687444   0.17354374 0.5008687  0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.37460187 0.6445171  0.19376433 0.48831096 0.18      ], Reward: 5.0\n",
            "Processed Log → State: [0.42878044 0.58995587 0.18997954 0.4958197  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.50305974 0.5371096  0.20575468 0.3900132  0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.5873093  0.56994116 0.17664617 0.42644548 0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.58434325 0.46409178 0.19309928 0.4697633  0.3       ], Reward: -1.0\n",
            "Processed Log → State: [0.581569   0.60576445 0.1408053  0.46940604 0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.6204679  0.69594616 0.17879866 0.3906188  0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.7018851  0.7467169  0.23923525 0.37144363 0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.7496616  0.76557344 0.28379077 0.3391661  0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.7731931  0.82736486 0.34832633 0.28208995 0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.865341   0.87445414 0.3605768  0.2687823  0.48      ], Reward: 6.0\n",
            "Processed Log → State: [0.87383515 0.91987747 0.41383627 0.2408146  0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.94508505 0.8474259  0.44737953 0.31856284 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.97125226 0.89755803 0.4221814  0.37031856 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.8673188  0.45552006 0.4476691  0.6       ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.8158855  0.44666755 0.44505006 0.63      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.94016016 0.47216472 0.5166718  0.66      ], Reward: 5.0\n",
            "Processed Log → State: [1.         0.9886218  0.48785654 0.5141204  0.69      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.95720166 0.46121982 0.5482893  0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.9541909  0.8883527  0.49217498 0.5196071  0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.93312746 0.8547224  0.51531595 0.45115706 0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.94215703 0.80175984 0.5380204  0.44498816 0.81      ], Reward: 0.0\n",
            "Processed Log → State: [0.93008286 0.800488   0.53511065 0.45525277 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.859214   0.8806038  0.5343803  0.50032824 0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.8146892  0.89527327 0.5701646  0.4942196  0.9       ], Reward: -1.0\n",
            "Processed Log → State: [0.18665329 0.33376455 0.04764254 0.5639795  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.17243764 0.3108449  0.10598785 0.57481456 0.06      ], Reward: 5.0\n",
            "Processed Log → State: [0.14184877 0.31938094 0.11387587 0.57267046 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.22859949 0.3756226  0.0956692  0.57838595 0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.24346553 0.36367905 0.14409289 0.46007526 0.15      ], Reward: -1.0\n",
            "Processed Log → State: [0.24024577 0.3994722  0.13891092 0.485625   0.18      ], Reward: 5.0\n",
            "Processed Log → State: [0.282462   0.44297683 0.09237634 0.47827116 0.21      ], Reward: 0.0\n",
            "Processed Log → State: [0.32254088 0.5214413  0.09723733 0.5629561  0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.39835513 0.6412156  0.11473377 0.6512237  0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.5015406  0.69639456 0.11977568 0.6667938  0.3       ], Reward: -1.0\n",
            "Processed Log → State: [0.5026424  0.77266234 0.18293913 0.6952737  0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.4625374 0.7963258 0.1815406 0.6991253 0.36     ], Reward: 2.0\n",
            "Processed Log → State: [0.5647202  0.89035004 0.19833113 0.6457812  0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.5621721  0.89567226 0.18996017 0.7051037  0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.50115937 0.95086855 0.14344774 0.6251951  0.45      ], Reward: 6.0\n",
            "Processed Log → State: [0.4910578  0.90852547 0.14747524 0.6217042  0.48      ], Reward: 7.0\n",
            "Processed Log → State: [0.55931264 0.9846228  0.16259839 0.55442995 0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.52260196 1.         0.20347823 0.5787381  0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.5166512  0.9649334  0.21255876 0.60928607 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.4982868  0.9893774  0.20377505 0.6237     0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.6160962  0.99667317 0.24365555 0.65610796 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.7103951  0.93852186 0.28444713 0.63961726 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.6816534 0.9732763 0.306766  0.6142249 0.69     ], Reward: 1.0\n",
            "Processed Log → State: [0.6583697  1.         0.26370445 0.70008    0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.68783987 0.9734579  0.2893799  0.69157934 0.75      ], Reward: 5.0\n",
            "Processed Log → State: [0.77301866 0.9990504  0.29938757 0.6170949  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.7646401  1.         0.29364568 0.72714853 0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.8016841  0.99809164 0.32236004 0.7238558  0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.7999809  0.9998919  0.32279733 0.61016417 0.87      ], Reward: 5.0\n",
            "Processed Log → State: [0.8525806  0.94803715 0.34435946 0.5719297  0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.26455393 0.3423343  0.12352845 0.5161092  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.30234128 0.39947033 0.1496345  0.38095352 0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.34269536 0.44575045 0.1109245  0.28899357 0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.42781577 0.45567957 0.11765572 0.21858811 0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.4729933  0.4975862  0.10308059 0.09360593 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.52010393 0.48896006 0.20880277 0.07797785 0.18      ], Reward: 7.0\n",
            "Processed Log → State: [0.561125   0.5554726  0.29043886 0.05122265 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.5840104  0.55648273 0.35693896 0.         0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.562183   0.6110277  0.27426448 0.00550153 0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.701923   0.62168366 0.31156382 0.05624273 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.7453187  0.64249015 0.3697931  0.0407833  0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.7360727  0.7051955  0.49976113 0.         0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.7881988 0.7456381 0.5191384 0.        0.39     ], Reward: 6.0\n",
            "Processed Log → State: [0.7758693  0.7443623  0.55240935 0.08597559 0.42      ], Reward: 0.0\n",
            "Processed Log → State: [0.79290324 0.7430046  0.5379046  0.09476124 0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.7992584  0.77211577 0.5977409  0.1056353  0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.8390035  0.72134244 0.6838524  0.15399794 0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.8189526  0.76444507 0.74452615 0.16478969 0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.79661036 0.7466984  0.7066401  0.2179353  0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.844361   0.70952475 0.69416326 0.25644207 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.79558855 0.70105994 0.68261915 0.33286437 0.63      ], Reward: 6.0\n",
            "Processed Log → State: [0.8185667  0.6775976  0.61756915 0.31908798 0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.84271985 0.68216723 0.5786549  0.38449344 0.69      ], Reward: 7.0\n",
            "Processed Log → State: [0.86025107 0.69340026 0.53415513 0.42098904 0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.89640784 0.656346   0.6036418  0.4764604  0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.92678314 0.71490896 0.60550225 0.4556435  0.78      ], Reward: 2.0\n",
            "Processed Log → State: [1.         0.68041945 0.64434403 0.47512653 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.6803033  0.6058393  0.47824192 0.84      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.65178335 0.69703007 0.50714034 0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.9837249  0.6560775  0.707643   0.48121807 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.18445285 0.3553119  0.06184985 0.56233376 0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.16891411 0.45451236 0.05969544 0.44092274 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.10710969 0.55470014 0.1104079  0.45796898 0.09      ], Reward: -2.0\n",
            "Processed Log → State: [0.09026618 0.6308254  0.12307157 0.48193145 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.11962332 0.6122139  0.19261219 0.43376958 0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.12157    0.6352155  0.22403325 0.3866695  0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.11681168 0.5986579  0.25768214 0.37591088 0.21      ], Reward: 7.0\n",
            "Processed Log → State: [0.13994439 0.6526192  0.2276417  0.4265212  0.24      ], Reward: 0.0\n",
            "Processed Log → State: [0.1049832  0.70586807 0.23169754 0.39085925 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.15258522 0.772134   0.25695735 0.41671202 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.2579147  0.77858615 0.2589815  0.4446196  0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.28138    0.7906133  0.2490249  0.30756035 0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.32570758 0.8282499  0.30355582 0.3475772  0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.3833072  0.8072633  0.34742582 0.27468082 0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.42649052 0.9130271  0.38165995 0.1983188  0.45      ], Reward: -1.0\n",
            "Processed Log → State: [0.41007802 0.923697   0.39303672 0.2235827  0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.4969138  0.951582   0.3871742  0.29436368 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.5582539  0.93293536 0.40103698 0.3882681  0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.5916757  0.9022266  0.38505343 0.23475243 0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.5698711  0.97614014 0.37345457 0.2674847  0.6       ], Reward: 6.0\n",
            "Processed Log → State: [0.6274719 0.9959308 0.3549338 0.3076706 0.63     ], Reward: 3.0\n",
            "Processed Log → State: [0.6410255  0.9050081  0.37856835 0.26432943 0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.632697   0.9337765  0.4068797  0.21279176 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.7224191  0.8660591  0.43042812 0.35118896 0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.74108195 0.89581084 0.41108978 0.36736676 0.75      ], Reward: 0.0\n",
            "Processed Log → State: [0.76821667 0.9044053  0.45474255 0.393772   0.78      ], Reward: 6.0\n",
            "Processed Log → State: [0.7777878  0.8517064  0.47153366 0.447432   0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.7035242  0.88943064 0.4810109  0.3505703  0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.6192871  0.8209668  0.5297897  0.34457186 0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.65120804 0.7830486  0.522635   0.34444034 0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.2162515  0.34395143 0.04941342 0.6849008  0.03      ], Reward: 7.0\n",
            "Processed Log → State: [0.21363029 0.40445223 0.06301013 0.7414244  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.2077069  0.45309475 0.06762189 0.7651715  0.09      ], Reward: 2.0\n",
            "Processed Log → State: [0.2571875  0.47640386 0.08365614 0.74613404 0.12      ], Reward: 7.0\n",
            "Processed Log → State: [0.27742007 0.5086093  0.09122704 0.7873259  0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.3023748  0.55317867 0.09842371 0.76690364 0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.323803   0.5500848  0.10709869 0.8122794  0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.33738786 0.5698052  0.10728782 0.85991824 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.3533732  0.54225165 0.12036762 0.8453452  0.27      ], Reward: -1.0\n",
            "Processed Log → State: [0.3913834  0.56604564 0.13274214 0.96845233 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.42218867 0.5682221  0.1263689  1.         0.33      ], Reward: 2.0\n",
            "Processed Log → State: [0.43442777 0.578799   0.13869382 1.         0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.45705128 0.61464214 0.14845961 0.9746134  0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.48697022 0.58382    0.1533441  1.         0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.5025134  0.5773672  0.18065579 0.9859724  0.45      ], Reward: 0.0\n",
            "Processed Log → State: [0.5257053  0.5999712  0.19735956 1.         0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.5439434  0.6058738  0.21770167 1.         0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.53711945 0.6091096  0.19780283 1.         0.54      ], Reward: 5.0\n",
            "Processed Log → State: [0.5243688  0.64222306 0.20522061 1.         0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.5598745  0.6451347  0.20495413 0.968357   0.6       ], Reward: 0.0\n",
            "Processed Log → State: [0.5700656  0.6694658  0.20630929 0.9365497  0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.58902997 0.6748504  0.19922283 0.96377313 0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.61259794 0.6808114  0.21056022 1.         0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.6169271  0.70227504 0.21075155 0.98825914 0.72      ], Reward: -2.0\n",
            "Processed Log → State: [0.60734487 0.7042711  0.22205737 1.         0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.5874205  0.7108419  0.21913482 1.         0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.6148127  0.7469782  0.24356014 1.         0.81      ], Reward: 2.0\n",
            "Processed Log → State: [0.6217987  0.7380704  0.2646552  0.98848313 0.84      ], Reward: 0.0\n",
            "Processed Log → State: [0.63674927 0.76469195 0.28338617 0.9143095  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.6306953  0.7593043  0.2907529  0.92504275 0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.34955236 0.34344175 0.09311906 0.5507359  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.37535268 0.36083543 0.1458118  0.47396737 0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.32067192 0.4508031  0.15809171 0.49462706 0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.28707382 0.48964456 0.17900924 0.38875404 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.32528797 0.581853   0.18018577 0.3171198  0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.30060527 0.6012929  0.18164371 0.32409462 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.27502152 0.53038687 0.22775339 0.3942896  0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.39222774 0.59706485 0.2348239  0.32378295 0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.42074344 0.59732735 0.2759392  0.34129977 0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.42258447 0.60372305 0.26658902 0.38356906 0.3       ], Reward: 7.0\n",
            "Processed Log → State: [0.48112065 0.6788686  0.2686306  0.39593962 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.49380293 0.69495654 0.2554273  0.39438543 0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.4874607 0.8232768 0.1975174 0.4318682 0.39     ], Reward: 0.0\n",
            "Processed Log → State: [0.45113906 0.86234653 0.18892696 0.42826405 0.42      ], Reward: 5.0\n",
            "Processed Log → State: [0.4590105  0.90273875 0.20635068 0.44733274 0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.3924211  0.94254583 0.22461548 0.41642225 0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.35672778 0.9077792  0.20687228 0.4626051  0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.3448924  0.9289162  0.23979186 0.40801078 0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.3919894  0.9201339  0.27974525 0.40179142 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.36870432 0.9003824  0.30179015 0.4159738  0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.3359995  0.9362652  0.3471052  0.42265305 0.63      ], Reward: -2.0\n",
            "Processed Log → State: [0.34595695 0.92932266 0.39220533 0.3696932  0.66      ], Reward: 0.0\n",
            "Processed Log → State: [0.37759483 1.         0.40197122 0.44939676 0.69      ], Reward: 8.0\n",
            "Processed Log → State: [0.34595263 0.99845546 0.4249216  0.5475318  0.72      ], Reward: 7.0\n",
            "Processed Log → State: [0.4383431  1.         0.42855573 0.5372206  0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.55206025 0.9870464  0.40436897 0.5381347  0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.5743713  1.         0.41302565 0.53757846 0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.65909714 0.98208755 0.41625285 0.5079016  0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.6348559  0.9650102  0.42537025 0.5788955  0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.6634294 1.        0.4669508 0.6212347 0.9      ], Reward: 0.0\n",
            "Processed Log → State: [0.22681868 0.31077155 0.04682449 0.6020689  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.2262235  0.32951733 0.03995104 0.65827674 0.06      ], Reward: 0.0\n",
            "Processed Log → State: [0.2661234  0.32419032 0.0396283  0.59970295 0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.2756456  0.31890208 0.04903016 0.6241424  0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.29066512 0.30035514 0.06628566 0.6732708  0.15      ], Reward: 0.0\n",
            "Processed Log → State: [0.28405517 0.32943353 0.04896322 0.6957883  0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.3151747  0.33886692 0.03976807 0.7256401  0.21      ], Reward: 7.0\n",
            "Processed Log → State: [0.31982103 0.3639508  0.03150675 0.7094583  0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.31263003 0.35755375 0.03409488 0.7054654  0.27      ], Reward: 7.0\n",
            "Processed Log → State: [0.35855836 0.3819199  0.05265155 0.7250228  0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.36409068 0.4211808  0.06623618 0.74559367 0.33      ], Reward: 5.0\n",
            "Processed Log → State: [0.34048477 0.433741   0.07502635 0.66582084 0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.33789286 0.429764   0.09335386 0.69822556 0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.34483355 0.42260176 0.10924658 0.75105965 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.36810988 0.46303216 0.10747384 0.7586372  0.45      ], Reward: 0.0\n",
            "Processed Log → State: [0.39536062 0.4461621  0.10647526 0.8187286  0.48      ], Reward: 4.0\n",
            "Processed Log → State: [0.4317332  0.45808613 0.103383   0.93079484 0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.44806543 0.45274323 0.12674212 0.97397614 0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.43590584 0.5072801  0.12651053 1.         0.57      ], Reward: 2.0\n",
            "Processed Log → State: [0.45140383 0.5512012  0.12424514 0.98178667 0.6       ], Reward: 6.0\n",
            "Processed Log → State: [0.49215448 0.5591122  0.1268294  1.         0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.4862595  0.53634924 0.11486626 1.         0.66      ], Reward: 7.0\n",
            "Processed Log → State: [0.47994414 0.57432365 0.09592012 0.975607   0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.47415718 0.5863684  0.10277227 0.9661371  0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.49784192 0.5936085  0.10715262 0.9900341  0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.48978648 0.60460275 0.11621383 0.9776096  0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.49254698 0.63226736 0.13398711 1.         0.81      ], Reward: 2.0\n",
            "Processed Log → State: [0.4584234  0.61339647 0.15802766 1.         0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.43144268 0.6376956  0.18059473 1.         0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.415775   0.63846046 0.1859371  0.9786901  0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.23639666 0.33519953 0.09082626 0.5980515  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.32446566 0.4063213  0.13613315 0.5938545  0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.30751082 0.49125245 0.22229703 0.44524145 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.4083804  0.4658922  0.26705873 0.42945582 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.40718594 0.44680446 0.33130264 0.38134652 0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.46702865 0.4722825  0.35606065 0.4053385  0.18      ], Reward: 0.0\n",
            "Processed Log → State: [0.5309686  0.52083665 0.33953837 0.3305387  0.21      ], Reward: 0.0\n",
            "Processed Log → State: [0.5153564  0.4920217  0.42675266 0.23936181 0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.5677798  0.48222992 0.48721474 0.24679056 0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.4903984  0.4673261  0.46120575 0.25624582 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.46725312 0.5289162  0.54874915 0.33514196 0.33      ], Reward: 2.0\n",
            "Processed Log → State: [0.43554536 0.48496842 0.573837   0.3443046  0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.5401123  0.5265446  0.62188196 0.3951265  0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.5722605  0.55859685 0.66035014 0.3844394  0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.59980655 0.5592392  0.74135023 0.38485953 0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.6518162  0.6048186  0.7829032  0.49583468 0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.6894703 0.6305766 0.7702614 0.5180223 0.51     ], Reward: 5.0\n",
            "Processed Log → State: [0.779794  0.6500119 0.8222344 0.6203623 0.54     ], Reward: 8.0\n",
            "Processed Log → State: [0.78377557 0.7396738  0.8672847  0.63962716 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.7925809  0.7375182  0.96027964 0.7154426  0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.84229255 0.756728   1.         0.62319946 0.63      ], Reward: 7.0\n",
            "Processed Log → State: [0.82745993 0.77349603 1.         0.59413743 0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.921259  0.7377792 0.9864892 0.6078396 0.69     ], Reward: 2.0\n",
            "Processed Log → State: [0.9589358  0.743284   0.9846062  0.55170774 0.72      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.71304417 1.         0.49340832 0.75      ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.70114785 0.9611214  0.4797051  0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.9882669  0.71615887 0.9990659  0.4870559  0.81      ], Reward: 7.0\n",
            "Processed Log → State: [0.8855743 0.7631608 1.        0.4730392 0.84     ], Reward: 3.0\n",
            "Processed Log → State: [0.9226754 0.7863135 0.997307  0.5106544 0.87     ], Reward: 8.0\n",
            "Processed Log → State: [0.90621203 0.8352477  0.9783402  0.53242636 0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.2307876  0.3004228  0.06176865 0.6210524  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.24606441 0.3347685  0.06942184 0.70065486 0.06      ], Reward: 7.0\n",
            "Processed Log → State: [0.2853265  0.3589151  0.08258398 0.6701457  0.09      ], Reward: -2.0\n",
            "Processed Log → State: [0.29111588 0.38638827 0.11207828 0.77465075 0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.32788083 0.40616146 0.11741935 0.79451495 0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.34882408 0.41464323 0.11373355 0.82374907 0.18      ], Reward: 4.0\n",
            "Processed Log → State: [0.32176873 0.39811873 0.13958107 0.9052419  0.21      ], Reward: 7.0\n",
            "Processed Log → State: [0.3465083  0.38867366 0.15088378 0.8735606  0.24      ], Reward: 5.0\n",
            "Processed Log → State: [0.3921106  0.41822237 0.16712417 0.88125604 0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.42312157 0.46386755 0.19461964 0.9032514  0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.4444395  0.4676922  0.19719508 0.9192122  0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.4564872  0.49097514 0.19859181 0.9233244  0.36      ], Reward: 4.0\n",
            "Processed Log → State: [0.47892314 0.51702505 0.20694426 0.940851   0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.49436128 0.50547504 0.19483657 0.9433574  0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.53142697 0.49224713 0.19534016 0.94550973 0.45      ], Reward: 6.0\n",
            "Processed Log → State: [0.53519666 0.4689161  0.2126966  0.9751442  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.53063554 0.4707774  0.23122534 0.90348583 0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.5453097  0.49443114 0.22334601 0.93022144 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.5787935  0.5292315  0.21236059 0.9670048  0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.5588581  0.5465703  0.21633464 0.9806329  0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.56386864 0.57379967 0.22147101 1.         0.63      ], Reward: 6.0\n",
            "Processed Log → State: [0.5636535  0.598111   0.20382059 1.         0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.5867542  0.62455106 0.21336442 1.         0.69      ], Reward: 7.0\n",
            "Processed Log → State: [0.6201871  0.65285975 0.2102459  0.95510286 0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.60578066 0.6498922  0.21865918 0.9345196  0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.62235    0.66221416 0.23551261 0.9527767  0.78      ], Reward: 5.0\n",
            "Processed Log → State: [0.62137276 0.6570747  0.2500397  0.9070382  0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.65184194 0.68731636 0.26942435 0.9699938  0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.64288825 0.65382814 0.30691364 0.9956111  0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.6158759  0.66679853 0.32119727 1.         0.9       ], Reward: 7.0\n",
            "Processed Log → State: [0.253661   0.29224306 0.07856718 0.56082195 0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.30417162 0.3594052  0.09721809 0.5571817  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.3051718  0.33356705 0.23245175 0.5268171  0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.38019454 0.3581065  0.2934953  0.5079145  0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.48959833 0.36988246 0.33282802 0.42968413 0.15      ], Reward: 5.0\n",
            "Processed Log → State: [0.4735072  0.41300672 0.37490916 0.3913145  0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.47723976 0.43545976 0.3303428  0.324826   0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.4888625  0.47340447 0.39960727 0.28739265 0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.4755926  0.47593784 0.5012652  0.31992245 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.5094053  0.53511226 0.47193033 0.27801776 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.5500955  0.54671615 0.38500828 0.2018704  0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.52068007 0.5572808  0.3595177  0.25920954 0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.5095313  0.5986453  0.42735395 0.20464084 0.39      ], Reward: 0.0\n",
            "Processed Log → State: [0.5441076  0.63125473 0.47114438 0.21573837 0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.58156735 0.64717966 0.50322324 0.27508983 0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.67979306 0.664244   0.54982454 0.3066506  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.61539584 0.7051211  0.5854351  0.29205206 0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.6732555  0.75684935 0.67263377 0.25363594 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.6835286 0.7528885 0.6160212 0.3381417 0.57     ], Reward: 0.0\n",
            "Processed Log → State: [0.79778355 0.78666544 0.6600072  0.38676494 0.6       ], Reward: 6.0\n",
            "Processed Log → State: [0.8493829  0.77387625 0.65231365 0.3166458  0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.801875   0.7695559  0.6495737  0.35210517 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.84229463 0.8104427  0.60188985 0.411617   0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.92736775 0.8193392  0.5931801  0.41664034 0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.94803196 0.85959125 0.67829436 0.36803564 0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.97357005 0.84154296 0.6819836  0.3754344  0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.9822485  0.817877   0.74123216 0.39553893 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.85449547 0.72977954 0.40484866 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.99502486 0.9221332  0.7572553  0.48711    0.87      ], Reward: 2.0\n",
            "Processed Log → State: [1.        0.9088142 0.831302  0.5202144 0.9      ], Reward: -2.0\n",
            "Processed Log → State: [0.22455268 0.42389554 0.07238407 0.5499139  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.23687649 0.39465067 0.13814287 0.52552605 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.25259155 0.48867056 0.15602028 0.516631   0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.2214341  0.5110317  0.21092351 0.5139111  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.27886608 0.51946473 0.20540883 0.5580299  0.15      ], Reward: 4.0\n",
            "Processed Log → State: [0.3690995  0.51215065 0.22338907 0.5488631  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.41812155 0.5410523  0.23088492 0.5200989  0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.46414617 0.628252   0.21762174 0.5484294  0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.5627538 0.7435182 0.2319108 0.6140073 0.27     ], Reward: 8.0\n",
            "Processed Log → State: [0.6060451  0.84304893 0.23986748 0.6193414  0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.66662264 0.8407091  0.24947095 0.5803601  0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.7765087  0.9261662  0.29284814 0.5376617  0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.79267454 0.95145684 0.34675863 0.59196365 0.39      ], Reward: 7.0\n",
            "Processed Log → State: [0.7798086  0.95607376 0.36198828 0.5323816  0.42      ], Reward: 0.0\n",
            "Processed Log → State: [0.802706   0.9257689  0.40131027 0.44734347 0.45      ], Reward: 6.0\n",
            "Processed Log → State: [0.81340176 0.92579347 0.4050561  0.4746925  0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.8052361  0.9498047  0.4125976  0.50606143 0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.7599499  0.92568976 0.38683993 0.48846823 0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.7160776  0.8755006  0.42066035 0.47384685 0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.70296615 0.8877392  0.43805733 0.53433245 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.8139216  0.86276525 0.44408652 0.61776274 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.7178685  0.8254966  0.45674768 0.534326   0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.7519952  0.8645076  0.46730283 0.5803293  0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.7352117  0.88712794 0.42571726 0.6421277  0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.7315333  0.9300547  0.44854382 0.6246974  0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.7038059  0.94319093 0.4584604  0.64599925 0.78      ], Reward: 5.0\n",
            "Processed Log → State: [0.7248608  0.89275575 0.47824556 0.6854457  0.81      ], Reward: 2.0\n",
            "Processed Log → State: [0.70102787 0.9545557  0.5142374  0.6387398  0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.73291373 0.9721723  0.5103229  0.6606149  0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.71470517 0.96543694 0.47804728 0.7388592  0.9       ], Reward: 7.0\n",
            "Processed Log → State: [0.21925503 0.29752812 0.05218462 0.60948855 0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.22620209 0.30169085 0.07932523 0.5968641  0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.21455799 0.34813666 0.0798966  0.65156007 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.23565036 0.3434931  0.08321626 0.69433546 0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.23849602 0.38855627 0.07838783 0.7275987  0.15      ], Reward: 6.0\n",
            "Processed Log → State: [0.20753478 0.37052554 0.08319902 0.7355548  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.19424897 0.3595658  0.09319443 0.87743604 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.20722462 0.36088052 0.10012059 1.         0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.21690184 0.4123467  0.11833731 1.         0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.23769559 0.4164623  0.11987611 1.         0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.27016145 0.44014883 0.11062782 1.         0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.25447795 0.50317556 0.10776857 1.         0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.23478912 0.5051998  0.11775164 0.9686857  0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.26933554 0.5663627  0.13354212 0.9916358  0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.28101134 0.58157223 0.13414852 1.         0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.27202138 0.57698536 0.13795146 1.         0.48      ], Reward: 5.0\n",
            "Processed Log → State: [0.26051226 0.54580104 0.14128837 1.         0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.26315388 0.5722715  0.13943431 1.         0.54      ], Reward: 5.0\n",
            "Processed Log → State: [0.24093659 0.5937785  0.12636824 0.979133   0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.27640364 0.6231771  0.12343309 0.97250754 0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.27493843 0.6135441  0.13607682 1.         0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.29769954 0.63430625 0.1479539  1.         0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.29758728 0.63004273 0.14807649 1.         0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.28953466 0.6086432  0.14605156 0.98697764 0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.28793898 0.6147422  0.14841482 1.         0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.28673732 0.64893746 0.15029596 1.         0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.3020271  0.6367511  0.16565566 0.9607197  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.29255792 0.58655787 0.18749347 1.         0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.31143293 0.5901008  0.20571348 1.         0.87      ], Reward: 5.0\n",
            "Processed Log → State: [0.3085774  0.59264606 0.21643712 1.         0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.22828923 0.29916394 0.05252318 0.69957006 0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.2268519  0.29116634 0.05306781 0.725059   0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.22443387 0.3055088  0.0652584  0.71129096 0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.20519373 0.30426717 0.06243897 0.7573309  0.12      ], Reward: 4.0\n",
            "Processed Log → State: [0.2428882  0.33351523 0.05904942 0.7365187  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.26959807 0.3686274  0.07245777 0.7249398  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.29041862 0.41489542 0.05088516 0.7130966  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.2887091  0.43065557 0.05108745 0.6703143  0.24      ], Reward: 5.0\n",
            "Processed Log → State: [0.30314225 0.41830495 0.06925237 0.6949931  0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.33849886 0.42647034 0.07668842 0.6867014  0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.39426214 0.42668915 0.08966707 0.76294935 0.33      ], Reward: 5.0\n",
            "Processed Log → State: [0.41194758 0.44919792 0.09295499 0.834083   0.36      ], Reward: 7.0\n",
            "Processed Log → State: [0.40991783 0.4624201  0.08672664 0.9067572  0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.38932085 0.50113505 0.08322355 0.9230646  0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.38708964 0.46762934 0.08499063 0.92710364 0.45      ], Reward: 5.0\n",
            "Processed Log → State: [0.40057385 0.46142375 0.09411678 0.9364149  0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.42502412 0.44056565 0.09681989 0.97516376 0.51      ], Reward: 7.0\n",
            "Processed Log → State: [0.43443602 0.44686052 0.10047964 0.9799192  0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.44211784 0.46851426 0.09963245 0.9439323  0.57      ], Reward: 0.0\n",
            "Processed Log → State: [0.4207473  0.4592145  0.10907444 0.94623387 0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.3993592  0.48222715 0.10164789 1.         0.63      ], Reward: 2.0\n",
            "Processed Log → State: [0.41859537 0.47378772 0.1032038  1.         0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.44621482 0.45806915 0.10372823 1.         0.69      ], Reward: 8.0\n",
            "Processed Log → State: [0.4448655  0.42978978 0.11379478 1.         0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.4208178  0.42323548 0.10656966 1.         0.75      ], Reward: 6.0\n",
            "Processed Log → State: [0.4438594  0.4451584  0.10099156 0.9768295  0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.44785464 0.4250045  0.12520091 0.9436508  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.440385   0.4016208  0.14962503 0.98555386 0.84      ], Reward: 0.0\n",
            "Processed Log → State: [0.4806337  0.41408077 0.17956798 1.         0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.49865875 0.42597306 0.18922086 1.         0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.13416345 0.37129703 0.09626544 0.53008074 0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.1959304  0.4261687  0.09529173 0.4799827  0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.3117079  0.47578028 0.15798613 0.5025183  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.36669326 0.57145375 0.18074124 0.4818803  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.46749556 0.5684329  0.22940938 0.46233767 0.15      ], Reward: 2.0\n",
            "Processed Log → State: [0.42379838 0.5907312  0.2370929  0.5017359  0.18      ], Reward: 0.0\n",
            "Processed Log → State: [0.4272732  0.6773844  0.22394128 0.3922171  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.43264452 0.72195154 0.24001904 0.3604076  0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.5029663  0.8044939  0.24300252 0.37847334 0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.6244705  0.8486177  0.271473   0.33526254 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.6324226  0.868939   0.3198624  0.37813592 0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.6454266  0.89006424 0.34430513 0.30985734 0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.6787993  0.8141514  0.34190187 0.36897776 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.7101106  0.8781821  0.34985614 0.3015626  0.42      ], Reward: 0.0\n",
            "Processed Log → State: [0.705402   0.8603967  0.3395773  0.29133227 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.83101285 1.         0.30620843 0.23709027 0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.81611085 1.         0.28911346 0.26777008 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.85638976 1.         0.310732   0.3264098  0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.88705957 0.99360615 0.25868937 0.35396948 0.57      ], Reward: 0.0\n",
            "Processed Log → State: [0.8075508 1.        0.2654524 0.338217  0.6      ], Reward: 8.0\n",
            "Processed Log → State: [0.8211351  1.         0.2931863  0.34276384 0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.88233167 1.         0.3179673  0.3491532  0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.8237531  0.92131346 0.2659066  0.35471112 0.69      ], Reward: -1.0\n",
            "Processed Log → State: [0.8079426  0.9982778  0.2540215  0.36335745 0.72      ], Reward: 5.0\n",
            "Processed Log → State: [0.8283326  0.95076233 0.2345582  0.32357177 0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.89842546 1.         0.25191087 0.40097868 0.78      ], Reward: 7.0\n",
            "Processed Log → State: [0.8561645  0.97979856 0.27163982 0.33506456 0.81      ], Reward: 6.0\n",
            "Processed Log → State: [0.82050586 1.         0.29275197 0.28852344 0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.8055452  0.9372048  0.32437974 0.3810274  0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.9331915 0.9453639 0.3371281 0.3959276 0.9      ], Reward: 6.0\n",
            "Processed Log → State: [0.23257296 0.29294214 0.0356507  0.6357436  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.2554735  0.32450294 0.02357131 0.5628362  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.23177452 0.38314477 0.0140891  0.5843454  0.09      ], Reward: 2.0\n",
            "Processed Log → State: [0.23537084 0.40325987 0.01511584 0.60325205 0.12      ], Reward: 0.0\n",
            "Processed Log → State: [0.25178793 0.42202017 0.01926009 0.52851087 0.15      ], Reward: 2.0\n",
            "Processed Log → State: [0.28328183 0.4043702  0.0098006  0.60309047 0.18      ], Reward: 5.0\n",
            "Processed Log → State: [0.3126949  0.4214038  0.01104748 0.6571395  0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.33071348 0.43318254 0.01877381 0.73590523 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.33127195 0.48493668 0.02790427 0.79764384 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.33648166 0.48593324 0.02354342 0.8179017  0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.33321574 0.496629   0.03267967 0.8223427  0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.35481846 0.51988345 0.02373501 0.834946   0.36      ], Reward: -1.0\n",
            "Processed Log → State: [0.36459067 0.5565329  0.01565221 0.901334   0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.37471467 0.57055277 0.0316274  0.9293978  0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.42975712 0.5571387  0.02226883 0.89548296 0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.44878742 0.5738499  0.02344587 0.9319749  0.48      ], Reward: 5.0\n",
            "Processed Log → State: [0.4600426  0.5806535  0.01913865 1.         0.51      ], Reward: 5.0\n",
            "Processed Log → State: [0.47313988 0.5819756  0.01933791 0.9549674  0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.46385136 0.57486415 0.05222134 0.9710185  0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.44191724 0.5711301  0.05493489 1.         0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.4454815  0.57392985 0.04497883 1.         0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.45278516 0.5987383  0.04001491 1.         0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.42718524 0.6487569  0.03807639 0.94914526 0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.41062254 0.64773905 0.04973205 0.8845315  0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.42467168 0.6405691  0.05839828 0.9596752  0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.411503   0.663703   0.06576436 0.9805561  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.40869316 0.6909162  0.07428993 0.94082075 0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.39526397 0.6687508  0.09760403 0.9091482  0.84      ], Reward: 7.0\n",
            "Processed Log → State: [0.42429218 0.65526295 0.11941709 0.81321526 0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.46982828 0.6354911  0.14259082 0.85295445 0.9       ], Reward: 5.0\n",
            "Processed Log → State: [0.21027967 0.27269113 0.04516978 0.56955284 0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.238727   0.26656836 0.04959648 0.4839934  0.06      ], Reward: -1.0\n",
            "Processed Log → State: [0.2586803  0.27203983 0.03655711 0.46969965 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.24992765 0.2859623  0.05145187 0.4719735  0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.26028028 0.263676   0.04805436 0.5013702  0.15      ], Reward: 5.0\n",
            "Processed Log → State: [0.25669295 0.2783269  0.05267901 0.5630596  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.24118584 0.28489357 0.06337924 0.5126796  0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.26112077 0.29402605 0.06526274 0.54855484 0.24      ], Reward: 7.0\n",
            "Processed Log → State: [0.27050653 0.31542248 0.06490885 0.5372882  0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.2681613  0.32768926 0.0601198  0.5131261  0.3       ], Reward: 2.0\n",
            "Processed Log → State: [0.2640977  0.36274987 0.04870908 0.62962407 0.33      ], Reward: 7.0\n",
            "Processed Log → State: [0.28452748 0.3642833  0.05740409 0.58401144 0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.29146892 0.38553417 0.06225382 0.56279045 0.39      ], Reward: 7.0\n",
            "Processed Log → State: [0.33415368 0.39767128 0.05681318 0.5773697  0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.34088013 0.47864696 0.04686323 0.5488454  0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.30494753 0.45544767 0.06416123 0.56957245 0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.31242284 0.48182    0.07185638 0.49794188 0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.34794787 0.5163389  0.06850088 0.57535815 0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.36836982 0.5431097  0.05227056 0.6759584  0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.35085762 0.5332191  0.05754282 0.64448607 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.35104173 0.5329006  0.06095307 0.6923223  0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.33814487 0.54032904 0.05476499 0.7137109  0.66      ], Reward: 7.0\n",
            "Processed Log → State: [0.34252512 0.5295679  0.05678549 0.7081698  0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.35965866 0.5375296  0.06521348 0.6927714  0.72      ], Reward: 0.0\n",
            "Processed Log → State: [0.40228507 0.5248391  0.05255098 0.68938917 0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.40037817 0.5519926  0.0502704  0.74753463 0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.38621762 0.5692713  0.07813265 0.73292893 0.81      ], Reward: 5.0\n",
            "Processed Log → State: [0.41668898 0.57859117 0.1025562  0.8073717  0.84      ], Reward: 7.0\n",
            "Processed Log → State: [0.4297003  0.5933983  0.11429409 0.7790372  0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.40350342 0.60712326 0.13823982 0.80620825 0.9       ], Reward: 0.0\n",
            "Processed Log → State: [0.20644693 0.3387264  0.06732932 0.6559341  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.23064762 0.3753569  0.06589898 0.62691903 0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.20703755 0.42506889 0.08312713 0.6438053  0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.25599664 0.44050288 0.07738971 0.69968486 0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.2795105  0.47243294 0.07620927 0.76061976 0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.29249504 0.48669916 0.08663762 0.73088163 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.2846598  0.49642888 0.09614199 0.78398156 0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.3085487  0.53351086 0.11187894 0.82178885 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.35763654 0.5596068  0.09959175 0.85161906 0.27      ], Reward: 5.0\n",
            "Processed Log → State: [0.3655002  0.57096773 0.12837836 0.8540954  0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.39934254 0.59627765 0.14789201 0.9469815  0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.42054486 0.5759209  0.13843766 0.955816   0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.44758338 0.5786493  0.1521455  1.         0.39      ], Reward: 7.0\n",
            "Processed Log → State: [0.45461076 0.5849514  0.15285133 0.9880411  0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.49140632 0.6050722  0.16696993 0.93871737 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.48968408 0.6247647  0.16954309 0.95539635 0.48      ], Reward: 2.0\n",
            "Processed Log → State: [0.49433503 0.63762903 0.18629491 0.8956284  0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.500784   0.6241688  0.20237498 0.9123122  0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.5323208  0.622253   0.19749928 0.9095336  0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.53936917 0.6603487  0.19614568 0.98383945 0.6       ], Reward: 0.0\n",
            "Processed Log → State: [0.5593235  0.6699292  0.19555397 1.         0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.57722664 0.67436683 0.18348324 0.9624795  0.66      ], Reward: 4.0\n",
            "Processed Log → State: [0.56923574 0.6942478  0.1806118  0.9854251  0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.55642235 0.6953341  0.17663606 1.         0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.59967184 0.70332587 0.17126304 1.         0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.6178693  0.71733594 0.163611   1.         0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.61624545 0.70349133 0.18797135 1.         0.81      ], Reward: 7.0\n",
            "Processed Log → State: [0.604111   0.71697325 0.21044059 1.         0.84      ], Reward: 0.0\n",
            "Processed Log → State: [0.6031711 0.7140201 0.222282  1.        0.87     ], Reward: 8.0\n",
            "Processed Log → State: [0.61830306 0.6999694  0.24229293 1.         0.9       ], Reward: 6.0\n",
            "Processed Log → State: [0.2428202  0.3344736  0.13206284 0.5351453  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.27314088 0.3858034  0.21111935 0.5262453  0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.30960453 0.4025349  0.2141674  0.53140783 0.09      ], Reward: 5.0\n",
            "Processed Log → State: [0.28030977 0.43926838 0.28887206 0.48582673 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.355906   0.46149236 0.3109963  0.4445253  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.32639354 0.44287083 0.3685233  0.40065107 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.33229136 0.47845235 0.4004561  0.30443236 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.33461937 0.50386566 0.39786154 0.29527047 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.444794   0.5547556  0.5298333  0.39769247 0.27      ], Reward: 0.0\n",
            "Processed Log → State: [0.47878137 0.56434757 0.63335174 0.399613   0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.5296812  0.6131782  0.7051957  0.40175548 0.33      ], Reward: 2.0\n",
            "Processed Log → State: [0.5621821  0.64554363 0.7234967  0.29744643 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.6396779  0.6036899  0.84141725 0.32514015 0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.6078092  0.65486467 0.8156819  0.29231575 0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.6464348  0.69442505 0.73000294 0.27421468 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.5722154  0.7113598  0.73171955 0.24329107 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.5644456  0.73850656 0.6729472  0.2587988  0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.5807674  0.7846971  0.66357577 0.2961987  0.54      ], Reward: 0.0\n",
            "Processed Log → State: [0.61367685 0.8260773  0.6310684  0.2939066  0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.64861023 0.85156655 0.5975385  0.3162296  0.6       ], Reward: 6.0\n",
            "Processed Log → State: [0.7288974  0.86341417 0.5888318  0.29706886 0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.80413973 0.8313448  0.535347   0.33453104 0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.7293945  0.78665525 0.6422204  0.34939528 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.7578123  0.738974   0.61145306 0.36006528 0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.7245944  0.7468683  0.62977886 0.32638472 0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.786917   0.7522516  0.7255822  0.30133992 0.78      ], Reward: 0.0\n",
            "Processed Log → State: [0.79200554 0.6823339  0.8050933  0.3447512  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.80415684 0.74075586 0.8407422  0.28098807 0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.9004909  0.7264216  0.88174736 0.26998577 0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.9034819  0.7277797  0.87431645 0.2755138  0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.30371287 0.34320775 0.05930375 0.5292682  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.3216327  0.30950502 0.11095115 0.4647495  0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.37433362 0.42841634 0.13866162 0.59091145 0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.41612864 0.49265575 0.15479082 0.5435657  0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.4321679  0.5438615  0.113244   0.58273834 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.3910121  0.58068854 0.14390653 0.4984072  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.39765176 0.6677421  0.10164988 0.55301315 0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.34819087 0.63370836 0.18200926 0.5462113  0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.32559368 0.681145   0.17161427 0.56776917 0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.32483903 0.6861591  0.18049312 0.55970746 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.35888588 0.75545883 0.18575028 0.4754046  0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.4480388  0.87348187 0.17472649 0.4981747  0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.5218727  0.94716257 0.15596484 0.4384578  0.39      ], Reward: 5.0\n",
            "Processed Log → State: [0.5457768  0.9565363  0.13313362 0.40101    0.42      ], Reward: -2.0\n",
            "Processed Log → State: [0.6080413  0.96705323 0.22705327 0.4063558  0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.6456035  0.9867427  0.2241823  0.44116944 0.48      ], Reward: 6.0\n",
            "Processed Log → State: [0.6319693  1.         0.20920938 0.42186242 0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.6615775  0.9708278  0.26296556 0.37223387 0.54      ], Reward: 0.0\n",
            "Processed Log → State: [0.64365095 1.         0.24322674 0.4679298  0.57      ], Reward: 2.0\n",
            "Processed Log → State: [0.63586843 1.         0.23940435 0.50135213 0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.672126   1.         0.2516725  0.42663142 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.7117357  1.         0.2642392  0.41372165 0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.6694095  1.         0.2283387  0.47409084 0.69      ], Reward: 8.0\n",
            "Processed Log → State: [0.69024974 1.         0.24493349 0.47199377 0.72      ], Reward: 6.0\n",
            "Processed Log → State: [0.7483921  1.         0.3098123  0.42528874 0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.7545003  1.         0.25997558 0.4172348  0.78      ], Reward: 0.0\n",
            "Processed Log → State: [0.6664093  0.95828986 0.32857633 0.38976288 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.68476486 0.9778047  0.34655145 0.40768772 0.84      ], Reward: 7.0\n",
            "Processed Log → State: [0.68972987 1.         0.40104827 0.5164842  0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.72827244 0.99528444 0.38126692 0.48565203 0.9       ], Reward: 0.0\n",
            "Processed Log → State: [0.22663923 0.35337707 0.08753192 0.5820459  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.32313895 0.41312486 0.11982771 0.55485344 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.394927   0.45955482 0.19887757 0.47616646 0.09      ], Reward: 7.0\n",
            "Processed Log → State: [0.49661303 0.5166183  0.2431109  0.45751008 0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.5517739  0.5309019  0.21698944 0.37911832 0.15      ], Reward: -1.0\n",
            "Processed Log → State: [0.60141236 0.55895793 0.24387482 0.3956412  0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.6497469  0.5845913  0.266247   0.41486278 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.6704543  0.612688   0.32717544 0.34108326 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.6876212  0.6478302  0.2923716  0.37478533 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.7879476 0.6846032 0.28451   0.2995417 0.3      ], Reward: 6.0\n",
            "Processed Log → State: [0.8036824  0.7199287  0.39441484 0.30891487 0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.85239387 0.7154718  0.39229453 0.39714098 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.8725689  0.73145837 0.5038489  0.37514225 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.84205395 0.6998782  0.5418852  0.32424664 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.86003137 0.7271984  0.53872144 0.29827392 0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.83857876 0.7579335  0.53036344 0.29159012 0.48      ], Reward: 6.0\n",
            "Processed Log → State: [0.84163964 0.7644418  0.5355207  0.31341517 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.7666228  0.84300905 0.52450645 0.34856194 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.73599595 0.83802843 0.6016151  0.38016614 0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.7984766  0.8264392  0.5954329  0.34438512 0.6       ], Reward: 0.0\n",
            "Processed Log → State: [0.90460557 0.83163625 0.6266894  0.39377674 0.63      ], Reward: 7.0\n",
            "Processed Log → State: [0.95583093 0.7773824  0.61609375 0.43151966 0.66      ], Reward: 7.0\n",
            "Processed Log → State: [1.         0.74732906 0.5694881  0.35107684 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.71834123 0.634275   0.36335298 0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.97021395 0.76223207 0.62336487 0.29102275 0.75      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.7470052  0.69135493 0.2736579  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.9284437  0.72500783 0.61113954 0.2402269  0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.94006205 0.7566284  0.6144613  0.26156035 0.84      ], Reward: 4.0\n",
            "Processed Log → State: [0.9200809  0.7619575  0.65330017 0.34973288 0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.9377568  0.8065765  0.59108835 0.36064148 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.17764    0.33864778 0.05823341 0.5281202  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.16777003 0.31983367 0.05690701 0.46977913 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.14508161 0.3535346  0.07139733 0.52826095 0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.14889859 0.34029454 0.07136644 0.5462221  0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.13497427 0.3494109  0.07171465 0.54876286 0.15      ], Reward: 2.0\n",
            "Processed Log → State: [0.13561682 0.3610288  0.0672847  0.6207222  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.15386388 0.3518824  0.06077393 0.63559496 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.1688548  0.3436529  0.06951672 0.6427222  0.24      ], Reward: -2.0\n",
            "Processed Log → State: [0.18563032 0.35403976 0.0719652  0.72163665 0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.21137407 0.36913082 0.07224458 0.7607253  0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.24226262 0.39139244 0.08108971 0.8864211  0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.23368791 0.38236502 0.08493596 0.8284515  0.36      ], Reward: 7.0\n",
            "Processed Log → State: [0.21925788 0.37214625 0.08927892 0.90192163 0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.26829478 0.40778998 0.08479703 0.85954934 0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.25958598 0.39928168 0.10018834 0.8568291  0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.27469563 0.40681764 0.09701113 0.9295677  0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.2704577  0.44738346 0.08982632 0.9352133  0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.32102102 0.4728585  0.08260911 1.         0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.34520152 0.49162188 0.09429655 1.         0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.3612886 0.5250456 0.0974232 1.        0.6      ], Reward: 3.0\n",
            "Processed Log → State: [0.36441305 0.53091633 0.10690728 0.96216875 0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.3667237  0.54457396 0.09964581 0.8827809  0.66      ], Reward: 5.0\n",
            "Processed Log → State: [0.38712707 0.55403477 0.1030912  0.8627877  0.69      ], Reward: 0.0\n",
            "Processed Log → State: [0.38471127 0.5469553  0.09459008 0.91439164 0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.37420586 0.5838627  0.10890498 0.9735652  0.75      ], Reward: 6.0\n",
            "Processed Log → State: [0.36368182 0.6050225  0.10757553 0.9596458  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.35627365 0.61336994 0.13470365 0.9585651  0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.3735857  0.6289163  0.16017298 0.97075915 0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.35208473 0.63553077 0.16811147 0.9488914  0.87      ], Reward: 2.0\n",
            "Processed Log → State: [0.3641545  0.646058   0.17253628 1.         0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.25392225 0.36266908 0.         0.6298217  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.30975354 0.4198665  0.10296695 0.6428488  0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.30453023 0.4863315  0.21563612 0.63268197 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.23954077 0.50294423 0.35720065 0.6403478  0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.21501632 0.57843643 0.44660944 0.71779495 0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.19951524 0.5935137  0.44387993 0.68643177 0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.2001039  0.67078364 0.50378644 0.7363511  0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.20250347 0.7511129  0.5355917  0.6856106  0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.34022436 0.82243854 0.5203339  0.6736063  0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.44666052 0.7891523  0.55142534 0.6044252  0.3       ], Reward: 5.0\n",
            "Processed Log → State: [0.56996244 0.7595227  0.5725104  0.6198899  0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.5947595  0.71675736 0.59805757 0.59014386 0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.7104157  0.73980314 0.6604172  0.5529249  0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.75371325 0.7380475  0.6311123  0.5113906  0.42      ], Reward: 7.0\n",
            "Processed Log → State: [0.7442396  0.76012653 0.7124581  0.6036148  0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.81938195 0.7915932  0.7003151  0.5390796  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.78895926 0.8069082  0.7688167  0.52348083 0.51      ], Reward: 2.0\n",
            "Processed Log → State: [0.8678553  0.8086739  0.72944164 0.56860596 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.8837827  0.820931   0.6116744  0.58238864 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.8693052  0.8000205  0.60790586 0.5691983  0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.8596721  0.7918039  0.54596555 0.5680814  0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.8263617  0.78311145 0.5944815  0.52689123 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.7942378  0.8256311  0.61092347 0.4764017  0.69      ], Reward: 8.0\n",
            "Processed Log → State: [0.7488071  0.83258086 0.55628633 0.52005124 0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.7625655  0.8241682  0.594905   0.49287513 0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.73129696 0.8363328  0.5748189  0.45825776 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.82184213 0.8430352  0.62149644 0.4323869  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.8668728  0.8336398  0.60289645 0.5368791  0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.83707947 0.81713694 0.6314876  0.54677    0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.7954246  0.79511315 0.64747626 0.57880884 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.2842205  0.37040976 0.05509551 0.5826492  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.2627836  0.41664907 0.07040252 0.59546673 0.06      ], Reward: 7.0\n",
            "Processed Log → State: [0.24301636 0.50338423 0.05918282 0.6467555  0.09      ], Reward: 7.0\n",
            "Processed Log → State: [0.31308547 0.5013056  0.07812975 0.66313416 0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.32520995 0.5861497  0.13306174 0.6986014  0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.4643297  0.6009     0.18195386 0.6664451  0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.4859777  0.6304608  0.19497669 0.62253326 0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.55880874 0.6255199  0.21085218 0.5288716  0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.6097681  0.7708822  0.16915664 0.5785559  0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.6332903  0.84201247 0.16624382 0.5824421  0.3       ], Reward: 7.0\n",
            "Processed Log → State: [0.6259565 0.8972728 0.231011  0.4556182 0.33     ], Reward: -1.0\n",
            "Processed Log → State: [0.64318603 0.95758647 0.26873636 0.39358842 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.6433468  0.99991727 0.25932992 0.34538215 0.39      ], Reward: 0.0\n",
            "Processed Log → State: [0.76568085 1.         0.30037844 0.3141696  0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.8585841 0.9909464 0.2861352 0.279042  0.45     ], Reward: 8.0\n",
            "Processed Log → State: [0.81631446 0.9763287  0.30991054 0.31302163 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.8521468  1.         0.3401374  0.33324558 0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.80622566 0.9963772  0.3313255  0.37507653 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.77216667 1.         0.35626915 0.41088954 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.76761633 0.994727   0.33225563 0.5017989  0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.81325203 1.         0.33769774 0.5007128  0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.8373606 0.9183153 0.3128105 0.425133  0.66     ], Reward: 2.0\n",
            "Processed Log → State: [0.9008597  0.9936183  0.36695296 0.40448844 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.9047831  0.91362184 0.33707526 0.39324367 0.72      ], Reward: 5.0\n",
            "Processed Log → State: [0.8616154  0.9095102  0.34550616 0.31629643 0.75      ], Reward: 5.0\n",
            "Processed Log → State: [0.8620948 0.9036611 0.3989256 0.3144963 0.78     ], Reward: 6.0\n",
            "Processed Log → State: [0.86122155 0.9157791  0.38346902 0.2927133  0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.86587816 0.9331117  0.3700722  0.31549212 0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.90617174 0.9390548  0.3894309  0.3044109  0.87      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.92235696 0.371732   0.33445072 0.9       ], Reward: 6.0\n",
            "Processed Log → State: [0.31534985 0.3407882  0.05220589 0.49983037 0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.3456852  0.34763122 0.13219976 0.58700436 0.06      ], Reward: 5.0\n",
            "Processed Log → State: [0.4504197  0.37895867 0.15150467 0.5408591  0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.49224314 0.4029188  0.29780418 0.49053526 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.520298   0.45240733 0.39985797 0.42768332 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.5645548  0.4633546  0.37264195 0.41135782 0.18      ], Reward: 0.0\n",
            "Processed Log → State: [0.63103527 0.46398956 0.4155079  0.39154488 0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.67851436 0.4816828  0.49202925 0.41995832 0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.6636856  0.4934505  0.5793479  0.46628004 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.59652203 0.53669477 0.6157359  0.46062213 0.3       ], Reward: 2.0\n",
            "Processed Log → State: [0.7030869 0.5641419 0.755884  0.3843807 0.33     ], Reward: 3.0\n",
            "Processed Log → State: [0.7379861 0.5865783 0.8333018 0.3897904 0.36     ], Reward: 8.0\n",
            "Processed Log → State: [0.77075773 0.5880184  0.8112127  0.4107931  0.39      ], Reward: 7.0\n",
            "Processed Log → State: [0.77223825 0.65265805 0.83617586 0.35675982 0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.85018605 0.730776   0.8104451  0.4066303  0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.84477746 0.7655303  0.81577504 0.38167104 0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.85951775 0.76903605 0.73054075 0.29710636 0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.8679423  0.78035194 0.71483016 0.3562837  0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.9097239 0.7783861 0.7327379 0.3127873 0.57     ], Reward: 8.0\n",
            "Processed Log → State: [0.97530663 0.78745216 0.7271458  0.33467764 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.99051136 0.8143161  0.73332494 0.31903288 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.80139405 0.71006835 0.31871697 0.66      ], Reward: 7.0\n",
            "Processed Log → State: [1.         0.79609203 0.79726964 0.3121349  0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.91625506 0.7649151  0.73611903 0.29667425 0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.92911756 0.7931838  0.7140443  0.34202072 0.75      ], Reward: 6.0\n",
            "Processed Log → State: [0.95471734 0.78933245 0.68214834 0.39132598 0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.88035685 0.795876   0.77836025 0.34216782 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.9378215  0.7825537  0.77618957 0.34989545 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.8667872  0.73734343 0.36536878 0.87      ], Reward: 7.0\n",
            "Processed Log → State: [0.96994114 0.8740116  0.8517387  0.3738969  0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.26767403 0.35476565 0.07140479 0.48282832 0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.29367483 0.3216791  0.07124926 0.42377335 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.28986526 0.2895903  0.08894307 0.4433808  0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.35291076 0.251248   0.10737967 0.46816444 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.35505053 0.33699596 0.15439261 0.36834258 0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.38564205 0.37109196 0.17808446 0.24891411 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.49119183 0.41724914 0.18410957 0.1888193  0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.5750221  0.41135582 0.260936   0.21492219 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.5356544  0.4597361  0.32152882 0.1746218  0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.57326996 0.48727083 0.31051654 0.10650069 0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.60895294 0.56115687 0.3892531  0.00476302 0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.6431577  0.58057314 0.41030172 0.0039874  0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.7095866  0.603135   0.47515044 0.         0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.8063393 0.6018365 0.5847348 0.        0.42     ], Reward: 1.0\n",
            "Processed Log → State: [0.85965514 0.58058    0.58683693 0.11816759 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.89618737 0.6325359  0.5639446  0.023818   0.48      ], Reward: 2.0\n",
            "Processed Log → State: [0.8381217  0.6846283  0.5626947  0.05131702 0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.79505986 0.68945616 0.5185409  0.05212596 0.54      ], Reward: 0.0\n",
            "Processed Log → State: [0.73941576 0.7133567  0.52306145 0.14030942 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.8361761  0.78916246 0.58341235 0.30710372 0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.8888823  0.7561395  0.6655494  0.24032836 0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.9405703  0.76888126 0.7422289  0.2083714  0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.8920201  0.7425333  0.76808345 0.14740652 0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.8640511  0.75794    0.71811396 0.22386457 0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.93172616 0.74637717 0.7710928  0.2765777  0.75      ], Reward: 0.0\n",
            "Processed Log → State: [0.895024  0.7371682 0.8372909 0.3117448 0.78     ], Reward: 6.0\n",
            "Processed Log → State: [0.9150767  0.6670647  0.87059236 0.3243683  0.81      ], Reward: 5.0\n",
            "Processed Log → State: [0.80429953 0.7255277  0.9014674  0.3445639  0.84      ], Reward: 7.0\n",
            "Processed Log → State: [0.7407524  0.7765314  0.8311833  0.31332895 0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.7726762  0.78949875 0.86410534 0.36249444 0.9       ], Reward: -1.0\n",
            "Processed Log → State: [0.2505529  0.3234546  0.18853588 0.49355295 0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.36122072 0.3040407  0.20192254 0.40618357 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.4072944  0.34090906 0.25652823 0.33084738 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.45990154 0.41376758 0.22368942 0.34477118 0.12      ], Reward: 0.0\n",
            "Processed Log → State: [0.5395274  0.408614   0.20304242 0.34310722 0.15      ], Reward: 2.0\n",
            "Processed Log → State: [0.5763051  0.42523354 0.26589227 0.38425732 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.619241   0.42986855 0.33646184 0.37762854 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.6731915  0.49180648 0.40428984 0.35569373 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.6881102  0.53258073 0.40802616 0.3650871  0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.73642856 0.49348816 0.498268   0.39779007 0.3       ], Reward: -1.0\n",
            "Processed Log → State: [0.7036998  0.5334152  0.5922374  0.41436833 0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.6969503  0.55473024 0.590954   0.308966   0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.69714963 0.56355023 0.5737181  0.2519235  0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.69003993 0.5832583  0.5975325  0.248613   0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.7821478  0.6161285  0.68851507 0.26867732 0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.80719274 0.6280948  0.7249236  0.32541376 0.48      ], Reward: 2.0\n",
            "Processed Log → State: [0.8069537  0.63921475 0.8105414  0.38288367 0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.81297946 0.6474661  0.7530188  0.4679543  0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.73199797 0.60916424 0.73749393 0.37232104 0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.7065935 0.6512547 0.7477276 0.4875818 0.6      ], Reward: 7.0\n",
            "Processed Log → State: [0.6990961  0.7271854  0.74190617 0.606697   0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.71146816 0.73571205 0.7481017  0.5912864  0.66      ], Reward: -2.0\n",
            "Processed Log → State: [0.73243725 0.7440571  0.7582643  0.57519615 0.69      ], Reward: 8.0\n",
            "Processed Log → State: [0.8482309 0.7554629 0.7324848 0.5864148 0.72     ], Reward: 6.0\n",
            "Processed Log → State: [0.8532964  0.76208144 0.7911063  0.5305109  0.75      ], Reward: 6.0\n",
            "Processed Log → State: [0.89968616 0.7810313  0.83570164 0.5182321  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.98717546 0.7620366  0.8249917  0.55935127 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [1.        0.7380546 0.8243967 0.5135399 0.84     ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.75242656 0.8711489  0.5144376  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.75200444 0.803519   0.5764322  0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.24792852 0.34284037 0.06678056 0.6632969  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.33514747 0.33929968 0.11142518 0.55530256 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.3409152  0.40270203 0.17382537 0.49001116 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.43903425 0.41508746 0.23510157 0.39932206 0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.5010583  0.46741113 0.36237648 0.2328916  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.5887156  0.49308825 0.4464096  0.18653923 0.18      ], Reward: 2.0\n",
            "Processed Log → State: [0.74326634 0.5588039  0.4451681  0.15643133 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.73638916 0.5808616  0.45963764 0.0983246  0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.74497026 0.6666197  0.53166676 0.07674469 0.27      ], Reward: 5.0\n",
            "Processed Log → State: [0.7861531  0.6620254  0.58421606 0.12648988 0.3       ], Reward: 7.0\n",
            "Processed Log → State: [0.78018486 0.6511289  0.5862636  0.0585065  0.33      ], Reward: 4.0\n",
            "Processed Log → State: [0.7749815  0.6203782  0.64118326 0.03951192 0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.748569  0.6585426 0.6736647 0.        0.39     ], Reward: 8.0\n",
            "Processed Log → State: [0.87257206 0.66540474 0.68212736 0.         0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.8166621  0.66586995 0.8031984  0.07716715 0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.8034538  0.7083918  0.86564815 0.05567497 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.83815765 0.7330361  0.868071   0.09867144 0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.9141221  0.7610416  0.79918295 0.17946802 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.903012   0.7474074  0.8761875  0.24078871 0.57      ], Reward: 7.0\n",
            "Processed Log → State: [0.94942886 0.7629761  0.896123   0.26127288 0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.9968936  0.77088225 0.85465866 0.31911135 0.63      ], Reward: -1.0\n",
            "Processed Log → State: [1.         0.81279474 0.8154877  0.33836678 0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.97939503 0.79340494 0.83491856 0.33678675 0.69      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.7930396  0.883157   0.35401717 0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.9228965  0.8263856  0.89540225 0.37596393 0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.8884167 0.8430223 0.822384  0.2727593 0.78     ], Reward: 1.0\n",
            "Processed Log → State: [0.8587747  0.8633863  0.75116193 0.26699546 0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.8804908 0.8543758 0.7848238 0.2934995 0.84     ], Reward: 7.0\n",
            "Processed Log → State: [0.81718135 0.84175414 0.72877276 0.2872589  0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.7815615  0.81047386 0.66025877 0.32616195 0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.23808685 0.3191842  0.07338745 0.64884573 0.03      ], Reward: -1.0\n",
            "Processed Log → State: [0.24017014 0.31734926 0.08039735 0.7521333  0.06      ], Reward: 7.0\n",
            "Processed Log → State: [0.28599977 0.29040667 0.0943174  0.7127104  0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.29888666 0.32681766 0.0887135  0.7226445  0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.3149833  0.3506095  0.07906441 0.73634624 0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.32816812 0.37979928 0.08133898 0.7532576  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.37685424 0.37029472 0.11101482 0.78988546 0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.36946505 0.3634977  0.12825024 0.8037994  0.24      ], Reward: 0.0\n",
            "Processed Log → State: [0.42022154 0.40931824 0.13483088 0.87154186 0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.42273724 0.45378664 0.15099311 0.8755779  0.3       ], Reward: 2.0\n",
            "Processed Log → State: [0.42865083 0.45672068 0.16198316 0.85824496 0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.4586869  0.5226632  0.15731898 0.9020064  0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.4534939  0.5645905  0.16760258 0.9507282  0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.48581532 0.5494049  0.17658933 1.         0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.48592237 0.5681521  0.182718   1.         0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.51316905 0.5929634  0.1980548  1.         0.48      ], Reward: 6.0\n",
            "Processed Log → State: [0.5436838  0.59878826 0.19799262 1.         0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.54939276 0.62096447 0.20415205 0.98249257 0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.5838779  0.65443087 0.20644112 1.         0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.58761215 0.6679781  0.2002373  1.         0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.5817931 0.6649919 0.2113393 1.        0.63     ], Reward: 7.0\n",
            "Processed Log → State: [0.59659535 0.68212825 0.21856315 0.9599879  0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.5896064  0.69293314 0.22585927 1.         0.69      ], Reward: 5.0\n",
            "Processed Log → State: [0.5764562  0.7104661  0.22910978 1.         0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.5840965  0.64781684 0.20562473 1.         0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.6113396  0.6942172  0.20246096 1.         0.78      ], Reward: 0.0\n",
            "Processed Log → State: [0.64280254 0.67553437 0.2309262  1.         0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.6460897  0.6413231  0.24298726 0.98598576 0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.6459468  0.6528798  0.25090218 1.         0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.6562502  0.65672034 0.2606145  1.         0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.20340003 0.4275811  0.02925887 0.60521066 0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.28354105 0.44611403 0.05786915 0.58049023 0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.34702462 0.5810366  0.09874744 0.60937357 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.50979567 0.63056946 0.09587623 0.61006695 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.55554104 0.6800706  0.08993202 0.650758   0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.6485895  0.7740722  0.13767068 0.61350924 0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.70283955 0.8302152  0.1561117  0.6966033  0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.73424625 0.8572984  0.20126288 0.6636045  0.24      ], Reward: 0.0\n",
            "Processed Log → State: [0.79457706 0.8929706  0.20403388 0.55568445 0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.8225832  0.89721656 0.23697755 0.53899974 0.3       ], Reward: 7.0\n",
            "Processed Log → State: [0.8288953  0.9215237  0.26189563 0.6055269  0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.8769487  0.8735607  0.32957065 0.5862073  0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.87164205 0.8359623  0.34073836 0.53035015 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.81545806 0.32297403 0.4854501  0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.9559225  0.8259533  0.32400236 0.46964023 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.92742217 0.32966855 0.5624918  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [1.        0.9643053 0.2981791 0.5248408 0.51     ], Reward: 6.0\n",
            "Processed Log → State: [1.        1.        0.2992449 0.5267924 0.54     ], Reward: 3.0\n",
            "Processed Log → State: [0.97402155 0.9357918  0.30060512 0.50487316 0.57      ], Reward: 2.0\n",
            "Processed Log → State: [0.96278995 0.9573502  0.3385307  0.568304   0.6       ], Reward: -1.0\n",
            "Processed Log → State: [1.         1.         0.33642778 0.57671905 0.63      ], Reward: 2.0\n",
            "Processed Log → State: [1.         0.9987249  0.36825135 0.51909864 0.66      ], Reward: 0.0\n",
            "Processed Log → State: [1.        1.        0.3949892 0.544967  0.69     ], Reward: 8.0\n",
            "Processed Log → State: [1.         1.         0.3952775  0.49340189 0.72      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.95675004 0.39706066 0.46809673 0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.93868303 0.91269976 0.37963292 0.48604763 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.9511377  0.87877846 0.3929338  0.5236236  0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.9753986  0.89771503 0.3996868  0.58177745 0.84      ], Reward: 4.0\n",
            "Processed Log → State: [1.         0.9215128  0.43782192 0.5750536  0.87      ], Reward: 0.0\n",
            "Processed Log → State: [0.98569876 0.8552948  0.42118227 0.602411   0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.15379374 0.3480965  0.09827263 0.53895694 0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.09880964 0.4118644  0.13842127 0.51629424 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.1420938  0.44216618 0.17414252 0.48071274 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.16050126 0.47555876 0.17819716 0.5139471  0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.23335153 0.49394003 0.24660268 0.46686924 0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.3074812  0.541161   0.30012098 0.45340872 0.18      ], Reward: 2.0\n",
            "Processed Log → State: [0.26886207 0.55163604 0.33732173 0.41686848 0.21      ], Reward: 0.0\n",
            "Processed Log → State: [0.3184091  0.60322297 0.3797878  0.38839954 0.24      ], Reward: 7.0\n",
            "Processed Log → State: [0.27068737 0.6204531  0.40477052 0.3092708  0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.35116267 0.68643194 0.4164013  0.2794541  0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.4924637  0.73399127 0.47129446 0.15899429 0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.574165   0.75627446 0.45247066 0.14021488 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.54557455 0.792184   0.4702159  0.23427077 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.49633732 0.7504676  0.5013862  0.25342217 0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.5344218  0.7977458  0.43673477 0.18101554 0.45      ], Reward: 7.0\n",
            "Processed Log → State: [0.57007515 0.798009   0.50118375 0.1759014  0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.64482576 0.82636076 0.49534565 0.27516356 0.51      ], Reward: 2.0\n",
            "Processed Log → State: [0.61710477 0.8651634  0.4370624  0.29470873 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.6305344  0.9013071  0.509828   0.32527032 0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.6851644  0.8783302  0.52413625 0.32067013 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.7174155  0.9183669  0.60670155 0.3325465  0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.7593536  0.8729123  0.4511288  0.35689095 0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.78762025 0.8956029  0.4819939  0.2902492  0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.7827307 0.864987  0.4886579 0.2810924 0.72     ], Reward: 3.0\n",
            "Processed Log → State: [0.72788775 0.8632192  0.42292336 0.24958354 0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.6570418  0.864917   0.50407255 0.27980846 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.6761496 0.8571031 0.5482496 0.3150526 0.81     ], Reward: 7.0\n",
            "Processed Log → State: [0.70654905 0.8624219  0.6345619  0.33161107 0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.80583215 0.8476404  0.6851543  0.33547086 0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.78844154 0.9005037  0.7543032  0.30569842 0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.3056489  0.32828295 0.20679915 0.5234835  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.33383417 0.40378422 0.32633793 0.48230618 0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.34505063 0.47519487 0.33381587 0.5305695  0.09      ], Reward: 0.0\n",
            "Processed Log → State: [0.42624918 0.49788722 0.28910652 0.5748913  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.3887106  0.52739596 0.28778645 0.53646123 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.40960965 0.57889193 0.23143697 0.6764241  0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.42764634 0.61606175 0.26957318 0.67355394 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.46787587 0.68113405 0.30899295 0.6968666  0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.4283223  0.7042202  0.41134897 0.62882495 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.4727939  0.75524515 0.45555967 0.49979097 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.60282433 0.7936072  0.46940374 0.49318936 0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.7382208  0.8357502  0.43822518 0.48470655 0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.7224702  0.8057107  0.560679   0.41917443 0.39      ], Reward: 6.0\n",
            "Processed Log → State: [0.8019484  0.77995765 0.5417125  0.42516267 0.42      ], Reward: -1.0\n",
            "Processed Log → State: [0.8189646  0.7757158  0.587972   0.41011968 0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.86066145 0.7843712  0.6381839  0.4069557  0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.88267154 0.8230041  0.6873758  0.37742296 0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.902706   0.77918977 0.7407403  0.32555166 0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.8625731  0.80086404 0.8590366  0.3817358  0.57      ], Reward: 7.0\n",
            "Processed Log → State: [0.91059613 0.82983    0.8001992  0.39761424 0.6       ], Reward: 6.0\n",
            "Processed Log → State: [0.8535988  0.810665   0.75415194 0.4055968  0.63      ], Reward: 7.0\n",
            "Processed Log → State: [0.8389447  0.8077673  0.6516235  0.43645716 0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.90175825 0.8128751  0.6319094  0.41229725 0.69      ], Reward: 8.0\n",
            "Processed Log → State: [0.9393142  0.85988146 0.6774051  0.39294332 0.72      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.85657084 0.65665644 0.40035433 0.75      ], Reward: 6.0\n",
            "Processed Log → State: [0.9720452  0.86451745 0.6273992  0.40716144 0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.9649722  0.88023764 0.6974334  0.38395703 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.81467444 0.79608876 0.4447586  0.84      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.79897237 0.8470063  0.43052158 0.87      ], Reward: 2.0\n",
            "Processed Log → State: [1.         0.8315541  0.8387104  0.44714493 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.33082983 0.34384826 0.10038436 0.57503366 0.03      ], Reward: 0.0\n",
            "Processed Log → State: [0.31068826 0.36760196 0.13918342 0.5389364  0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.28248665 0.38131282 0.15682393 0.4704753  0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.34454387 0.3873339  0.14853188 0.4900176  0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.41610175 0.4733634  0.15519065 0.38806477 0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.3797604  0.40252066 0.17059766 0.35314256 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.42752346 0.3927758  0.17619883 0.35277098 0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.49944878 0.5757048  0.18506743 0.3562299  0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.5295685  0.60528654 0.14556625 0.32536823 0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.52356136 0.6136577  0.18302488 0.3296589  0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.47659558 0.648658   0.1693849  0.30197832 0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.4110238  0.69201165 0.18251766 0.38634777 0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.35556394 0.74751335 0.2293442  0.32663408 0.39      ], Reward: 6.0\n",
            "Processed Log → State: [0.27662346 0.85057247 0.24556506 0.39596242 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.3468418  0.8532432  0.22573037 0.33247522 0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.34075254 0.7841837  0.21073389 0.31063494 0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.42118216 0.79921305 0.23362596 0.34330273 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.396744   0.83835363 0.26430652 0.34854224 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.4251457  0.9576565  0.29360136 0.41905883 0.57      ], Reward: 2.0\n",
            "Processed Log → State: [0.43154824 0.9663311  0.2860731  0.51491505 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.46393365 0.9490428  0.2620142  0.47191343 0.63      ], Reward: 7.0\n",
            "Processed Log → State: [0.4422394  0.9511206  0.24632655 0.42459157 0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.43970442 0.94361085 0.22717196 0.5481227  0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.44064134 0.93585753 0.293837   0.56849605 0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.6202317  0.86723584 0.31981012 0.59566045 0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.7043496  0.9205166  0.3745561  0.57210773 0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.70388275 0.89834434 0.32437018 0.552634   0.81      ], Reward: 6.0\n",
            "Processed Log → State: [0.71382195 0.8812395  0.32726943 0.53429574 0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.71446043 0.9637705  0.36738715 0.522333   0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.60657024 0.9697529  0.3805     0.6029369  0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.24518993 0.36743054 0.04968599 0.65805495 0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.24056354 0.37472913 0.05186872 0.62866586 0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.18035816 0.4560079  0.10135759 0.6870746  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.22823793 0.52908707 0.12167948 0.71091604 0.12      ], Reward: -1.0\n",
            "Processed Log → State: [0.24778737 0.5333797  0.13077626 0.652356   0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.31682584 0.6438675  0.10356694 0.59549886 0.18      ], Reward: 5.0\n",
            "Processed Log → State: [0.37963584 0.7478728  0.152266   0.5632376  0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.29844224 0.85304826 0.0840179  0.54315853 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.30064702 0.85973626 0.12202674 0.51944685 0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.34744975 0.8907341  0.11786887 0.45257646 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.37689996 0.9595779  0.15473846 0.40915418 0.33      ], Reward: -1.0\n",
            "Processed Log → State: [0.37836945 0.9754783  0.16605055 0.3976375  0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.37849614 0.97135854 0.21510729 0.43420807 0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.45912966 1.         0.21349113 0.43640947 0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.50813067 1.         0.2228559  0.4681327  0.45      ], Reward: 6.0\n",
            "Processed Log → State: [0.5130686  1.         0.2611838  0.48051447 0.48      ], Reward: -1.0\n",
            "Processed Log → State: [0.562968   1.         0.29464218 0.37940994 0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.5489525  0.98499155 0.3501437  0.41200653 0.54      ], Reward: 0.0\n",
            "Processed Log → State: [0.5273673 1.        0.4202077 0.3741215 0.57     ], Reward: 5.0\n",
            "Processed Log → State: [0.54462415 0.94585663 0.46179947 0.37349    0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.48374635 0.90147936 0.46227637 0.3873664  0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.40312982 0.8180007  0.45160955 0.43544152 0.66      ], Reward: 0.0\n",
            "Processed Log → State: [0.4085991  0.8966314  0.4267098  0.49774557 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.61307335 0.86215985 0.437985   0.5130308  0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.6111578 0.869779  0.422085  0.562128  0.75     ], Reward: 2.0\n",
            "Processed Log → State: [0.54815006 0.8480792  0.3842338  0.5747513  0.78      ], Reward: 0.0\n",
            "Processed Log → State: [0.5860999  0.8383325  0.38943407 0.56645054 0.81      ], Reward: 5.0\n",
            "Processed Log → State: [0.53001136 0.8771797  0.36544758 0.6275572  0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.4617818  0.85746264 0.4096983  0.6879461  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.48805168 0.88757306 0.429775   0.6425892  0.9       ], Reward: 7.0\n",
            "Processed Log → State: [0.24130069 0.30739594 0.06163963 0.6867756  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.24869107 0.33145842 0.0566297  0.74890167 0.06      ], Reward: 0.0\n",
            "Processed Log → State: [0.2928246  0.33736145 0.07629903 0.81199676 0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.27830416 0.3630437  0.09003083 0.8979199  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.28608298 0.3691882  0.10119181 0.9475356  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.3094225  0.389119   0.10614768 0.9808746  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.3643294  0.43631303 0.12821268 1.         0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.38375545 0.47853187 0.14798656 1.         0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.38956025 0.49463132 0.16335173 1.         0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.44966024 0.5006503  0.17578883 1.         0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.4653055  0.49242103 0.18957154 0.971459   0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.47879052 0.5351878  0.18368396 0.99374455 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.49990112 0.5758735  0.18094145 1.         0.39      ], Reward: 5.0\n",
            "Processed Log → State: [0.4807257  0.61992776 0.17321698 1.         0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.5370016  0.63627225 0.17873271 1.         0.45      ], Reward: 6.0\n",
            "Processed Log → State: [0.541557   0.64829993 0.1772308  0.9738865  0.48      ], Reward: 5.0\n",
            "Processed Log → State: [0.5955195  0.6909066  0.18894926 0.95780265 0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.61470294 0.7316729  0.19988954 1.         0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.61883676 0.7407866  0.21119234 1.         0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.62650496 0.7360535  0.21073392 1.         0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.6257342  0.7299696  0.23011754 1.         0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.6095936  0.7170784  0.22190194 0.9893481  0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.61560726 0.734059   0.22776183 1.         0.69      ], Reward: -1.0\n",
            "Processed Log → State: [0.6243503  0.7491319  0.21868585 0.970956   0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.62247074 0.7417284  0.21930927 0.8899154  0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.6657641  0.73578984 0.24436252 0.9017199  0.78      ], Reward: 0.0\n",
            "Processed Log → State: [0.6950617  0.751709   0.27127087 0.8990888  0.81      ], Reward: 6.0\n",
            "Processed Log → State: [0.6999513  0.7253192  0.27723944 0.9430572  0.84      ], Reward: 0.0\n",
            "Processed Log → State: [0.7264693  0.720275   0.29981863 0.9239126  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.7217476  0.6832712  0.31413445 0.9695017  0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.1999564  0.31763774 0.13638155 0.5031321  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.2912622  0.3350321  0.11887443 0.4456638  0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.31616864 0.3354402  0.2657151  0.398486   0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.36576888 0.40712926 0.39818576 0.2952609  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.44119364 0.3816145  0.47743922 0.32729483 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.5289834  0.42208576 0.44129777 0.3416259  0.18      ], Reward: 6.0\n",
            "Processed Log → State: [0.5744681  0.45060068 0.44592533 0.19792973 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.6077983  0.502233   0.4860821  0.21907188 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.69088566 0.5853651  0.5725368  0.26095808 0.27      ], Reward: -1.0\n",
            "Processed Log → State: [0.6617998  0.58887565 0.6153921  0.26342508 0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.74867576 0.62100273 0.6549157  0.29088262 0.33      ], Reward: 7.0\n",
            "Processed Log → State: [0.7192178  0.63264847 0.7153313  0.39980793 0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.652518   0.67699283 0.7279641  0.29302785 0.39      ], Reward: 7.0\n",
            "Processed Log → State: [0.735178   0.6583513  0.7024416  0.29922503 0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.86395705 0.6492769  0.73114556 0.31868988 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.8127445 0.6866882 0.8248058 0.2550371 0.48     ], Reward: 5.0\n",
            "Processed Log → State: [0.7738061  0.6957257  0.86942154 0.28601578 0.51      ], Reward: -1.0\n",
            "Processed Log → State: [0.705471   0.755618   0.85648596 0.2901964  0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.76356107 0.79826224 0.8865683  0.36378595 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.8716388  0.788059   0.96537817 0.38451976 0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.9320253  0.83387434 0.9715274  0.41798052 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.8927956 0.8464783 1.        0.3390735 0.66     ], Reward: 3.0\n",
            "Processed Log → State: [0.9757116  0.8337375  0.99582916 0.30484912 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.92826146 0.8100999  0.991839   0.29976764 0.72      ], Reward: 5.0\n",
            "Processed Log → State: [0.96106917 0.8314034  0.97842866 0.30490193 0.75      ], Reward: 6.0\n",
            "Processed Log → State: [0.9449769 0.8805489 0.9600922 0.3522043 0.78     ], Reward: 3.0\n",
            "Processed Log → State: [0.95010203 0.8497561  0.96074456 0.3976037  0.81      ], Reward: 2.0\n",
            "Processed Log → State: [0.91648203 0.88090485 0.8754393  0.45115578 0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.9535285  0.8617523  0.82577765 0.50189525 0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.90370625 0.8426059  0.8713361  0.503788   0.9       ], Reward: 0.0\n",
            "Processed Log → State: [0.23052226 0.32245    0.06596341 0.59565705 0.03      ], Reward: 2.0\n",
            "Processed Log → State: [0.25865686 0.31592077 0.0891384  0.6161918  0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.29260823 0.3329512  0.0911729  0.66025245 0.09      ], Reward: 2.0\n",
            "Processed Log → State: [0.26505923 0.3502627  0.09685714 0.6856557  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.2928972  0.3587527  0.11203612 0.6636352  0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.33355474 0.39591148 0.12324511 0.66579896 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.3420432  0.39718902 0.12012624 0.7175401  0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.35325268 0.44840324 0.11463112 0.7556999  0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.3773897  0.50074166 0.11616741 0.85037524 0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.3734639  0.52533334 0.12998408 0.8894398  0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.3705458  0.5336031  0.11564892 0.90677804 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.3992417  0.55257124 0.1262007  0.862615   0.36      ], Reward: 7.0\n",
            "Processed Log → State: [0.41850385 0.574268   0.12745659 0.8975984  0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.43895844 0.59665096 0.1184753  0.9544987  0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.4489105  0.6020266  0.1334402  0.95772344 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.44192204 0.583911   0.14214277 0.9609238  0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.44303736 0.57900167 0.15214269 1.         0.51      ], Reward: 7.0\n",
            "Processed Log → State: [0.46251646 0.61668813 0.14450395 1.         0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.50111926 0.64225173 0.14621519 1.         0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.5065097  0.6490125  0.16174746 0.9455826  0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.51010126 0.6257213  0.16958593 0.9788972  0.63      ], Reward: 7.0\n",
            "Processed Log → State: [0.5547212  0.64912796 0.1633412  1.         0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.55985457 0.64112794 0.16633776 1.         0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.5605048  0.6736978  0.17429638 1.         0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.5502249  0.6950098  0.18277262 1.         0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.53834325 0.7112403  0.17840883 1.         0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.5221636  0.71116817 0.20511396 1.         0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.53131527 0.6950222  0.2290547  0.9934824  0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.54509294 0.7095099  0.22486888 0.9512595  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.528129   0.70223755 0.26140785 0.98666483 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.27293512 0.30528688 0.04177771 0.63584954 0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.34674388 0.2997091  0.10406154 0.625895   0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.37676603 0.31070146 0.16199185 0.55912477 0.09      ], Reward: 7.0\n",
            "Processed Log → State: [0.33356252 0.4025073  0.23499428 0.46767274 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.33760858 0.4415649  0.3652832  0.41202843 0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.4846552  0.4899095  0.45967928 0.3618896  0.18      ], Reward: 6.0\n",
            "Processed Log → State: [0.60470855 0.50964135 0.5271458  0.34073985 0.21      ], Reward: 0.0\n",
            "Processed Log → State: [0.7014538  0.54548204 0.5573667  0.39143583 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.74330395 0.53111774 0.56386435 0.29037336 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.65388536 0.5633398  0.5219619  0.20001937 0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.7148997  0.5878178  0.53313637 0.11894945 0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.759191   0.6093696  0.5701828  0.13063319 0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.7248925  0.67067945 0.55741817 0.17431064 0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.8045315  0.6652004  0.5373182  0.21360151 0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.8179046  0.6888605  0.5471122  0.24231215 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.85466945 0.72409123 0.6080844  0.24678993 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.8678406  0.7217054  0.5477484  0.16992986 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.90036166 0.7395468  0.6441342  0.11568418 0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.96941525 0.72726    0.72320664 0.16817959 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.9629509  0.74168444 0.75930065 0.14294367 0.6       ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.72381973 0.7334355  0.19105491 0.63      ], Reward: 7.0\n",
            "Processed Log → State: [1.         0.7685594  0.69630224 0.25063714 0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.9728875  0.7634364  0.6622872  0.25300214 0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.9696494  0.8034922  0.66134244 0.27795288 0.72      ], Reward: 5.0\n",
            "Processed Log → State: [0.9846074  0.8012484  0.58132714 0.2846921  0.75      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.75784755 0.58219594 0.18432091 0.78      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.79046905 0.5709538  0.17411633 0.81      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.78875184 0.6596442  0.25227576 0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.96219784 0.74991405 0.659335   0.23797083 0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.983675   0.7550615  0.69354767 0.18896195 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.30456993 0.3843108  0.16050461 0.5778893  0.03      ], Reward: 5.0\n",
            "Processed Log → State: [0.29936743 0.46194255 0.3036248  0.5906217  0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.24426019 0.49829552 0.35183996 0.615061   0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.26788077 0.5386659  0.4819247  0.55112106 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.3343666  0.64617366 0.4518068  0.5588672  0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.3923091  0.6233271  0.49391353 0.4933587  0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.5245251  0.6651693  0.51805574 0.43941423 0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.61032945 0.72578204 0.5451746  0.33297983 0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.6522907  0.6993183  0.6258418  0.29675013 0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.7175578  0.6922648  0.59392464 0.3511884  0.3       ], Reward: 0.0\n",
            "Processed Log → State: [0.73105794 0.70198166 0.57910645 0.32168263 0.33      ], Reward: 2.0\n",
            "Processed Log → State: [0.77355343 0.79503053 0.5971742  0.2451435  0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.82015896 0.81121665 0.62932986 0.1788787  0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.8304079  0.80448484 0.6237711  0.15484248 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.8082392  0.78666866 0.5888489  0.15861353 0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.9426794  0.7894786  0.6976555  0.23947723 0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.9070776  0.76949745 0.71273196 0.19856662 0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.92410517 0.7832219  0.6867441  0.2750846  0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.91551834 0.7719573  0.77056724 0.21883027 0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.978715   0.7800806  0.7507144  0.19938341 0.6       ], Reward: 0.0\n",
            "Processed Log → State: [0.98093146 0.7379054  0.7366442  0.14745925 0.63      ], Reward: 5.0\n",
            "Processed Log → State: [1.         0.7309406  0.7303812  0.13659573 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.7371481  0.72062886 0.15855828 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.77128315 0.6870841  0.18578357 0.72      ], Reward: 2.0\n",
            "Processed Log → State: [1.         0.7399272  0.65321934 0.2669437  0.75      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.7887984  0.648074   0.17353654 0.78      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.7524319  0.6954029  0.19982176 0.81      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.7665566  0.64047384 0.20098639 0.84      ], Reward: 7.0\n",
            "Processed Log → State: [0.97020245 0.734051   0.67385983 0.25900155 0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.8906453  0.7338615  0.699235   0.24836276 0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.20949942 0.32425684 0.0475658  0.63077956 0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.24562575 0.35092077 0.04247823 0.6629257  0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.23543194 0.3769437  0.03909945 0.6738241  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.29567865 0.4045095  0.03260775 0.6781001  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.3492471  0.4248598  0.03666452 0.75957483 0.15      ], Reward: 6.0\n",
            "Processed Log → State: [0.3977507  0.44993418 0.04634147 0.8225826  0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.44955418 0.46370223 0.04293937 0.93012    0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.4545655  0.4828669  0.05845196 0.9771089  0.24      ], Reward: 7.0\n",
            "Processed Log → State: [0.4941462  0.50586885 0.07823094 1.         0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.54281706 0.5546946  0.07773047 1.         0.3       ], Reward: 0.0\n",
            "Processed Log → State: [0.5402722 0.5870901 0.0888172 1.        0.33     ], Reward: 5.0\n",
            "Processed Log → State: [0.53286165 0.62874043 0.10247775 0.9989539  0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.5546095  0.6715267  0.10894946 1.         0.39      ], Reward: 6.0\n",
            "Processed Log → State: [0.5549998  0.6937831  0.11593771 0.9925687  0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.58227235 0.70247877 0.11626236 0.99213177 0.45      ], Reward: 6.0\n",
            "Processed Log → State: [0.6238379  0.71320355 0.14015704 1.         0.48      ], Reward: 6.0\n",
            "Processed Log → State: [0.6343009  0.7102369  0.13599724 0.98198587 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.6373226 0.7263542 0.1536817 1.        0.54     ], Reward: 5.0\n",
            "Processed Log → State: [0.65755403 0.74843234 0.1780723  1.         0.57      ], Reward: 0.0\n",
            "Processed Log → State: [0.67408067 0.7785858  0.1688743  1.         0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.6789984  0.77375007 0.18070194 1.         0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.6575329  0.76255065 0.17718497 1.         0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.6096961  0.74531895 0.18743747 1.         0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.5908951  0.7303878  0.19977298 1.         0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.6265599  0.7485431  0.20204578 1.         0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.6232573  0.76190025 0.1875865  1.         0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.61522764 0.7829236  0.20455895 1.         0.81      ], Reward: 0.0\n",
            "Processed Log → State: [0.61021006 0.7434136  0.23037331 0.99202335 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.6002803  0.75869465 0.24927448 0.9967518  0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.62024367 0.7005683  0.269971   1.         0.9       ], Reward: 6.0\n",
            "Processed Log → State: [0.217098   0.38111964 0.10900863 0.5765228  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.25584748 0.41374484 0.17419872 0.57734054 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.29791033 0.46907043 0.21497925 0.60941154 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.3185686  0.5686983  0.21015245 0.611107   0.12      ], Reward: -1.0\n",
            "Processed Log → State: [0.46063367 0.6039315  0.20634702 0.52404165 0.15      ], Reward: 6.0\n",
            "Processed Log → State: [0.44286957 0.6360733  0.26651892 0.62969786 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.49175718 0.71842223 0.31441852 0.5847975  0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.46492273 0.7869273  0.3301702  0.5575873  0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.41299117 0.81714857 0.34250578 0.45482877 0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.42711672 0.8359885  0.38110644 0.46357867 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.4880017  0.8691215  0.41649792 0.41000044 0.33      ], Reward: 2.0\n",
            "Processed Log → State: [0.5111298  0.9239778  0.33963886 0.445233   0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.4713678  0.9724498  0.31298622 0.46998262 0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.5018173  1.         0.37281147 0.45620102 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.492263   0.98000324 0.39318365 0.45895466 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.5765662  0.9328672  0.36136395 0.51322705 0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.5687275  0.8858041  0.37740394 0.42400017 0.51      ], Reward: 5.0\n",
            "Processed Log → State: [0.64095044 0.93175656 0.4020574  0.3825095  0.54      ], Reward: 7.0\n",
            "Processed Log → State: [0.72655815 0.9947875  0.37554726 0.5017205  0.57      ], Reward: 5.0\n",
            "Processed Log → State: [0.6852382  0.96692395 0.41606474 0.51227677 0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.67667556 0.9993261  0.40822414 0.49511895 0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.64432174 1.         0.4400306  0.4923823  0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.76452386 1.         0.4827905  0.46347523 0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.8379919  0.9834147  0.47802806 0.4756863  0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.77846736 0.96379954 0.49289557 0.55619097 0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.8368811  1.         0.43924734 0.6210409  0.78      ], Reward: 6.0\n",
            "Processed Log → State: [0.8461886 1.        0.4240879 0.5799493 0.81     ], Reward: 0.0\n",
            "Processed Log → State: [0.8596759  0.98930895 0.47313213 0.5693625  0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.8405351  0.9402598  0.44501802 0.64578265 0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.83441424 1.         0.4040917  0.6455419  0.9       ], Reward: 6.0\n",
            "Processed Log → State: [0.18975312 0.27889284 0.05733036 0.62627715 0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.21487498 0.29598957 0.05960551 0.63110423 0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.2568753  0.2870833  0.05971226 0.6916402  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.28594676 0.2608644  0.05607238 0.6764761  0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.32703528 0.26750523 0.04812714 0.6187858  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.30024546 0.2819679  0.03773601 0.62820977 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.30146542 0.28335848 0.04305256 0.73141766 0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.3281053  0.30806917 0.06007227 0.6687897  0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.3477264  0.33918142 0.05949704 0.6418801  0.27      ], Reward: 5.0\n",
            "Processed Log → State: [0.39574766 0.37647194 0.06756334 0.7263724  0.3       ], Reward: 0.0\n",
            "Processed Log → State: [0.39515284 0.39919263 0.07730986 0.7040256  0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.4030004  0.40074703 0.07492653 0.75239414 0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.39944375 0.38835633 0.09024575 0.8145464  0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.4220109  0.38585788 0.09114903 0.8817138  0.42      ], Reward: 4.0\n",
            "Processed Log → State: [0.42975256 0.37186694 0.09910896 0.94977313 0.45      ], Reward: 0.0\n",
            "Processed Log → State: [0.40797406 0.38457027 0.09616293 0.9711496  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.41750902 0.4107452  0.10256647 1.         0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.44893587 0.43039376 0.08962628 1.         0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.43111825 0.47879565 0.08527773 1.         0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.41991583 0.48143476 0.07517081 1.         0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.43085194 0.47046483 0.0885528  0.960426   0.63      ], Reward: 2.0\n",
            "Processed Log → State: [0.42549363 0.46927968 0.0833778  0.9089897  0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.42456326 0.5136763  0.08239733 0.9522015  0.69      ], Reward: 8.0\n",
            "Processed Log → State: [0.4429249  0.5156139  0.10572121 0.9876417  0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.43136898 0.52937096 0.09711101 0.98593897 0.75      ], Reward: 4.0\n",
            "Processed Log → State: [0.46237853 0.52702165 0.0871449  1.         0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.4801929 0.5490347 0.1210558 1.        0.81     ], Reward: 5.0\n",
            "Processed Log → State: [0.47715294 0.58705455 0.12802282 0.99042225 0.84      ], Reward: 0.0\n",
            "Processed Log → State: [0.48837596 0.5808438  0.15451115 1.         0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.5147579  0.5806458  0.160773   0.97041327 0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.2814103  0.4024939  0.10888749 0.6243406  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.23915254 0.47969428 0.15702647 0.5741391  0.06      ], Reward: 4.0\n",
            "Processed Log → State: [0.28020403 0.4716678  0.1748214  0.6154156  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.38052028 0.5183134  0.1944737  0.63404244 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.5007761  0.55544925 0.19454949 0.61089665 0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.55860776 0.6598867  0.22817412 0.5979404  0.18      ], Reward: 0.0\n",
            "Processed Log → State: [0.5402999  0.68557894 0.24745116 0.6989299  0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.5794331  0.76244134 0.249998   0.60914105 0.24      ], Reward: 0.0\n",
            "Processed Log → State: [0.6234519  0.8217161  0.26334363 0.5392386  0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.7187862  0.89348966 0.2673057  0.49807075 0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.71459025 0.9421649  0.3040716  0.5908432  0.33      ], Reward: 2.0\n",
            "Processed Log → State: [0.77748865 1.         0.25498387 0.6205211  0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.77271533 1.         0.2989788  0.6051302  0.39      ], Reward: 0.0\n",
            "Processed Log → State: [0.81245714 1.         0.31168854 0.5713158  0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.7850904  1.         0.31611463 0.5214492  0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.78672165 1.         0.2858165  0.4557305  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.8217975 1.        0.2981347 0.3936752 0.51     ], Reward: 8.0\n",
            "Processed Log → State: [0.83755046 1.         0.31623515 0.40459296 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.92547196 1.         0.30219278 0.34512648 0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.9081786  1.         0.27007496 0.3105593  0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.89041984 0.9650697  0.26409423 0.32059392 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.97224844 0.9860218  0.27742952 0.34315145 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.85883904 1.         0.2855336  0.41393337 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.91530085 0.9715849  0.30008218 0.3569617  0.72      ], Reward: 5.0\n",
            "Processed Log → State: [0.84272    0.9688203  0.34158984 0.36544478 0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.8420886  1.         0.38982737 0.30672663 0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.875979   1.         0.40519756 0.3408749  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.92130554 1.         0.40082362 0.42026824 0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.9681389 1.        0.4298493 0.4375397 0.87     ], Reward: 3.0\n",
            "Processed Log → State: [0.89860106 1.         0.4520154  0.4098547  0.9       ], Reward: 0.0\n",
            "Processed Log → State: [0.28855538 0.3072232  0.07294955 0.62472653 0.03      ], Reward: 5.0\n",
            "Processed Log → State: [0.19107223 0.3085216  0.08605827 0.52882004 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.11412029 0.35026175 0.09152912 0.5199965  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.24716881 0.48935774 0.10216602 0.5683931  0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.39422548 0.51583034 0.12437072 0.51790595 0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.48809937 0.52892804 0.16645357 0.48188892 0.18      ], Reward: 5.0\n",
            "Processed Log → State: [0.5499093  0.56648886 0.18512307 0.47706103 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.6435961  0.60202485 0.17934813 0.5196075  0.24      ], Reward: 7.0\n",
            "Processed Log → State: [0.67349684 0.688656   0.23147623 0.4362946  0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.6643631  0.7921795  0.21619625 0.34068686 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.6670606  0.8938787  0.26234466 0.3549359  0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.71530557 0.91379076 0.33986366 0.43796456 0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.69948304 0.9066301  0.32441482 0.3611448  0.39      ], Reward: 0.0\n",
            "Processed Log → State: [0.7531719  0.9261005  0.31512627 0.38999182 0.42      ], Reward: -1.0\n",
            "Processed Log → State: [0.7642903  0.8902554  0.29721573 0.3918227  0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.7314483 0.9173875 0.2828838 0.3804664 0.48     ], Reward: 7.0\n",
            "Processed Log → State: [0.78638774 0.95394945 0.2733644  0.37249947 0.51      ], Reward: 7.0\n",
            "Processed Log → State: [0.8021347  0.99841446 0.27493957 0.39620015 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.7600474  0.96886766 0.24792194 0.3170905  0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.8080421 0.8644619 0.2594063 0.3108024 0.6      ], Reward: 2.0\n",
            "Processed Log → State: [0.89333373 0.89179045 0.3038892  0.35716555 0.63      ], Reward: 6.0\n",
            "Processed Log → State: [0.9232679  0.9339526  0.33963788 0.36337546 0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.86114043 1.         0.32952732 0.32649198 0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.8918229  1.         0.33768293 0.37527224 0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.93165463 1.         0.30838335 0.40487504 0.75      ], Reward: 6.0\n",
            "Processed Log → State: [0.88376707 0.9966418  0.32317433 0.42983192 0.78      ], Reward: -2.0\n",
            "Processed Log → State: [0.89791185 1.         0.3691998  0.4293713  0.81      ], Reward: 0.0\n",
            "Processed Log → State: [0.96620363 0.9268192  0.38536817 0.37928408 0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.9797561  1.         0.3786425  0.44724715 0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.9629967  1.         0.44666705 0.48488614 0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.25349352 0.30111182 0.06122577 0.66186243 0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.26509693 0.3185932  0.08372681 0.6810875  0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.27321032 0.35777918 0.1056416  0.6375758  0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.28050813 0.35755306 0.1167993  0.6409597  0.12      ], Reward: 0.0\n",
            "Processed Log → State: [0.29943752 0.36783808 0.1441875  0.72482556 0.15      ], Reward: 2.0\n",
            "Processed Log → State: [0.2935784  0.39305407 0.1582299  0.72827107 0.18      ], Reward: 6.0\n",
            "Processed Log → State: [0.32398692 0.4479404  0.16463883 0.7452122  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.36413938 0.46982804 0.17290287 0.8608168  0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.3907777  0.5162465  0.18608503 0.93174875 0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.41452974 0.5290031  0.21258585 0.9762775  0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.39305362 0.5546082  0.21234126 1.         0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.41223976 0.597418   0.20828305 1.         0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.42356175 0.6253998  0.20751864 1.         0.39      ], Reward: 5.0\n",
            "Processed Log → State: [0.48399022 0.6148683  0.20610194 1.         0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.497351   0.67215455 0.20260698 1.         0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.5311907  0.697809   0.21281041 1.         0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.5310534  0.7244593  0.21922699 1.         0.51      ], Reward: 2.0\n",
            "Processed Log → State: [0.52386785 0.7059583  0.23479554 1.         0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.5684627  0.74700344 0.22280978 0.9707953  0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.57070875 0.7889248  0.22900295 0.9766986  0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.55627567 0.8243106  0.23926216 1.         0.63      ], Reward: 6.0\n",
            "Processed Log → State: [0.5829003  0.80848044 0.24075203 1.         0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.5951601  0.7833347  0.23835178 1.         0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.637045  0.7677598 0.2586162 1.        0.72     ], Reward: 7.0\n",
            "Processed Log → State: [0.6597614  0.76798195 0.26163387 1.         0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.64453226 0.7450741  0.254934   1.         0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.63851804 0.7439597  0.28267282 0.9931622  0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.6342612  0.7168084  0.30280134 1.         0.84      ], Reward: 0.0\n",
            "Processed Log → State: [0.6486549  0.69548416 0.31496185 0.9399856  0.87      ], Reward: 2.0\n",
            "Processed Log → State: [0.66805774 0.67840135 0.31712216 0.95489883 0.9       ], Reward: 7.0\n",
            "Processed Log → State: [0.23404564 0.30544472 0.05481775 0.6213211  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.2276966  0.31301522 0.05264764 0.6022061  0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.24747026 0.31370962 0.06644344 0.64131194 0.09      ], Reward: 0.0\n",
            "Processed Log → State: [0.2660787  0.34088627 0.08007599 0.71760035 0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.29602644 0.36156973 0.08891814 0.68583554 0.15      ], Reward: 7.0\n",
            "Processed Log → State: [0.3439633  0.3989523  0.10019201 0.7123641  0.18      ], Reward: 7.0\n",
            "Processed Log → State: [0.34444985 0.43153453 0.10285122 0.73041546 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.3631229  0.46191397 0.10489232 0.7072959  0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.36976594 0.50049454 0.11893424 0.6898499  0.27      ], Reward: -2.0\n",
            "Processed Log → State: [0.40079588 0.51174337 0.12613712 0.72252274 0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.3945328  0.49610558 0.12641367 0.82696384 0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.41013482 0.51898277 0.14197399 0.84133685 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.45124456 0.5031254  0.15080108 0.8702947  0.39      ], Reward: 0.0\n",
            "Processed Log → State: [0.4617272  0.5187067  0.1618463  0.91196895 0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.48935997 0.51242083 0.15525849 0.97509444 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.5205292  0.5248415  0.17256153 1.         0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.5373888 0.535101  0.1888994 0.9823749 0.51     ], Reward: 6.0\n",
            "Processed Log → State: [0.58795065 0.58220243 0.19638346 0.9942473  0.54      ], Reward: 4.0\n",
            "Processed Log → State: [0.602443   0.59788644 0.18605673 0.961898   0.57      ], Reward: 7.0\n",
            "Processed Log → State: [0.5908426  0.6300766  0.20017408 0.93708557 0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.6196316  0.66085917 0.20873843 0.92014337 0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.6005446  0.6666684  0.19850183 1.         0.66      ], Reward: 7.0\n",
            "Processed Log → State: [0.6170244  0.7084104  0.18832546 1.         0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.6094434  0.7041181  0.17795902 1.         0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.6045692  0.6926615  0.18201417 0.97888666 0.75      ], Reward: 5.0\n",
            "Processed Log → State: [0.61339    0.69039214 0.17355916 0.9615897  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.5855762  0.69581044 0.19121337 0.9154152  0.81      ], Reward: 6.0\n",
            "Processed Log → State: [0.56724685 0.70791566 0.20848495 0.9344154  0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.5898782  0.7053186  0.22096802 0.9613963  0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.6126528  0.6840031  0.21715005 1.         0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.22140475 0.32571483 0.03911595 0.57434314 0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.2292621  0.30590114 0.04358197 0.52506167 0.06      ], Reward: 5.0\n",
            "Processed Log → State: [0.22440347 0.28606233 0.06560415 0.56630254 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.23932642 0.30414718 0.06539965 0.6739076  0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.29084486 0.32490993 0.05685683 0.7648633  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.31634447 0.32984024 0.04897749 0.82021296 0.18      ], Reward: 6.0\n",
            "Processed Log → State: [0.32659477 0.34128606 0.03748842 0.77962005 0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.32566002 0.3693744  0.03688851 0.76548064 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.34812832 0.33776075 0.03687708 0.84713686 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.35660523 0.35812247 0.03826521 0.7891591  0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.36087784 0.3746305  0.04907045 0.8119795  0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.3615227  0.3601012  0.05179391 0.8147337  0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.3807487  0.39033055 0.0584586  0.82530886 0.39      ], Reward: 6.0\n",
            "Processed Log → State: [0.39707565 0.3905697  0.0775937  0.8385885  0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.41223535 0.41514274 0.07570464 0.82108986 0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.41201216 0.40367734 0.06921791 0.79245067 0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.41746733 0.4097426  0.0761542  0.79751927 0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.43584594 0.4302599  0.0643395  0.8607489  0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.46666932 0.45597035 0.06237016 0.9636444  0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.46663254 0.42276818 0.06847016 0.9369146  0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.47955963 0.39335632 0.07029624 0.89591336 0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.49803135 0.43408138 0.06867317 0.8426748  0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.47160593 0.45474365 0.07180328 0.8522918  0.69      ], Reward: 7.0\n",
            "Processed Log → State: [0.4660433  0.4498758  0.06986262 0.86095625 0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.5057123  0.45285025 0.06485873 0.75146097 0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.52406335 0.45128822 0.07610677 0.7236815  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.5279446  0.4487849  0.09208006 0.7724669  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.54279965 0.45434153 0.12280991 0.84419864 0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.5760666  0.45439613 0.13216823 0.8326261  0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.56286657 0.4385617  0.14528112 0.8633891  0.9       ], Reward: 6.0\n",
            "Processed Log → State: [0.19635704 0.34601656 0.06104456 0.6219831  0.03      ], Reward: -1.0\n",
            "Processed Log → State: [0.19077022 0.33500823 0.06043627 0.6656785  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.18581817 0.33240828 0.05998976 0.68093264 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.21420912 0.3281018  0.05865685 0.71624184 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.21683587 0.31084013 0.0446408  0.7645493  0.15      ], Reward: 0.0\n",
            "Processed Log → State: [0.21598671 0.33659425 0.04476592 0.8048408  0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.21137142 0.35578614 0.04359561 0.7964156  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.22707844 0.38260394 0.03800951 0.8261496  0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.25078616 0.399563   0.0456027  0.8191689  0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.28814787 0.37576953 0.04172653 0.86172295 0.3       ], Reward: 0.0\n",
            "Processed Log → State: [0.2637201  0.41855863 0.05216318 0.9555003  0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.28657842 0.4362523  0.0561492  0.9453224  0.36      ], Reward: 5.0\n",
            "Processed Log → State: [0.26212955 0.4626817  0.05168572 0.8590279  0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.29161495 0.49600562 0.03529136 0.86523354 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.31648135 0.50107473 0.03423521 0.9315022  0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.2885781  0.49460852 0.04889788 0.8847064  0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.2932039  0.5062321  0.07256222 0.8849582  0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.27874914 0.5161997  0.07225354 0.8507765  0.54      ], Reward: -2.0\n",
            "Processed Log → State: [0.281072   0.5256126  0.08158727 0.91851276 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.2593565  0.5499988  0.07641225 0.9334258  0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.30355975 0.56639516 0.06324241 0.9384911  0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.32530788 0.5652624  0.06338757 1.         0.66      ], Reward: 5.0\n",
            "Processed Log → State: [0.3471872  0.57278043 0.0590502  1.         0.69      ], Reward: 5.0\n",
            "Processed Log → State: [0.37536943 0.6095865  0.05748539 1.         0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.3804369 0.6268226 0.0683972 1.        0.75     ], Reward: 0.0\n",
            "Processed Log → State: [0.37027237 0.61663544 0.07004957 0.9642922  0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.342301  0.6003832 0.0816134 0.8850183 0.81     ], Reward: 8.0\n",
            "Processed Log → State: [0.31629196 0.6020333  0.0831377  0.9328856  0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.31491795 0.6182875  0.10925105 0.9324366  0.87      ], Reward: 2.0\n",
            "Processed Log → State: [0.35095695 0.6261226  0.12830791 0.99641144 0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.23247848 0.2983011  0.06066782 0.65609986 0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.255032   0.3324176  0.08629321 0.7166893  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.2889199  0.34149987 0.09997072 0.69239724 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.30505538 0.35080162 0.10842495 0.69928265 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.2691875  0.38609585 0.10667732 0.76303923 0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.32975286 0.38175377 0.10291582 0.79396844 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.32926384 0.3872304  0.1186896  0.8621075  0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.35343537 0.4208417  0.12359687 0.90824425 0.24      ], Reward: 7.0\n",
            "Processed Log → State: [0.35824668 0.4473057  0.11972591 0.93571454 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.4149239  0.4362711  0.12708801 0.9180011  0.3       ], Reward: 2.0\n",
            "Processed Log → State: [0.4354532  0.4390799  0.12900953 0.957619   0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.457119   0.43894088 0.14333788 0.95605737 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.49832034 0.46814904 0.15578969 0.9541704  0.39      ], Reward: 6.0\n",
            "Processed Log → State: [0.5418944  0.49615514 0.17448738 0.97230583 0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.52924144 0.5463486  0.17056212 1.         0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.50808555 0.54581654 0.17978962 1.         0.48      ], Reward: 7.0\n",
            "Processed Log → State: [0.5230207  0.5638866  0.17652223 1.         0.51      ], Reward: 8.0\n",
            "Processed Log → State: [0.51052105 0.5653818  0.18971394 1.         0.54      ], Reward: 5.0\n",
            "Processed Log → State: [0.5159413 0.6047672 0.2147322 0.933639  0.57     ], Reward: -1.0\n",
            "Processed Log → State: [0.51501334 0.67286927 0.23122598 0.8737811  0.6       ], Reward: 0.0\n",
            "Processed Log → State: [0.52954155 0.6722809  0.23180337 0.9090876  0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.5213972  0.69589823 0.22133908 0.9426928  0.66      ], Reward: 7.0\n",
            "Processed Log → State: [0.530195   0.71610224 0.215752   0.96884197 0.69      ], Reward: 5.0\n",
            "Processed Log → State: [0.5603714  0.74961144 0.21755229 0.9625737  0.72      ], Reward: 5.0\n",
            "Processed Log → State: [0.58128774 0.76132256 0.21784233 0.92643327 0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.5998862  0.73673135 0.22394204 0.974149   0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.64818    0.73172075 0.2431068  0.9493465  0.81      ], Reward: -1.0\n",
            "Processed Log → State: [0.6433379  0.73546124 0.26140782 0.96694386 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.62008166 0.7501225  0.26517037 0.97524315 0.87      ], Reward: 0.0\n",
            "Processed Log → State: [0.6361278  0.72416776 0.27510077 1.         0.9       ], Reward: 6.0\n",
            "Processed Log → State: [0.20420274 0.32854068 0.07187185 0.6130653  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.23891245 0.3426611  0.08439387 0.6665518  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.2719228  0.34265706 0.10123856 0.6393625  0.09      ], Reward: 0.0\n",
            "Processed Log → State: [0.31602916 0.35324937 0.09023346 0.69022477 0.12      ], Reward: 0.0\n",
            "Processed Log → State: [0.3772027 0.4238768 0.1107477 0.6710943 0.15     ], Reward: 5.0\n",
            "Processed Log → State: [0.39911598 0.40920845 0.12805547 0.7738088  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.37705618 0.4200505  0.1641819  0.7848469  0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.38955215 0.41488796 0.17985544 0.8291981  0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.42888892 0.46532488 0.17553112 0.8618901  0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.4565453  0.47935084 0.1749075  0.92056334 0.3       ], Reward: 5.0\n",
            "Processed Log → State: [0.42282507 0.48493493 0.19522698 0.962013   0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.42801583 0.5389099  0.1960283  1.         0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.4296372  0.5728704  0.22252053 0.9767356  0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.4690427 0.5974888 0.2413417 1.        0.42     ], Reward: 1.0\n",
            "Processed Log → State: [0.5104889  0.60265255 0.24488619 1.         0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.5449821  0.61161184 0.25773224 1.         0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.5772596 0.6492277 0.2688695 1.        0.51     ], Reward: 2.0\n",
            "Processed Log → State: [0.5678194  0.67282134 0.2653527  1.         0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.5675593  0.69318795 0.24938415 1.         0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.6085929  0.7104888  0.26346654 0.9813231  0.6       ], Reward: 5.0\n",
            "Processed Log → State: [0.59504277 0.7066996  0.25861675 0.9680666  0.63      ], Reward: 7.0\n",
            "Processed Log → State: [0.6029716  0.70134    0.25273508 1.         0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.60053426 0.7248518  0.24644168 0.9901389  0.69      ], Reward: 0.0\n",
            "Processed Log → State: [0.5850396  0.7314685  0.23231845 1.         0.72      ], Reward: 6.0\n",
            "Processed Log → State: [0.586003   0.75859123 0.23739478 0.9368137  0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.6269797  0.7558527  0.22895321 0.9544423  0.78      ], Reward: 6.0\n",
            "Processed Log → State: [0.62606686 0.7808541  0.24502301 0.9235573  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.6475134  0.7576054  0.2553058  0.93358076 0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.65598565 0.7621059  0.26116976 0.9132682  0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.66664594 0.74864066 0.2697656  0.85354984 0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.21149142 0.35729054 0.14092696 0.5490625  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.30370897 0.37448934 0.20896141 0.59753937 0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.4232458  0.4384621  0.21992533 0.55870706 0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.43086222 0.4423632  0.36312774 0.51013047 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.5251948  0.48626935 0.43221518 0.5225841  0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.58200336 0.5023819  0.38930476 0.43306652 0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.59949625 0.5377377  0.39685705 0.40879667 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.66122127 0.5459457  0.4824904  0.34728253 0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.68249196 0.57927877 0.5957595  0.26009968 0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.7714573  0.53601736 0.52590305 0.2743224  0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.77394795 0.5643173  0.6331903  0.2834596  0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.84402543 0.5712252  0.58091843 0.2748121  0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.8387258  0.5647935  0.70783055 0.24988262 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.96685493 0.54903036 0.7176677  0.2580062  0.42      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.5189745  0.70099723 0.2055385  0.45      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.54748404 0.7142003  0.16486621 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.5525797  0.74209404 0.1924289  0.51      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.61720455 0.7623413  0.16563757 0.54      ], Reward: 2.0\n",
            "Processed Log → State: [1.         0.6626731  0.6903058  0.13579072 0.57      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.6779735  0.6778159  0.17860594 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.6896518  0.72738075 0.19120033 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.67557645 0.7322851  0.23332803 0.66      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.7085567  0.7391018  0.26461866 0.69      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.6973117  0.7683672  0.24436149 0.72      ], Reward: 7.0\n",
            "Processed Log → State: [1.         0.7069732  0.68668824 0.30496106 0.75      ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.75445795 0.67350894 0.4234302  0.78      ], Reward: 6.0\n",
            "Processed Log → State: [0.95006955 0.7591508  0.66981995 0.48699954 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.944279   0.741515   0.6169305  0.52206886 0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.98558384 0.7885039  0.67638755 0.6145352  0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.8761596  0.81355053 0.62487614 0.6094451  0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.20523739 0.3211975  0.16003928 0.7015402  0.03      ], Reward: 5.0\n",
            "Processed Log → State: [0.25908232 0.36109498 0.1945303  0.72559845 0.06      ], Reward: 5.0\n",
            "Processed Log → State: [0.31402504 0.42693248 0.22761829 0.7239026  0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.37535277 0.37779036 0.3171272  0.67014587 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.31768513 0.4070662  0.36544597 0.5476115  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.36219954 0.42584    0.4087342  0.5614164  0.18      ], Reward: 0.0\n",
            "Processed Log → State: [0.4552154  0.46030083 0.48547375 0.5443994  0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.4356384  0.47622687 0.5564735  0.5204911  0.24      ], Reward: 0.0\n",
            "Processed Log → State: [0.46459    0.54085946 0.59171665 0.4643274  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.49969953 0.5630134  0.61267865 0.5000994  0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.4944368  0.60052764 0.6345452  0.47509763 0.33      ], Reward: 7.0\n",
            "Processed Log → State: [0.52227575 0.65428853 0.5515103  0.4883177  0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.5454573 0.683808  0.5943462 0.49583   0.39     ], Reward: 2.0\n",
            "Processed Log → State: [0.6212247  0.6829659  0.61499363 0.49177718 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.5779043  0.7477124  0.55509126 0.4487877  0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.5948668 0.7207695 0.5988022 0.4568152 0.48     ], Reward: 8.0\n",
            "Processed Log → State: [0.6362199  0.75979    0.69541204 0.53336704 0.51      ], Reward: 5.0\n",
            "Processed Log → State: [0.70610976 0.7653996  0.771623   0.4763965  0.54      ], Reward: 7.0\n",
            "Processed Log → State: [0.6516185  0.7909386  0.72938275 0.5586077  0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.65903103 0.79484063 0.73477364 0.53679854 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.6671634  0.81649536 0.7856937  0.6065414  0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.75085497 0.8006969  0.8148729  0.56135714 0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.77925354 0.8155391  0.8005951  0.49958295 0.69      ], Reward: 0.0\n",
            "Processed Log → State: [0.76731366 0.7961693  0.8217901  0.5744998  0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.81004584 0.8441323  0.75301635 0.5957329  0.75      ], Reward: 7.0\n",
            "Processed Log → State: [0.8289817  0.87752885 0.75576437 0.5811123  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.84930664 0.8996267  0.7724492  0.6099732  0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.8425828 0.8871102 0.8612809 0.6220245 0.84     ], Reward: 7.0\n",
            "Processed Log → State: [0.8646278  0.838112   0.89342034 0.6384719  0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.952521   0.83060455 0.90853226 0.57375973 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.25628427 0.32884708 0.07516761 0.63863146 0.03      ], Reward: 5.0\n",
            "Processed Log → State: [0.25020725 0.35257787 0.08391044 0.7216347  0.06      ], Reward: 5.0\n",
            "Processed Log → State: [0.24782081 0.3704309  0.10408075 0.7543536  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.26768866 0.37517527 0.10385363 0.8644126  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.28414527 0.46245804 0.11211798 0.86882675 0.15      ], Reward: 5.0\n",
            "Processed Log → State: [0.33873782 0.45717138 0.12156563 0.8854653  0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.3514148  0.49906787 0.14741153 0.94177246 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.3746444  0.54785717 0.15107273 0.9607777  0.24      ], Reward: 0.0\n",
            "Processed Log → State: [0.4067026  0.5883364  0.16511503 0.974923   0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.43173075 0.6170685  0.17864014 1.         0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.4461946 0.6411222 0.1920035 0.9948148 0.33     ], Reward: 2.0\n",
            "Processed Log → State: [0.43550396 0.6360962  0.18952936 1.         0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.4713936  0.68587095 0.18407261 0.96563184 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.4592952  0.7080153  0.2045579  0.95458424 0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.48518372 0.70143    0.20750564 0.9635102  0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.5075093  0.7207336  0.19944909 0.9373812  0.48      ], Reward: 7.0\n",
            "Processed Log → State: [0.52213454 0.70537364 0.21117769 0.9927051  0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.49922922 0.7172589  0.22175767 1.         0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.5189068  0.73625535 0.21430042 1.         0.57      ], Reward: 2.0\n",
            "Processed Log → State: [0.55796903 0.72726005 0.21841335 1.         0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.5610889  0.71839714 0.22818375 1.         0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.61545056 0.7306285  0.22972777 1.         0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.6148452  0.74462247 0.24237698 1.         0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.64673424 0.7393292  0.24311486 1.         0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.6588297  0.7300817  0.23390163 0.98356056 0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.67021334 0.72546226 0.24167179 1.         0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.68914515 0.72872025 0.26538217 0.9872027  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.66982347 0.73560405 0.26699626 1.         0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.6769342  0.7418226  0.28183177 1.         0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.6459533  0.73024476 0.29813433 0.91342103 0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.22576886 0.29378545 0.03123489 0.6276748  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.23864    0.31364664 0.01931777 0.6238573  0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.2680395  0.3353683  0.00114417 0.6523756  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.29517296 0.32241553 0.         0.66975623 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.32296354 0.33265    0.         0.7203201  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.34831923 0.39180523 0.         0.7805435  0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.351077   0.40901062 0.00869581 0.74307346 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.32552922 0.41947842 0.01012588 0.7821271  0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.3380982  0.4371253  0.00811997 0.83393383 0.27      ], Reward: 6.0\n",
            "Processed Log → State: [0.3443799  0.40784836 0.00637374 0.8415891  0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.35353297 0.39127994 0.01269095 0.8480811  0.33      ], Reward: 7.0\n",
            "Processed Log → State: [0.37313655 0.4039851  0.00545231 0.7730432  0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.41134784 0.41484982 0.00847615 0.8107634  0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.3773615  0.4182223  0.01167402 0.8194379  0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.38107702 0.41141176 0.00277479 0.89197826 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.3939597  0.3874333  0.02528027 0.8111456  0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.35742244 0.4288543  0.0487888  0.79155856 0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.35465795 0.43521523 0.04687095 0.77853906 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.3752799  0.4338942  0.05515099 0.76556456 0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.37079623 0.4576899  0.04925105 0.7357234  0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.37399215 0.47481936 0.06576348 0.79795796 0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.3933324  0.48428246 0.06152877 0.76283914 0.66      ], Reward: 5.0\n",
            "Processed Log → State: [0.39990073 0.4915382  0.06314217 0.76234215 0.69      ], Reward: 5.0\n",
            "Processed Log → State: [0.38861072 0.49740222 0.0635231  0.7340689  0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.3983372  0.49086195 0.07471219 0.7434851  0.75      ], Reward: 0.0\n",
            "Processed Log → State: [0.39847532 0.49820763 0.06941525 0.73459977 0.78      ], Reward: 6.0\n",
            "Processed Log → State: [0.39907974 0.5072276  0.09036713 0.7537635  0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.38226202 0.5407629  0.11058087 0.7297661  0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.38874388 0.54569405 0.13144176 0.79981315 0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.40019313 0.57047206 0.1546589  0.9263489  0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.16737214 0.27807793 0.05309033 0.6910392  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.14416164 0.31357628 0.05913125 0.81039935 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.1579826  0.3372525  0.05472905 0.9892847  0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.15296908 0.35663754 0.05070088 1.         0.12      ], Reward: 0.0\n",
            "Processed Log → State: [0.17409718 0.37311986 0.05748338 1.         0.15      ], Reward: 2.0\n",
            "Processed Log → State: [0.2307592  0.39673954 0.06092515 0.9751322  0.18      ], Reward: 2.0\n",
            "Processed Log → State: [0.27850965 0.41528937 0.05013185 0.9848965  0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.28422126 0.43900448 0.06320817 0.9627457  0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.29952005 0.4441628  0.04580628 0.99653816 0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.3127962  0.47237706 0.04472376 1.         0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.3719717  0.5209831  0.07185396 0.9956825  0.33      ], Reward: 5.0\n",
            "Processed Log → State: [0.40469342 0.5436571  0.07736635 1.         0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.41229007 0.5509301  0.08872899 1.         0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.44899884 0.55898315 0.08717741 0.9929959  0.42      ], Reward: 0.0\n",
            "Processed Log → State: [0.4455522  0.5710388  0.08945207 1.         0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.43599647 0.614503   0.0863836  0.98644376 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.4537029  0.6201151  0.07842363 1.         0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.45709628 0.6343948  0.06880286 0.9782379  0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.46521667 0.60866374 0.05629613 0.9609055  0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.47862774 0.625557   0.0486444  1.         0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.4528162  0.6376761  0.05862651 1.         0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.44754037 0.6423719  0.06896712 0.9077627  0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.44506773 0.6538396  0.07528342 0.9097973  0.69      ], Reward: 4.0\n",
            "Processed Log → State: [0.42830324 0.66051495 0.08152971 0.8387645  0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.41434148 0.68991005 0.08742257 0.8393951  0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.43208823 0.693821   0.0950847  0.84886813 0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.440686   0.70394117 0.10373346 0.74967813 0.81      ], Reward: 2.0\n",
            "Processed Log → State: [0.4626289  0.7352046  0.11128963 0.7754527  0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.47211352 0.7271747  0.13010731 0.8033404  0.87      ], Reward: 0.0\n",
            "Processed Log → State: [0.49762237 0.69855803 0.1639955  0.75526404 0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.19904877 0.3342413  0.04748109 0.6505484  0.03      ], Reward: 7.0\n",
            "Processed Log → State: [0.1950258  0.340925   0.04928861 0.721094   0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.2334675  0.35990706 0.06151048 0.7574568  0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.24473198 0.38381994 0.05234639 0.8236797  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.26879904 0.4095389  0.06155556 0.8276199  0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.27833638 0.45185623 0.06893975 0.87632185 0.18      ], Reward: 7.0\n",
            "Processed Log → State: [0.2814147  0.44724336 0.09702411 0.8269944  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.29993606 0.45986462 0.10036319 0.8650666  0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.30693957 0.49415904 0.10638487 0.90403914 0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.31382674 0.49937913 0.12872382 0.92943484 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.35119265 0.4832441  0.13032833 0.9735938  0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.37822306 0.48900035 0.149441   0.9566873  0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.39136717 0.5186052  0.14518802 0.9706012  0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.4087377  0.55284697 0.16515836 1.         0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.3887707  0.5936553  0.17293952 0.9621007  0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.42493993 0.6140389  0.17135467 0.994614   0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.44130048 0.6621315  0.18523318 0.9229812  0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.46207517 0.6662329  0.18416257 0.9529086  0.54      ], Reward: 2.0\n",
            "Processed Log → State: [0.49030504 0.7110014  0.17553756 0.95820075 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.53073627 0.727613   0.17043851 0.9995824  0.6       ], Reward: 7.0\n",
            "Processed Log → State: [0.5520211 0.7526268 0.1690758 1.        0.63     ], Reward: 2.0\n",
            "Processed Log → State: [0.5487594  0.7545201  0.17587294 0.9671099  0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.5956651  0.7391681  0.17305453 1.         0.69      ], Reward: 7.0\n",
            "Processed Log → State: [0.6126509  0.73216635 0.1781208  1.         0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.6235623  0.74357903 0.17581916 1.         0.75      ], Reward: 0.0\n",
            "Processed Log → State: [0.6468838 0.750549  0.1709316 1.        0.78     ], Reward: 8.0\n",
            "Processed Log → State: [0.66808575 0.730564   0.21568948 0.95398337 0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.65158045 0.7326485  0.24337722 0.95729053 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.64929914 0.74611366 0.25303164 0.9727112  0.87      ], Reward: 0.0\n",
            "Processed Log → State: [0.6170407  0.7394336  0.26895967 0.8850292  0.9       ], Reward: -1.0\n",
            "Processed Log → State: [0.23664998 0.33889472 0.07042043 0.616414   0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.25191784 0.4264839  0.07700082 0.6457142  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.27620983 0.4765618  0.08104243 0.7432081  0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.27155426 0.51650476 0.09611299 0.7938476  0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.28901768 0.5438923  0.10006744 0.8692247  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.30650198 0.5805666  0.12180129 0.8698022  0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.3271106  0.59825975 0.12569366 0.9810253  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.33406988 0.6120666  0.12411345 1.         0.24      ], Reward: -1.0\n",
            "Processed Log → State: [0.3916864  0.6493102  0.13918716 1.         0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.41577062 0.6861933  0.14080803 1.         0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.43010566 0.72109747 0.15035285 0.99167365 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.4478374  0.73687124 0.17143118 0.95643663 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.4681447  0.72334576 0.18356168 0.92819685 0.39      ], Reward: 6.0\n",
            "Processed Log → State: [0.47478268 0.7513752  0.1906801  0.91802543 0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.47614378 0.77249426 0.18871677 0.9244711  0.45      ], Reward: 0.0\n",
            "Processed Log → State: [0.46680534 0.78003275 0.18635814 0.97686166 0.48      ], Reward: 7.0\n",
            "Processed Log → State: [0.44583112 0.7731855  0.18869032 1.         0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.46234515 0.75720996 0.17947528 0.9644164  0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.4340545  0.7683735  0.18349837 0.94087404 0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.4295698  0.78114647 0.20277682 0.96066046 0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.41419926 0.7618452  0.20934668 0.9932571  0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.41858727 0.7351106  0.21875422 1.         0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.46957484 0.79114497 0.22086099 1.         0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.4511301  0.81360567 0.2189115  0.9939862  0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.46980086 0.7942843  0.22085026 1.         0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.48125607 0.79469955 0.20638736 1.         0.78      ], Reward: 4.0\n",
            "Processed Log → State: [0.4974833  0.8091755  0.21754253 1.         0.81      ], Reward: 2.0\n",
            "Processed Log → State: [0.5275572  0.75248367 0.23731288 1.         0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.5234175  0.77264243 0.24479951 1.         0.87      ], Reward: 7.0\n",
            "Processed Log → State: [0.53973323 0.76432526 0.26091504 0.99551797 0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.20580138 0.32303077 0.04228705 0.66473687 0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.19534232 0.34997097 0.04698218 0.7822969  0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.21381655 0.343889   0.04857529 0.7686239  0.09      ], Reward: 7.0\n",
            "Processed Log → State: [0.1783345  0.36541688 0.0509282  0.75804466 0.12      ], Reward: 7.0\n",
            "Processed Log → State: [0.16600417 0.37854722 0.0537721  0.70849603 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.17828771 0.3836125  0.04933859 0.67423844 0.18      ], Reward: 0.0\n",
            "Processed Log → State: [0.20696937 0.38316697 0.04964319 0.6923535  0.21      ], Reward: 7.0\n",
            "Processed Log → State: [0.17386582 0.36958888 0.04928146 0.6891577  0.24      ], Reward: 7.0\n",
            "Processed Log → State: [0.18326971 0.35729954 0.04885389 0.64241743 0.27      ], Reward: -2.0\n",
            "Processed Log → State: [0.23959692 0.32840058 0.05507739 0.675637   0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.25540987 0.3432419  0.03965461 0.6251828  0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.28623664 0.34831575 0.05122748 0.67383564 0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.31141853 0.34971568 0.05357631 0.75923574 0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.33094212 0.3668611  0.05912973 0.7859113  0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.33205438 0.42381373 0.07410873 0.70879257 0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.3220209  0.46162894 0.08607801 0.6593496  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.3240646  0.50410086 0.10741927 0.7109622  0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.36959872 0.5053891  0.10534781 0.7350963  0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.3737685  0.47033453 0.10685771 0.66666126 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.315543   0.48483306 0.10191821 0.671103   0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.3205669  0.50674003 0.10483281 0.6873963  0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.34259284 0.51990694 0.10880958 0.7265866  0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.34557986 0.53960997 0.09622087 0.8016219  0.69      ], Reward: 5.0\n",
            "Processed Log → State: [0.32901782 0.5508966  0.09966723 0.9482564  0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.33448046 0.5443299  0.10281227 0.96698624 0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.34490317 0.5760127  0.10764476 0.96241933 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.3217987  0.56623936 0.13719957 0.8449498  0.81      ], Reward: 2.0\n",
            "Processed Log → State: [0.3254858  0.5748956  0.14402868 0.8148364  0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.32353115 0.5556857  0.16068774 0.86888427 0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.28987965 0.52920705 0.1693292  0.87529176 0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.2253506  0.33843938 0.14069483 0.56548655 0.03      ], Reward: 0.0\n",
            "Processed Log → State: [0.22838002 0.37897456 0.16027606 0.5573155  0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.12026273 0.38753676 0.24279015 0.54507977 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.1668752  0.42822587 0.27803376 0.5161791  0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.20623447 0.51642495 0.33841005 0.42258993 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.21905729 0.5828435  0.3713301  0.4214549  0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.23114222 0.6132431  0.4562483  0.40406933 0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.34409496 0.5820097  0.4740582  0.35888305 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.46779147 0.59595305 0.5055447  0.42951992 0.27      ], Reward: 0.0\n",
            "Processed Log → State: [0.52681965 0.64838034 0.45258188 0.46448803 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.5913269  0.70258075 0.4802801  0.57626325 0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.6248653  0.77218324 0.51257455 0.5543226  0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.63742006 0.7688194  0.5112748  0.44144255 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.724366   0.79174656 0.5076207  0.54951745 0.42      ], Reward: -2.0\n",
            "Processed Log → State: [0.74655163 0.7714794  0.5176711  0.47082475 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.8347938  0.77717686 0.53523695 0.41439134 0.48      ], Reward: 6.0\n",
            "Processed Log → State: [0.91737443 0.81888026 0.52900654 0.4183383  0.51      ], Reward: 7.0\n",
            "Processed Log → State: [0.92565274 0.8775126  0.5093794  0.34164956 0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.9202875  0.85909563 0.5878918  0.2807025  0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.94482654 0.9157713  0.59608155 0.2765047  0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.9072544  0.91029143 0.5966012  0.26784214 0.63      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.90380496 0.62429434 0.22448185 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.9371427  0.6010164  0.28525382 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.96682394 0.9525899  0.5495814  0.26935974 0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.9468649  0.98806715 0.5434064  0.23796904 0.75      ], Reward: -1.0\n",
            "Processed Log → State: [0.9487062  0.9781929  0.5840748  0.25397873 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.88854545 0.99054563 0.626337   0.26315773 0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.9646141  1.         0.69147193 0.34325328 0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.95959765 1.         0.7513529  0.3699035  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.94973993 0.9854249  0.7814114  0.35968924 0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.19378941 0.36084393 0.0626304  0.63991034 0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.2277913  0.39298767 0.0677244  0.66482735 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.25765136 0.41247842 0.08045709 0.7077278  0.09      ], Reward: 8.0\n",
            "Processed Log → State: [0.24305756 0.45480362 0.08747929 0.7081396  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.2432673  0.4466369  0.10493689 0.69128114 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.271846   0.46609145 0.11033147 0.63960284 0.18      ], Reward: 7.0\n",
            "Processed Log → State: [0.27604157 0.42918843 0.12513572 0.6147635  0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.31265628 0.40716767 0.14270301 0.6727723  0.24      ], Reward: 5.0\n",
            "Processed Log → State: [0.34092614 0.40807775 0.15032703 0.74173146 0.27      ], Reward: 7.0\n",
            "Processed Log → State: [0.34456185 0.4482729  0.15628764 0.8010619  0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.39936337 0.46358624 0.17001916 0.77676105 0.33      ], Reward: 4.0\n",
            "Processed Log → State: [0.44565073 0.49838233 0.17786403 0.7878753  0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.4641856  0.5498079  0.1844036  0.74964553 0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.46126422 0.60519034 0.1823768  0.80576444 0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.47408384 0.6111452  0.18456715 0.8291285  0.45      ], Reward: 6.0\n",
            "Processed Log → State: [0.49305728 0.64982915 0.19186229 0.7897922  0.48      ], Reward: 2.0\n",
            "Processed Log → State: [0.5091432  0.6274552  0.19976601 0.8336554  0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.5266469  0.65277064 0.1983887  0.84362113 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.5533777  0.64723575 0.20998557 0.85179186 0.57      ], Reward: 5.0\n",
            "Processed Log → State: [0.555645   0.6680427  0.2106208  0.83977014 0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.5978026  0.6894922  0.21003605 0.8623814  0.63      ], Reward: 2.0\n",
            "Processed Log → State: [0.62647617 0.7047362  0.21455485 0.8810752  0.66      ], Reward: 0.0\n",
            "Processed Log → State: [0.61769235 0.72853786 0.21670629 0.90262634 0.69      ], Reward: 8.0\n",
            "Processed Log → State: [0.63211083 0.7075726  0.20710817 0.9358331  0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.6052456  0.7079311  0.21485783 0.91342753 0.75      ], Reward: 5.0\n",
            "Processed Log → State: [0.62295085 0.70926267 0.23165044 0.935529   0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.63575846 0.68246865 0.26196805 0.99329567 0.81      ], Reward: 2.0\n",
            "Processed Log → State: [0.65575784 0.6816025  0.28340006 0.98022026 0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.65883803 0.7084974  0.2960584  0.9753744  0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.66058564 0.72526866 0.30072173 1.         0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.19131154 0.32697073 0.06315946 0.6612954  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.17150177 0.39866865 0.06961758 0.5891682  0.06      ], Reward: 7.0\n",
            "Processed Log → State: [0.17399673 0.44994327 0.07622171 0.5508095  0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.23860939 0.5153811  0.0710531  0.52309954 0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.28732693 0.65858644 0.12105253 0.5103039  0.15      ], Reward: 2.0\n",
            "Processed Log → State: [0.38846877 0.7669448  0.15850878 0.44466254 0.18      ], Reward: 2.0\n",
            "Processed Log → State: [0.35244504 0.81096786 0.15914707 0.41122591 0.21      ], Reward: -1.0\n",
            "Processed Log → State: [0.46265703 0.84242314 0.20258328 0.35659608 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.4544514  0.8884009  0.20087114 0.3637247  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.5136419  0.9497808  0.19429778 0.43330994 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.44083056 0.9914066  0.16225646 0.43583012 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.48903748 0.9932351  0.20717645 0.42605612 0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.52748376 1.         0.19002229 0.46976775 0.39      ], Reward: 0.0\n",
            "Processed Log → State: [0.5204775  1.         0.20809136 0.47324654 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.5705819  1.         0.22944243 0.50615495 0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.5913771  1.         0.22366285 0.52414197 0.48      ], Reward: 5.0\n",
            "Processed Log → State: [0.58897185 1.         0.17559601 0.5693294  0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.5934775  0.9447674  0.15073292 0.58528614 0.54      ], Reward: 7.0\n",
            "Processed Log → State: [0.5159473 1.        0.1716216 0.6094684 0.57     ], Reward: 2.0\n",
            "Processed Log → State: [0.5153314 1.        0.1678208 0.6276462 0.6      ], Reward: 2.0\n",
            "Processed Log → State: [0.5964648  0.98730034 0.19127825 0.5971254  0.63      ], Reward: 2.0\n",
            "Processed Log → State: [0.6231244  1.         0.24455054 0.6254363  0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.73628914 1.         0.23956758 0.70066375 0.69      ], Reward: 4.0\n",
            "Processed Log → State: [0.70453256 1.         0.21474715 0.67291796 0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.71087694 1.         0.20244344 0.6756547  0.75      ], Reward: 6.0\n",
            "Processed Log → State: [0.7181824  1.         0.19381389 0.6477624  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.68637276 0.9564499  0.22822031 0.6110288  0.81      ], Reward: 8.0\n",
            "Processed Log → State: [0.6537708  0.93554634 0.28896448 0.69151866 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.69752216 0.9266127  0.289291   0.78229636 0.87      ], Reward: 5.0\n",
            "Processed Log → State: [0.73655504 0.9569902  0.34958178 0.78161335 0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.20471486 0.36220315 0.03995255 0.55461913 0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.22301242 0.36075115 0.08327407 0.59648263 0.06      ], Reward: 0.0\n",
            "Processed Log → State: [0.25291988 0.37792802 0.12759025 0.5649326  0.09      ], Reward: 0.0\n",
            "Processed Log → State: [0.19770437 0.43110642 0.19045593 0.6448606  0.12      ], Reward: 7.0\n",
            "Processed Log → State: [0.29660746 0.44746575 0.23433463 0.622401   0.15      ], Reward: 6.0\n",
            "Processed Log → State: [0.38558736 0.50535    0.28980088 0.56412166 0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.4744468  0.51397395 0.28012756 0.5326424  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.54454577 0.5089246  0.2947199  0.50116336 0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.6223557  0.53938264 0.39419246 0.57603556 0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.6818392  0.540963   0.44020852 0.62433976 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.62815297 0.5649888  0.43335104 0.6136566  0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.66303843 0.5580345  0.47823635 0.55934364 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.7062152  0.6108615  0.5161779  0.58788383 0.39      ], Reward: 7.0\n",
            "Processed Log → State: [0.6857773  0.62206626 0.4584594  0.48315933 0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.7348994  0.64472324 0.53287613 0.49003446 0.45      ], Reward: 0.0\n",
            "Processed Log → State: [0.7775822  0.6272752  0.57474625 0.49861956 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.7954954  0.62877023 0.6221456  0.47652844 0.51      ], Reward: 7.0\n",
            "Processed Log → State: [0.8335573  0.5817795  0.66761345 0.46708426 0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.91588044 0.6437751  0.6888855  0.5231557  0.57      ], Reward: 5.0\n",
            "Processed Log → State: [0.91544974 0.69680357 0.7861665  0.47066167 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.9819345  0.69365597 0.75342554 0.53540045 0.63      ], Reward: 8.0\n",
            "Processed Log → State: [1.        0.705752  0.7536392 0.5071082 0.66     ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.66630447 0.7234355  0.48727867 0.69      ], Reward: 1.0\n",
            "Processed Log → State: [1.        0.6631366 0.706851  0.5588645 0.72     ], Reward: 2.0\n",
            "Processed Log → State: [0.9903605  0.66053265 0.79676706 0.5729687  0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.99381095 0.6589253  0.7334259  0.5306408  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.91587174 0.68176    0.76545554 0.51358277 0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.8779444  0.64070654 0.8262136  0.49155167 0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.96856904 0.62533087 0.8114054  0.5384629  0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.96078205 0.6064985  0.7941649  0.54342973 0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.31757668 0.2985597  0.13320152 0.5284501  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.3677018  0.346407   0.20836963 0.5581121  0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.47114068 0.35372323 0.1892585  0.613891   0.09      ], Reward: 2.0\n",
            "Processed Log → State: [0.49326596 0.38676262 0.25597927 0.6099446  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.6683985  0.44057626 0.33579028 0.57264155 0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.6526691  0.47238684 0.45934615 0.49561405 0.18      ], Reward: 6.0\n",
            "Processed Log → State: [0.7076234  0.50738263 0.4442643  0.41077906 0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.7859535  0.578618   0.5236331  0.39446306 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.8021095  0.6171358  0.5790344  0.44812712 0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.81453395 0.64908105 0.63751155 0.49156195 0.3       ], Reward: 2.0\n",
            "Processed Log → State: [0.8338843  0.6066917  0.6064555  0.49153647 0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.8420898  0.64721614 0.641054   0.49618426 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.8200003 0.6604741 0.6934212 0.5161701 0.39     ], Reward: 3.0\n",
            "Processed Log → State: [0.7783242  0.65965486 0.64639187 0.5465922  0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.7927645 0.6928068 0.7053156 0.5039765 0.45     ], Reward: 1.0\n",
            "Processed Log → State: [0.7939651 0.7005651 0.6940076 0.5602926 0.48     ], Reward: 2.0\n",
            "Processed Log → State: [0.87405527 0.72193056 0.5623937  0.5109935  0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.87930524 0.76328796 0.57818645 0.4941755  0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.92294616 0.7431965  0.6216273  0.46799502 0.57      ], Reward: 2.0\n",
            "Processed Log → State: [0.91868204 0.7436387  0.64817905 0.31747687 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.984857  0.7442166 0.6388273 0.2706744 0.63     ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.7325949  0.6583896  0.31264424 0.66      ], Reward: 5.0\n",
            "Processed Log → State: [0.9810031  0.7302362  0.66217464 0.3634979  0.69      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.72585666 0.6975815  0.38609806 0.72      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.7145815  0.72347796 0.31212136 0.75      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.7142298  0.71257937 0.32040668 0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.9975441  0.74900645 0.7346357  0.3192149  0.81      ], Reward: 7.0\n",
            "Processed Log → State: [1.         0.82785547 0.74805045 0.36837068 0.84      ], Reward: 0.0\n",
            "Processed Log → State: [0.96293706 0.79978365 0.7871417  0.4056929  0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.99103594 0.840303   0.76978457 0.4391651  0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.22693618 0.31900904 0.07356329 0.5827705  0.03      ], Reward: 2.0\n",
            "Processed Log → State: [0.30051327 0.3269832  0.10937688 0.66655064 0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.30933365 0.33655122 0.1268011  0.7726228  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.31151593 0.36817607 0.13472891 0.77649766 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.34735656 0.386793   0.13632442 0.8337753  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.38335073 0.43654642 0.1470943  0.914552   0.18      ], Reward: 4.0\n",
            "Processed Log → State: [0.39353013 0.4760736  0.14502086 0.935436   0.21      ], Reward: 0.0\n",
            "Processed Log → State: [0.41675985 0.5079044  0.14793159 0.9802036  0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.44437212 0.5250934  0.14426646 1.         0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.420132   0.54337436 0.15853272 0.9704185  0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.43300816 0.54599255 0.16667986 1.         0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.42659307 0.5355438  0.16321689 0.9850705  0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.44650218 0.5834475  0.17371169 1.         0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.4859884  0.60194993 0.18619815 0.9977989  0.42      ], Reward: 5.0\n",
            "Processed Log → State: [0.49348158 0.59645027 0.18653597 0.97373486 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.55177045 0.6103496  0.19287218 1.         0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.57459325 0.61228323 0.19717719 1.         0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.5797771  0.62393963 0.18738475 1.         0.54      ], Reward: 7.0\n",
            "Processed Log → State: [0.6275647  0.62778753 0.18697232 1.         0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.63946754 0.64795566 0.1938608  1.         0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.618103   0.673513   0.20392394 1.         0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.6289453  0.71193135 0.2075334  0.9357367  0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.62574923 0.7086139  0.2155363  0.97319794 0.69      ], Reward: 0.0\n",
            "Processed Log → State: [0.6559083  0.7356944  0.24589601 1.         0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.6528542  0.73383695 0.22600721 1.         0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.6338314  0.69877034 0.21552317 1.         0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.6385246  0.7033842  0.23506849 1.         0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.656085   0.72454566 0.2462254  1.         0.84      ], Reward: 3.0\n",
            "Processed Log → State: [0.6872757  0.7385768  0.26411176 1.         0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.6941826  0.7273482  0.27505168 0.9699129  0.9       ], Reward: 5.0\n",
            "Processed Log → State: [0.21860263 0.3488565  0.04283606 0.63211834 0.03      ], Reward: -1.0\n",
            "Processed Log → State: [0.22626121 0.37569442 0.03981749 0.6876051  0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.2373005  0.41419762 0.03006523 0.70566887 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.24450336 0.39177984 0.02885248 0.6663406  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.26058865 0.4058937  0.03219857 0.6060954  0.15      ], Reward: 2.0\n",
            "Processed Log → State: [0.22959442 0.42458758 0.04020561 0.62511605 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.23624936 0.4300454  0.03871183 0.68482596 0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.2423547  0.4735095  0.03316233 0.6717034  0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.23756161 0.51118565 0.04167626 0.6784013  0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.22524318 0.50750536 0.04579074 0.75174177 0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.2347956  0.51844424 0.03644621 0.7423449  0.33      ], Reward: 7.0\n",
            "Processed Log → State: [0.26663032 0.5471986  0.04632904 0.6899392  0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.26476583 0.57733977 0.03192113 0.76356643 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.28475356 0.60091805 0.02314264 0.82540154 0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.27484584 0.6083719  0.01856312 0.8257358  0.45      ], Reward: 0.0\n",
            "Processed Log → State: [0.2502821  0.62030977 0.03829594 0.9135672  0.48      ], Reward: 2.0\n",
            "Processed Log → State: [0.26496318 0.649057   0.03469396 0.9421798  0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.26269454 0.6197061  0.03288944 1.         0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.26443955 0.6496033  0.04402258 1.         0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.28450704 0.67444277 0.04066891 0.9753977  0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.31243867 0.68425834 0.04370747 0.9976943  0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.31999043 0.6934169  0.03949182 0.987363   0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.33851346 0.6965707  0.04633779 0.84332967 0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.33239543 0.6847471  0.05063215 0.84518963 0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.32590812 0.6594817  0.05447615 0.8603666  0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.29259735 0.6513185  0.05967454 0.88956225 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.32631165 0.6705307  0.1040928  0.81529593 0.81      ], Reward: -2.0\n",
            "Processed Log → State: [0.32776877 0.6364485  0.11949379 0.8697228  0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.31046236 0.6037197  0.13982412 0.9474517  0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.28670192 0.6087271  0.15813    0.9554891  0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.20419408 0.31750324 0.04555741 0.6753471  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.24987794 0.31851032 0.06254797 0.7161166  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.24872288 0.35476616 0.07429451 0.7998417  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.26232702 0.3955487  0.06976952 0.76942885 0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.2670969  0.3968754  0.07997318 0.7823126  0.15      ], Reward: 2.0\n",
            "Processed Log → State: [0.28896084 0.4442176  0.08754284 0.73336524 0.18      ], Reward: 6.0\n",
            "Processed Log → State: [0.32307866 0.45808434 0.08378191 0.738077   0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.34994367 0.4726481  0.07001886 0.8293406  0.24      ], Reward: 8.0\n",
            "Processed Log → State: [0.3531459  0.4929364  0.08012658 0.82123876 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.34453875 0.51040864 0.0852669  0.80432683 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.35316548 0.5452496  0.0855503  0.8475154  0.33      ], Reward: 2.0\n",
            "Processed Log → State: [0.36142626 0.55388826 0.09291307 0.8775939  0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.39634466 0.5415997  0.10233483 0.80305    0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.39779934 0.57512283 0.10920016 0.84405273 0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.44490877 0.5946379  0.1023691  0.90148944 0.45      ], Reward: 0.0\n",
            "Processed Log → State: [0.44944713 0.58212847 0.12097978 0.86973333 0.48      ], Reward: 2.0\n",
            "Processed Log → State: [0.47272268 0.5722982  0.12118721 0.8788763  0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.5126147  0.55896646 0.12201748 0.8548719  0.54      ], Reward: 5.0\n",
            "Processed Log → State: [0.5238683  0.5707183  0.12941487 0.91947174 0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.5356578  0.6296423  0.14170462 0.9730976  0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.5506041  0.5943062  0.15238586 1.         0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.5422119  0.57479703 0.14186078 1.         0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.5460609  0.56716424 0.15378514 0.99653405 0.69      ], Reward: 8.0\n",
            "Processed Log → State: [0.56754297 0.58325666 0.15926634 1.         0.72      ], Reward: 3.0\n",
            "Processed Log → State: [0.62532586 0.61288804 0.17778197 0.99410385 0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.65159607 0.6460287  0.18943805 0.9959042  0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.6687093  0.6654097  0.21602269 0.9605152  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.6460862  0.6854006  0.22906102 0.94757617 0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.7010062 0.7162858 0.2531661 0.949911  0.87     ], Reward: 3.0\n",
            "Processed Log → State: [0.68423957 0.7058908  0.26806962 0.96607435 0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.19500929 0.29660308 0.06698833 0.7383964  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.18762699 0.27727267 0.07610475 0.59966785 0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.20180397 0.311674   0.05789923 0.5863959  0.09      ], Reward: 7.0\n",
            "Processed Log → State: [0.19857576 0.3197457  0.05714152 0.6461644  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.19454825 0.35851607 0.04865833 0.7339311  0.15      ], Reward: 6.0\n",
            "Processed Log → State: [0.22165295 0.41696626 0.05796426 0.7281174  0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.18266654 0.42309853 0.06704262 0.7653551  0.21      ], Reward: 5.0\n",
            "Processed Log → State: [0.19886877 0.43276012 0.07043464 0.76178366 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.21346834 0.4677144  0.08824433 0.7287666  0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.18650347 0.48586258 0.10072432 0.73246247 0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.19606435 0.49360707 0.11844887 0.72999096 0.33      ], Reward: 5.0\n",
            "Processed Log → State: [0.19409952 0.47376785 0.11613274 0.7227928  0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.18704434 0.49598548 0.12057318 0.756638   0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.22061332 0.5127453  0.13657005 0.7440818  0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.2345564  0.5241458  0.13953573 0.7189063  0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.2605554  0.5249015  0.12444905 0.71302    0.48      ], Reward: 6.0\n",
            "Processed Log → State: [0.29010043 0.52246207 0.11829963 0.70865357 0.51      ], Reward: 5.0\n",
            "Processed Log → State: [0.29011264 0.509244   0.11164024 0.7422678  0.54      ], Reward: 0.0\n",
            "Processed Log → State: [0.2836467  0.51536626 0.12883423 0.7776452  0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.30469635 0.5369512  0.12775333 0.7472736  0.6       ], Reward: 6.0\n",
            "Processed Log → State: [0.32979986 0.53558624 0.13005248 0.8054994  0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.36150864 0.5263215  0.14584455 0.91623384 0.66      ], Reward: 4.0\n",
            "Processed Log → State: [0.3894993  0.5266706  0.14497636 1.         0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.39704624 0.5695448  0.13802704 0.9930447  0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.37220907 0.58584577 0.14159277 1.         0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.35876784 0.62204826 0.12690322 1.         0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.39354622 0.6123604  0.14841376 0.9776557  0.81      ], Reward: 6.0\n",
            "Processed Log → State: [0.4256181  0.6110985  0.15905964 0.9369545  0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.41798735 0.6148512  0.1643787  0.9884457  0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.44354972 0.64118576 0.16762157 0.98813814 0.9       ], Reward: -1.0\n",
            "Processed Log → State: [0.22514907 0.29090938 0.05993701 0.6396288  0.03      ], Reward: 0.0\n",
            "Processed Log → State: [0.24214768 0.31694034 0.06365261 0.66096944 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.24672596 0.32379878 0.0555008  0.63081044 0.09      ], Reward: 5.0\n",
            "Processed Log → State: [0.31085292 0.30578136 0.05827307 0.78104174 0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.3059756  0.31941944 0.03652244 0.80560046 0.15      ], Reward: 2.0\n",
            "Processed Log → State: [0.3237512  0.34959507 0.05295175 0.94690037 0.18      ], Reward: 7.0\n",
            "Processed Log → State: [0.33283216 0.34714088 0.05364453 0.9844988  0.21      ], Reward: 0.0\n",
            "Processed Log → State: [0.33351752 0.3479975  0.04958577 0.9618314  0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.32364708 0.3757409  0.04108122 0.9442662  0.27      ], Reward: 5.0\n",
            "Processed Log → State: [0.38085714 0.37737787 0.04237655 0.9935796  0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.41133723 0.37435478 0.05247235 0.95643395 0.33      ], Reward: 4.0\n",
            "Processed Log → State: [0.3825134  0.3913226  0.04644141 0.98230267 0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.36452764 0.44218054 0.05853604 0.971146   0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.31724465 0.43820623 0.06623958 0.9756217  0.42      ], Reward: 0.0\n",
            "Processed Log → State: [0.30911198 0.44813743 0.07367116 0.9756729  0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.28534898 0.47731647 0.06919513 1.         0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.27468157 0.4660889  0.06181812 1.         0.51      ], Reward: 2.0\n",
            "Processed Log → State: [0.28998324 0.47049353 0.05273249 1.         0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.3022082  0.48046574 0.05820718 0.9944335  0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.30316487 0.4661394  0.06552286 0.98951745 0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.28582644 0.4890748  0.05849249 1.         0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.29481745 0.5068323  0.0811452  1.         0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.29969946 0.5173408  0.10053124 0.9713089  0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.28470615 0.5207402  0.08736093 1.         0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.26772898 0.5303689  0.07079996 0.983247   0.75      ], Reward: 7.0\n",
            "Processed Log → State: [0.28990352 0.5371746  0.07960533 1.         0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.2966594 0.5446017 0.1098811 1.        0.81     ], Reward: 6.0\n",
            "Processed Log → State: [0.30346218 0.5497144  0.123261   0.94385767 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.3294415  0.55174273 0.1320266  0.9519562  0.87      ], Reward: 0.0\n",
            "Processed Log → State: [0.3349046  0.5623871  0.15082625 0.9204057  0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.1943576  0.30731684 0.07281279 0.66331106 0.03      ], Reward: 0.0\n",
            "Processed Log → State: [0.24462438 0.30700186 0.09070953 0.65204483 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.28015944 0.33176282 0.09808144 0.66021174 0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.27234876 0.38520935 0.11146468 0.6668594  0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.31528896 0.41137633 0.11838762 0.65614676 0.15      ], Reward: 0.0\n",
            "Processed Log → State: [0.33157575 0.43739364 0.13203755 0.7020432  0.18      ], Reward: 7.0\n",
            "Processed Log → State: [0.33942708 0.4596103  0.13661866 0.7310996  0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.33482164 0.4594506  0.14546852 0.7556746  0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.38503066 0.47262046 0.15177277 0.84725547 0.27      ], Reward: 7.0\n",
            "Processed Log → State: [0.41270027 0.4613812  0.15155724 0.8407516  0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.44662046 0.4827755  0.16117692 0.89068496 0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.4575761  0.4860728  0.17911611 0.89788836 0.36      ], Reward: 0.0\n",
            "Processed Log → State: [0.5093232  0.4985646  0.16148064 0.9463689  0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.5166816  0.5163309  0.16564898 0.97933316 0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.51053894 0.57969666 0.17193893 1.         0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.51292723 0.57017565 0.18989305 1.         0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.52193475 0.6074996  0.19107082 1.         0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.55958456 0.62475646 0.21174954 0.9918709  0.54      ], Reward: -2.0\n",
            "Processed Log → State: [0.58125836 0.65683293 0.214429   1.         0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.6020212  0.6661693  0.20842853 1.         0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.61491764 0.6426138  0.21745704 0.99466467 0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.61653626 0.6503219  0.21275656 0.97000134 0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.6275913  0.656552   0.22920355 1.         0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.60158974 0.6267141  0.22461691 1.         0.72      ], Reward: 5.0\n",
            "Processed Log → State: [0.62782574 0.6441095  0.20824128 0.95904845 0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.6436292  0.64281964 0.193942   0.9820221  0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.6555336  0.63529485 0.22700089 0.95640445 0.81      ], Reward: 2.0\n",
            "Processed Log → State: [0.6644109  0.6594867  0.22802627 0.9242411  0.84      ], Reward: 7.0\n",
            "Processed Log → State: [0.68867105 0.6703629  0.25933093 0.94312817 0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.6828175  0.68089503 0.2779449  1.         0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.23777856 0.30767763 0.11083096 0.6129386  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.29092237 0.2980604  0.22974923 0.66879284 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.30008873 0.33020335 0.25757176 0.6274282  0.09      ], Reward: 0.0\n",
            "Processed Log → State: [0.3103928  0.34446904 0.3175298  0.5651279  0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.37045413 0.3428519  0.47094575 0.5179393  0.15      ], Reward: -1.0\n",
            "Processed Log → State: [0.44071293 0.39570978 0.52694505 0.50068057 0.18      ], Reward: 0.0\n",
            "Processed Log → State: [0.48858693 0.44617376 0.63770217 0.48860225 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.56979793 0.46510342 0.6493517  0.48588902 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.62676364 0.47882497 0.71141493 0.48339996 0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.6508464  0.52216285 0.6786947  0.44029158 0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.80821866 0.53432846 0.66052467 0.39937034 0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.85446143 0.5386646  0.71294814 0.36989447 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.55454236 0.7091268  0.31018287 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.57081205 0.71289617 0.31939456 0.42      ], Reward: 7.0\n",
            "Processed Log → State: [0.9278586  0.6379434  0.7981257  0.25895137 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.99593836 0.6958459  0.8479444  0.29151887 0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.9457542  0.70118743 0.8355141  0.25027794 0.51      ], Reward: 7.0\n",
            "Processed Log → State: [0.97013336 0.72119707 0.9005986  0.1887953  0.54      ], Reward: 5.0\n",
            "Processed Log → State: [0.9951774  0.68945754 0.9169079  0.20831406 0.57      ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.67742866 0.8855362  0.18300863 0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.94596744 0.684932   0.8856486  0.11444297 0.63      ], Reward: 8.0\n",
            "Processed Log → State: [1.         0.7160379  0.87291414 0.12622096 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.73414004 0.9134723  0.11788434 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.93044555 0.7914187  0.89736795 0.25233334 0.72      ], Reward: 0.0\n",
            "Processed Log → State: [0.9505637 0.8206836 0.8724698 0.2816135 0.75     ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.78925115 0.8845751  0.24782604 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.8005439  0.8589065  0.19489849 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.83413607 0.90069556 0.18462886 0.84      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.83333564 0.8695335  0.27684227 0.87      ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.8138737  0.84476733 0.22789526 0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.20918998 0.2752699  0.06717658 0.639268   0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.21500364 0.29357576 0.06419583 0.5588     0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.23821634 0.26573166 0.06044264 0.563607   0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.27399832 0.2567771  0.06540211 0.61075586 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.27564505 0.26505604 0.06749828 0.5412282  0.15      ], Reward: 8.0\n",
            "Processed Log → State: [0.277829   0.2424198  0.05877024 0.53106374 0.18      ], Reward: 5.0\n",
            "Processed Log → State: [0.2947379  0.23155528 0.03124693 0.6649195  0.21      ], Reward: 7.0\n",
            "Processed Log → State: [0.29571757 0.23401807 0.02417202 0.6957551  0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.30181724 0.25101203 0.037154   0.71851426 0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.307421   0.27058375 0.05272001 0.7201076  0.3       ], Reward: 2.0\n",
            "Processed Log → State: [0.33859548 0.293107   0.07147544 0.7088841  0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.36272338 0.35754272 0.07580925 0.70968753 0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.38344964 0.33430025 0.08678251 0.7298233  0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.38858852 0.3447927  0.09646935 0.74299175 0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.409003   0.3534089  0.10279264 0.84796655 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.4159388  0.3695681  0.12241445 0.86936396 0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.432474   0.37959126 0.13387309 0.82885075 0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.3995066  0.41447553 0.13572708 0.82540256 0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.40954334 0.44753772 0.12543063 0.84373015 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.3975208  0.4355699  0.13114902 0.7651691  0.6       ], Reward: 8.0\n",
            "Processed Log → State: [0.37653875 0.4421449  0.12972052 0.7017395  0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.3457003  0.4300681  0.13967818 0.7674911  0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.37337664 0.4617067  0.12997049 0.7424003  0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.35633084 0.48420653 0.11903489 0.7403726  0.72      ], Reward: -1.0\n",
            "Processed Log → State: [0.3718668  0.4893996  0.11368858 0.7790537  0.75      ], Reward: 1.0\n",
            "Processed Log → State: [0.40258047 0.51225007 0.11026577 0.7723794  0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.41038942 0.5149217  0.12551469 0.80708855 0.81      ], Reward: 7.0\n",
            "Processed Log → State: [0.39218342 0.53452665 0.14380671 0.81439096 0.84      ], Reward: -1.0\n",
            "Processed Log → State: [0.4428378  0.51857543 0.15311302 0.8825747  0.87      ], Reward: 7.0\n",
            "Processed Log → State: [0.4081966  0.5265675  0.1737752  0.90591156 0.9       ], Reward: 7.0\n",
            "Processed Log → State: [0.19792543 0.32551253 0.06758972 0.5605783  0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.22686127 0.3675873  0.05567536 0.6079664  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.24881765 0.3675744  0.05865844 0.6251478  0.09      ], Reward: 5.0\n",
            "Processed Log → State: [0.2902143  0.3686995  0.08503414 0.68675035 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.29289603 0.3784671  0.08302867 0.6575833  0.15      ], Reward: 2.0\n",
            "Processed Log → State: [0.29577804 0.39766556 0.08637403 0.72860134 0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.3172785  0.3987427  0.10032944 0.77808493 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.3418343  0.42662323 0.10740131 0.80580086 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.35741013 0.43631905 0.12547551 0.9058412  0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.35612306 0.47414303 0.14248776 1.         0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.36974207 0.50399494 0.15523559 0.98276323 0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.36567244 0.5071666  0.14842145 1.         0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.41789845 0.5219899  0.15177551 1.         0.39      ], Reward: 6.0\n",
            "Processed Log → State: [0.4069781  0.533017   0.15181644 1.         0.42      ], Reward: 2.0\n",
            "Processed Log → State: [0.42949623 0.5562245  0.15703519 1.         0.45      ], Reward: 6.0\n",
            "Processed Log → State: [0.45472744 0.59083223 0.17502034 1.         0.48      ], Reward: 7.0\n",
            "Processed Log → State: [0.49962336 0.6238034  0.17741565 1.         0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.5320683  0.64727724 0.17758863 1.         0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.5382042  0.64965963 0.18958758 1.         0.57      ], Reward: 5.0\n",
            "Processed Log → State: [0.5531158  0.62593454 0.19619721 1.         0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.5680353 0.6587701 0.1979319 1.        0.63     ], Reward: 8.0\n",
            "Processed Log → State: [0.56697285 0.6855852  0.20483492 1.         0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.54408014 0.6673665  0.19716913 0.96649635 0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.5770904  0.6660324  0.19179364 1.         0.72      ], Reward: 5.0\n",
            "Processed Log → State: [0.58681726 0.6738297  0.18441157 0.98611164 0.75      ], Reward: 6.0\n",
            "Processed Log → State: [0.59421116 0.68242395 0.1897604  0.9863412  0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.601437   0.6739809  0.22451347 0.9781567  0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.5833489  0.6547674  0.23890287 1.         0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.5627178  0.63195604 0.25076428 1.         0.87      ], Reward: 3.0\n",
            "Processed Log → State: [0.5541061  0.64144427 0.2842307  0.9953811  0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.23214452 0.28497186 0.0413765  0.5753023  0.03      ], Reward: 0.0\n",
            "Processed Log → State: [0.24520718 0.2781753  0.03386589 0.59247595 0.06      ], Reward: 4.0\n",
            "Processed Log → State: [0.27152362 0.2772801  0.04059982 0.63852495 0.09      ], Reward: 5.0\n",
            "Processed Log → State: [0.2693148  0.26440367 0.06172381 0.7072439  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.2671198  0.29057893 0.08204737 0.7259284  0.15      ], Reward: 7.0\n",
            "Processed Log → State: [0.27575776 0.26916632 0.08062473 0.8188966  0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.2710379  0.29113722 0.07731977 0.81086725 0.21      ], Reward: 6.0\n",
            "Processed Log → State: [0.25790578 0.31280455 0.0743473  0.80553323 0.24      ], Reward: 7.0\n",
            "Processed Log → State: [0.26687086 0.3176446  0.08774528 0.77824324 0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.27997646 0.3559444  0.08810533 0.7971726  0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.25813168 0.36007082 0.08630131 0.8089876  0.33      ], Reward: 5.0\n",
            "Processed Log → State: [0.23356903 0.39217922 0.0803666  0.82129127 0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.22886778 0.38711095 0.07245852 0.8612242  0.39      ], Reward: 7.0\n",
            "Processed Log → State: [0.24694893 0.44688714 0.08511557 0.9494835  0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.23309956 0.4200665  0.07816944 0.9466983  0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.25915483 0.43131858 0.0905816  0.9958779  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.25177744 0.4351164  0.07939878 0.93099105 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.2556087  0.40104285 0.07001057 0.9847707  0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.29592547 0.39718863 0.07093865 0.945053   0.57      ], Reward: 0.0\n",
            "Processed Log → State: [0.3316437  0.4042438  0.06584667 0.8929513  0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.3681363  0.36201924 0.07655539 0.9312452  0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.38462833 0.39332914 0.08119891 0.9173869  0.66      ], Reward: 4.0\n",
            "Processed Log → State: [0.36339012 0.4121489  0.08649208 0.9140547  0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.3588667  0.44506127 0.09128865 0.9323862  0.72      ], Reward: 6.0\n",
            "Processed Log → State: [0.37358218 0.44497854 0.09834048 0.9435717  0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.377194   0.4616181  0.07606013 0.9707215  0.78      ], Reward: 5.0\n",
            "Processed Log → State: [0.36101374 0.4805082  0.10480047 0.96508235 0.81      ], Reward: 2.0\n",
            "Processed Log → State: [0.38009703 0.43003452 0.1263602  0.9223542  0.84      ], Reward: 2.0\n",
            "Processed Log → State: [0.40181246 0.43767136 0.14042598 0.96543455 0.87      ], Reward: 0.0\n",
            "Processed Log → State: [0.4069384  0.41196895 0.15733248 0.97269213 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.24296093 0.32132426 0.00208091 0.62744015 0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.25780877 0.38116983 0.0666774  0.58965    0.06      ], Reward: 6.0\n",
            "Processed Log → State: [0.2932859  0.421156   0.07176536 0.5869288  0.09      ], Reward: 2.0\n",
            "Processed Log → State: [0.35725275 0.5328054  0.07889838 0.6565158  0.12      ], Reward: 0.0\n",
            "Processed Log → State: [0.33790573 0.5499461  0.08705172 0.6659186  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.40743706 0.5722085  0.11203877 0.67732114 0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.4948375  0.6754972  0.13223621 0.6039041  0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.45013413 0.71795905 0.14795956 0.6270031  0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.61556745 0.7417616  0.18553308 0.5268309  0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.6772201  0.76536083 0.22406942 0.47429886 0.3       ], Reward: 7.0\n",
            "Processed Log → State: [0.6545846  0.7445294  0.24675192 0.46651542 0.33      ], Reward: -1.0\n",
            "Processed Log → State: [0.66187763 0.7818971  0.2940854  0.47355762 0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.6457666  0.85345715 0.31824872 0.41601482 0.39      ], Reward: 5.0\n",
            "Processed Log → State: [0.6292119  0.9164051  0.2797115  0.46266818 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.60054094 0.90564424 0.28866407 0.44668758 0.45      ], Reward: 6.0\n",
            "Processed Log → State: [0.6994635  0.91592634 0.27131462 0.4319886  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.7182712  0.9735387  0.24438763 0.43614718 0.51      ], Reward: -1.0\n",
            "Processed Log → State: [0.7493197  0.99025464 0.2646973  0.46166542 0.54      ], Reward: 0.0\n",
            "Processed Log → State: [0.7668339  1.         0.236347   0.49295503 0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.73117286 0.9529998  0.21887562 0.5558486  0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.75339913 0.9374128  0.19283265 0.5662232  0.63      ], Reward: 2.0\n",
            "Processed Log → State: [0.82934195 0.9817599  0.20752536 0.6116791  0.66      ], Reward: 1.0\n",
            "Processed Log → State: [0.7884084 1.        0.184574  0.6348666 0.69     ], Reward: 2.0\n",
            "Processed Log → State: [0.74227923 0.9721398  0.17447996 0.6452342  0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.7218784  0.9635091  0.17874289 0.62537986 0.75      ], Reward: 0.0\n",
            "Processed Log → State: [0.6703009  1.         0.16878988 0.68403476 0.78      ], Reward: 2.0\n",
            "Processed Log → State: [0.6735257 1.        0.1652998 0.6080017 0.81     ], Reward: 7.0\n",
            "Processed Log → State: [0.6709503 1.        0.2398556 0.5456298 0.84     ], Reward: 3.0\n",
            "Processed Log → State: [0.71198064 1.         0.2359731  0.6175963  0.87      ], Reward: -1.0\n",
            "Processed Log → State: [0.7604343  0.99529505 0.26977727 0.55732924 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.12941071 0.29377878 0.10107558 0.48408702 0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.22420008 0.33344692 0.07648843 0.3980906  0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.2588524  0.35643888 0.20132713 0.33690327 0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.36163992 0.38925755 0.23987125 0.3451124  0.12      ], Reward: 0.0\n",
            "Processed Log → State: [0.35007066 0.39180756 0.22635269 0.31391734 0.15      ], Reward: 5.0\n",
            "Processed Log → State: [0.4203157  0.4663239  0.27054572 0.26466888 0.18      ], Reward: 2.0\n",
            "Processed Log → State: [0.4655144  0.5236046  0.30017012 0.21973005 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.5138514  0.55533826 0.31625903 0.20241062 0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.60391617 0.6549328  0.3525062  0.2285049  0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.59590024 0.6440641  0.3411625  0.20731147 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.59439105 0.7068015  0.3861502  0.2583912  0.33      ], Reward: 5.0\n",
            "Processed Log → State: [0.6942549  0.72462755 0.33830932 0.2399822  0.36      ], Reward: 6.0\n",
            "Processed Log → State: [0.71377116 0.70886624 0.2879265  0.22518848 0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.8241175  0.7367623  0.4477011  0.22120664 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.7647969  0.7776264  0.4798833  0.17705776 0.45      ], Reward: 7.0\n",
            "Processed Log → State: [0.7624723  0.8040228  0.5993296  0.23586607 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.78079796 0.82261765 0.68062454 0.23954663 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.80743945 0.800564   0.69564295 0.2705276  0.54      ], Reward: 7.0\n",
            "Processed Log → State: [0.83980936 0.8031988  0.68467003 0.21751568 0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.9100804  0.8118695  0.6796808  0.28247827 0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.8579389 0.8168955 0.6377163 0.3261285 0.63     ], Reward: 3.0\n",
            "Processed Log → State: [0.87793845 0.7963629  0.6046935  0.39995995 0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.8848608  0.8235855  0.6438179  0.40506828 0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.8804629  0.82158464 0.70270264 0.48010254 0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.8370156  0.8674689  0.6394161  0.50666755 0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.9160353  0.90441793 0.6003564  0.55738497 0.78      ], Reward: 0.0\n",
            "Processed Log → State: [0.9782389  0.85366243 0.6254075  0.49863845 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.96191233 0.88329303 0.63350445 0.56146187 0.84      ], Reward: 7.0\n",
            "Processed Log → State: [1.         0.8817035  0.69634855 0.5997489  0.87      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.8167118  0.6906141  0.58986676 0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.24241582 0.3540265  0.02166778 0.50375074 0.03      ], Reward: 5.0\n",
            "Processed Log → State: [0.1527767  0.36458027 0.07922604 0.48458087 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.1515639  0.45882377 0.10502031 0.3996708  0.09      ], Reward: 7.0\n",
            "Processed Log → State: [0.04304409 0.5190384  0.11374632 0.32716027 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.129101   0.5304734  0.13981152 0.32553288 0.15      ], Reward: 5.0\n",
            "Processed Log → State: [0.23149593 0.5934628  0.15301031 0.28022245 0.18      ], Reward: 5.0\n",
            "Processed Log → State: [0.2594989  0.5686336  0.13133761 0.28887445 0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.33684173 0.6102256  0.13561958 0.2608317  0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.2991822  0.6259771  0.16027108 0.24974881 0.27      ], Reward: 2.0\n",
            "Processed Log → State: [0.25493446 0.6752399  0.17762609 0.32736975 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.2122321  0.6896259  0.17368257 0.22815959 0.33      ], Reward: 2.0\n",
            "Processed Log → State: [0.20888183 0.7772253  0.2161449  0.158372   0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.1968204  0.8143061  0.23231284 0.16244392 0.39      ], Reward: 8.0\n",
            "Processed Log → State: [0.15582888 0.8315516  0.2595507  0.24092402 0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.18865627 0.9265035  0.2729239  0.3068874  0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.18116532 1.         0.27064395 0.36465383 0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.22544003 1.         0.22156999 0.38438818 0.51      ], Reward: 1.0\n",
            "Processed Log → State: [0.30788374 0.96176547 0.27810988 0.3793284  0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.3465563  1.         0.32482597 0.3120872  0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.32040802 1.         0.31750426 0.33493462 0.6       ], Reward: 6.0\n",
            "Processed Log → State: [0.3621229  1.         0.28481513 0.28286278 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.430804   1.         0.29633528 0.23686853 0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.5599009  1.         0.28493887 0.17471214 0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.5366678  1.         0.24748707 0.26097852 0.72      ], Reward: 6.0\n",
            "Processed Log → State: [0.55222756 1.         0.32465082 0.22769234 0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.6033075  1.         0.2861298  0.20293877 0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.65388167 0.9655937  0.28027624 0.22027412 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.63326883 0.9460767  0.27482575 0.2782674  0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.5478114  1.         0.3585818  0.27699614 0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.52906406 1.         0.33820072 0.33826542 0.9       ], Reward: 6.0\n",
            "Processed Log → State: [0.26738474 0.310198   0.05854793 0.5266536  0.03      ], Reward: 8.0\n",
            "Processed Log → State: [0.2803417  0.376185   0.14819704 0.44824088 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.32518417 0.3852006  0.26274306 0.39228827 0.09      ], Reward: 0.0\n",
            "Processed Log → State: [0.41943666 0.3858726  0.3043029  0.40053302 0.12      ], Reward: -1.0\n",
            "Processed Log → State: [0.49201754 0.46285927 0.30183133 0.34733403 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.53408015 0.49546528 0.28567195 0.3082195  0.18      ], Reward: 6.0\n",
            "Processed Log → State: [0.5563288  0.48667192 0.37878072 0.321574   0.21      ], Reward: 7.0\n",
            "Processed Log → State: [0.5788226  0.4935936  0.46716443 0.29127657 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.5790126  0.4947807  0.42545226 0.26780063 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.6083093  0.57576317 0.516729   0.26073587 0.3       ], Reward: 6.0\n",
            "Processed Log → State: [0.6612414  0.5815649  0.5087506  0.20747736 0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.6968058  0.64182484 0.48996818 0.19285445 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.7744926  0.6583102  0.50310653 0.17632876 0.39      ], Reward: 0.0\n",
            "Processed Log → State: [0.83905166 0.7511466  0.59540707 0.17724603 0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.85595053 0.7584735  0.64032835 0.19499314 0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.7238678  0.7681097  0.6818509  0.24189563 0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.79238266 0.79394144 0.6404535  0.2746806  0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.8050069  0.81812066 0.63056    0.31543714 0.54      ], Reward: -2.0\n",
            "Processed Log → State: [0.82544273 0.776122   0.6619689  0.29928344 0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.8433739  0.75844437 0.7122717  0.44029772 0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.8799236  0.77628386 0.74191767 0.40390727 0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.88665986 0.7886154  0.84091043 0.2974538  0.66      ], Reward: 6.0\n",
            "Processed Log → State: [0.8837395  0.7898681  0.8025037  0.27400795 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.9089637  0.7818262  0.702974   0.15773475 0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.89891833 0.8009947  0.7430354  0.1200816  0.75      ], Reward: 0.0\n",
            "Processed Log → State: [0.8690922  0.75611097 0.7815925  0.10650312 0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.88691115 0.726061   0.822209   0.17906588 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.8825459  0.7101177  0.80392706 0.30410284 0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.87841475 0.7408301  0.79732746 0.3067721  0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.8256957 0.7142217 0.7806146 0.2875148 0.9      ], Reward: 3.0\n",
            "Processed Log → State: [0.19227013 0.29458413 0.05678111 0.7406216  0.03      ], Reward: 0.0\n",
            "Processed Log → State: [0.22730084 0.29765144 0.06972198 0.7256183  0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.2178844  0.3184046  0.06985139 0.7832849  0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.19838743 0.32384983 0.05707948 0.84574735 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.21174768 0.35147637 0.05043817 0.9337384  0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.21015136 0.3707296  0.05314744 0.89968497 0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.21624213 0.40311363 0.03313665 0.8860786  0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.23246935 0.43041635 0.03273988 0.78456205 0.24      ], Reward: 6.0\n",
            "Processed Log → State: [0.22146109 0.45313007 0.01554643 0.8023702  0.27      ], Reward: -1.0\n",
            "Processed Log → State: [0.19574359 0.4969706  0.01719651 0.77202165 0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.18592922 0.5313931  0.01465496 0.7489833  0.33      ], Reward: 3.0\n",
            "Processed Log → State: [0.21939263 0.54934746 0.02170818 0.7216114  0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.26124915 0.5285271  0.01493716 0.7019381  0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.30817467 0.5200893  0.01170881 0.71895564 0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.30295113 0.5072779  0.01949031 0.67134315 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.29430822 0.539381   0.01176279 0.6402423  0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.33145463 0.5104344  0.0161185  0.64569765 0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.29636306 0.51542044 0.03095144 0.6930122  0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.28145838 0.5253706  0.04371217 0.684822   0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.29149452 0.5811463  0.04618513 0.7580567  0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.27817884 0.6408821  0.05142999 0.7876176  0.63      ], Reward: 5.0\n",
            "Processed Log → State: [0.26900408 0.6217653  0.0557141  0.75900215 0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.30219105 0.66269916 0.05986501 0.7474837  0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.29859334 0.6720757  0.08092194 0.8040575  0.72      ], Reward: 0.0\n",
            "Processed Log → State: [0.31338087 0.66722876 0.0826975  0.81485766 0.75      ], Reward: 6.0\n",
            "Processed Log → State: [0.33699593 0.6744824  0.08689642 0.87848014 0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.35054505 0.6697587  0.10475744 0.8725629  0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.35233092 0.6517845  0.11756702 0.87840074 0.84      ], Reward: 0.0\n",
            "Processed Log → State: [0.34313205 0.66576505 0.14287156 0.9050992  0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.35343373 0.6567558  0.15872225 0.89450806 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.2535802  0.385998   0.14166224 0.54640007 0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.2741589  0.43924934 0.13941325 0.53040683 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.33351588 0.42459217 0.2335168  0.54971635 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.3673741  0.46377933 0.28562286 0.53658557 0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.34688514 0.5247291  0.36649168 0.517457   0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.44599268 0.5343728  0.3710286  0.4939738  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.51549214 0.5200041  0.3765215  0.5571234  0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.5856352  0.5880684  0.45477518 0.5434133  0.24      ], Reward: 0.0\n",
            "Processed Log → State: [0.6272443  0.61479276 0.38576978 0.5164304  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.62085724 0.6052715  0.39459583 0.5177624  0.3       ], Reward: 5.0\n",
            "Processed Log → State: [0.63961285 0.6396468  0.4310232  0.52563715 0.33      ], Reward: 7.0\n",
            "Processed Log → State: [0.5770828  0.62745106 0.48383918 0.47508493 0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.52182096 0.641148   0.5054702  0.5273318  0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.55965453 0.668971   0.50597763 0.535014   0.42      ], Reward: 8.0\n",
            "Processed Log → State: [0.6546342 0.6489352 0.5518506 0.5406841 0.45     ], Reward: 3.0\n",
            "Processed Log → State: [0.70341665 0.6829115  0.5223394  0.58498424 0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.7963625  0.70031744 0.4774438  0.6738779  0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.79319227 0.7746561  0.4674537  0.6461538  0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.81545854 0.7856539  0.512975   0.6167816  0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.7926569  0.79288954 0.55233306 0.60332566 0.6       ], Reward: 0.0\n",
            "Processed Log → State: [0.79922444 0.8087595  0.6283399  0.5638976  0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.7665651  0.82786834 0.70451504 0.6554023  0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.81357896 0.845071   0.69597274 0.5958467  0.69      ], Reward: 8.0\n",
            "Processed Log → State: [0.81087476 0.8455706  0.6391724  0.56261724 0.72      ], Reward: 0.0\n",
            "Processed Log → State: [0.8700057  0.8500285  0.60824937 0.5677672  0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.87468004 0.83983386 0.67185456 0.6144704  0.78      ], Reward: 7.0\n",
            "Processed Log → State: [0.9007167  0.80824286 0.72384036 0.6142212  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.96573323 0.7626798  0.6811187  0.6974693  0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.93588567 0.7980181  0.6862649  0.71455663 0.87      ], Reward: 2.0\n",
            "Processed Log → State: [0.97773635 0.8130979  0.6696347  0.77139413 0.9       ], Reward: 6.0\n",
            "Processed Log → State: [0.27821591 0.30979517 0.05495994 0.6147106  0.03      ], Reward: -2.0\n",
            "Processed Log → State: [0.3047948  0.42967057 0.09382799 0.5606295  0.06      ], Reward: 0.0\n",
            "Processed Log → State: [0.33864626 0.37349752 0.12748303 0.5475038  0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.39804083 0.418121   0.1259036  0.46326992 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.48348758 0.47877273 0.14515078 0.29359958 0.15      ], Reward: 6.0\n",
            "Processed Log → State: [0.46684712 0.53392684 0.18836048 0.24183099 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.5302153  0.5508434  0.20543419 0.14053917 0.21      ], Reward: -2.0\n",
            "Processed Log → State: [0.5214156  0.6253901  0.25670463 0.14147373 0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.57031095 0.52113855 0.27434173 0.21464583 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.65989876 0.5774021  0.3023578  0.15751292 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.6548665 0.664301  0.2685043 0.1855342 0.33     ], Reward: 1.0\n",
            "Processed Log → State: [0.70979565 0.70577145 0.21557061 0.2218563  0.36      ], Reward: 5.0\n",
            "Processed Log → State: [0.6496767  0.71688193 0.24447392 0.28624275 0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.70683646 0.6419716  0.2564905  0.3283202  0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.76251364 0.69309974 0.24616966 0.20535354 0.45      ], Reward: -1.0\n",
            "Processed Log → State: [0.82804227 0.8257863  0.2765156  0.20767409 0.48      ], Reward: 1.0\n",
            "Processed Log → State: [0.8136818  0.8621731  0.28987828 0.2537579  0.51      ], Reward: 0.0\n",
            "Processed Log → State: [0.8438278  0.96271265 0.3084983  0.25753173 0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.944757   0.8917848  0.26604927 0.25146037 0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.9793014  0.9188187  0.24719946 0.25528863 0.6       ], Reward: -1.0\n",
            "Processed Log → State: [0.93213433 0.81256735 0.25814041 0.17517956 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.93131185 0.9501366  0.28134802 0.2955442  0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.9595889  0.9776022  0.30688974 0.31338236 0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.9904457  0.9509889  0.32110906 0.3386286  0.72      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.9730645  0.31701985 0.32891837 0.75      ], Reward: 2.0\n",
            "Processed Log → State: [0.9478885  0.98023266 0.3602756  0.29435366 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.8954066  0.8903309  0.41895723 0.3258082  0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.8064661  0.91785765 0.40909278 0.321777   0.84      ], Reward: 4.0\n",
            "Processed Log → State: [0.81739986 0.9376512  0.3927294  0.41126847 0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.78540605 0.8713043  0.34595025 0.43966633 0.9       ], Reward: -1.0\n",
            "Processed Log → State: [0.22614957 0.33004698 0.06294759 0.66413474 0.03      ], Reward: 0.0\n",
            "Processed Log → State: [0.22572057 0.35694027 0.07078664 0.6617631  0.06      ], Reward: 2.0\n",
            "Processed Log → State: [0.24766089 0.3726629  0.07083907 0.66155726 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.27742866 0.3985427  0.07538442 0.7031046  0.12      ], Reward: 6.0\n",
            "Processed Log → State: [0.31420422 0.40371034 0.07212283 0.724049   0.15      ], Reward: 0.0\n",
            "Processed Log → State: [0.34874648 0.42803615 0.06896891 0.83812344 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.36843735 0.46949306 0.07280324 0.89353395 0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.38436148 0.46554774 0.08555861 0.889803   0.24      ], Reward: 5.0\n",
            "Processed Log → State: [0.41456515 0.47600377 0.11217383 0.8731708  0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.41345578 0.4616065  0.1278946  0.8980192  0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.46201196 0.49240562 0.13684241 0.9463022  0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.4924544  0.53083515 0.15169756 0.97995806 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.47467548 0.5682613  0.15937544 1.         0.39      ], Reward: 2.0\n",
            "Processed Log → State: [0.47961462 0.5932474  0.17822765 0.9664086  0.42      ], Reward: 7.0\n",
            "Processed Log → State: [0.5046493 0.6029001 0.1895532 1.        0.45     ], Reward: 1.0\n",
            "Processed Log → State: [0.49215576 0.6077028  0.19650419 1.         0.48      ], Reward: 5.0\n",
            "Processed Log → State: [0.5343172  0.61993515 0.18752354 1.         0.51      ], Reward: -1.0\n",
            "Processed Log → State: [0.535291   0.6338236  0.19302236 1.         0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.5172325  0.6394426  0.18374693 1.         0.57      ], Reward: 6.0\n",
            "Processed Log → State: [0.5572393  0.6491716  0.18220176 0.953404   0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.5555522  0.69514555 0.18150453 1.         0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.5674257  0.7083543  0.18211322 1.         0.66      ], Reward: 5.0\n",
            "Processed Log → State: [0.5862889 0.7261732 0.2012094 1.        0.69     ], Reward: 3.0\n",
            "Processed Log → State: [0.58549106 0.745706   0.19373971 0.98856354 0.72      ], Reward: 8.0\n",
            "Processed Log → State: [0.60605115 0.7777286  0.19077812 1.         0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.6043007  0.7515081  0.18363556 1.         0.78      ], Reward: 6.0\n",
            "Processed Log → State: [0.60276365 0.7536703  0.19086678 1.         0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.563287   0.727444   0.20242602 1.         0.84      ], Reward: 6.0\n",
            "Processed Log → State: [0.52891773 0.7391636  0.2167429  1.         0.87      ], Reward: -1.0\n",
            "Processed Log → State: [0.5520247  0.7245948  0.24554425 0.9946192  0.9       ], Reward: 0.0\n",
            "Processed Log → State: [0.21722952 0.35557252 0.06345794 0.6186418  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.22999404 0.39479885 0.08119933 0.7048979  0.06      ], Reward: 7.0\n",
            "Processed Log → State: [0.25346446 0.37958336 0.08861067 0.72625226 0.09      ], Reward: 6.0\n",
            "Processed Log → State: [0.29268184 0.3976361  0.10191947 0.7960392  0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.33851764 0.45456964 0.1012021  0.86636126 0.15      ], Reward: 6.0\n",
            "Processed Log → State: [0.35995397 0.46694392 0.11299151 0.8857793  0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.41970977 0.47806424 0.11596432 0.88359267 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.44766113 0.47558838 0.11576411 0.95132    0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.4836003  0.4775118  0.11706562 0.9582261  0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.4730471  0.519209   0.12616624 1.         0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.5152948  0.5346899  0.12917776 1.         0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.574798   0.5587809  0.13808641 1.         0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.53900987 0.5825643  0.14186963 1.         0.39      ], Reward: 4.0\n",
            "Processed Log → State: [0.53186613 0.60033405 0.14953342 1.         0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.55390775 0.5809102  0.15077361 1.         0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.5449995  0.6044266  0.15505672 0.97646827 0.48      ], Reward: 8.0\n",
            "Processed Log → State: [0.5672954  0.60744524 0.16474527 1.         0.51      ], Reward: 7.0\n",
            "Processed Log → State: [0.59496015 0.63749254 0.16595653 0.9852014  0.54      ], Reward: 3.0\n",
            "Processed Log → State: [0.5886738  0.6625711  0.17709994 0.94626015 0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.6068795  0.70033896 0.18793324 1.         0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.6438256  0.72818595 0.18653601 1.         0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.65497696 0.70510405 0.18816826 0.9811265  0.66      ], Reward: 2.0\n",
            "Processed Log → State: [0.66732717 0.6901498  0.20351166 0.9815498  0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.69141126 0.7057715  0.20760705 1.         0.72      ], Reward: 6.0\n",
            "Processed Log → State: [0.6956407  0.7248532  0.19695304 1.         0.75      ], Reward: 7.0\n",
            "Processed Log → State: [0.71802324 0.69796306 0.19680955 0.9626359  0.78      ], Reward: 7.0\n",
            "Processed Log → State: [0.7167106  0.6952464  0.21179423 0.97488993 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.7070058  0.6742311  0.23755556 1.         0.84      ], Reward: 5.0\n",
            "Processed Log → State: [0.7294228  0.6690515  0.23780817 0.99191594 0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.7051168  0.6265985  0.25784528 0.9418375  0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.26065153 0.29663673 0.04422869 0.65462416 0.03      ], Reward: 2.0\n",
            "Processed Log → State: [0.26995814 0.2804573  0.03089736 0.71834165 0.06      ], Reward: 1.0\n",
            "Processed Log → State: [0.24452962 0.29827413 0.03262191 0.8044932  0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.24885733 0.3436812  0.02774473 0.9416393  0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.248983   0.33672455 0.04060574 0.94980925 0.15      ], Reward: 0.0\n",
            "Processed Log → State: [0.257351   0.3433258  0.05506288 0.99035686 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.2708687  0.347847   0.06135127 0.89771175 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.27017748 0.36998984 0.06932193 0.9185268  0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.24676593 0.3951283  0.06517919 0.8987938  0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.23653202 0.39504603 0.05633543 0.930625   0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.25381514 0.42643768 0.06175901 1.         0.33      ], Reward: 5.0\n",
            "Processed Log → State: [0.26278624 0.44772464 0.06297509 1.         0.36      ], Reward: 2.0\n",
            "Processed Log → State: [0.28286338 0.46480134 0.05723449 1.         0.39      ], Reward: 3.0\n",
            "Processed Log → State: [0.26435104 0.4363763  0.0700822  1.         0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.2847025  0.45891288 0.06188494 1.         0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.28053248 0.45614886 0.06194972 1.         0.48      ], Reward: 3.0\n",
            "Processed Log → State: [0.29507834 0.4368539  0.06111641 1.         0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.2920678  0.4289157  0.06843077 0.9548666  0.54      ], Reward: 6.0\n",
            "Processed Log → State: [0.30865183 0.4513001  0.071568   1.         0.57      ], Reward: 1.0\n",
            "Processed Log → State: [0.31544313 0.48319054 0.07432655 0.96357    0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.33634096 0.46792927 0.06474786 0.83287823 0.63      ], Reward: 5.0\n",
            "Processed Log → State: [0.3443074  0.515759   0.07578269 0.8234973  0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.38031137 0.5465376  0.08002103 0.78632593 0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.39956072 0.5508517  0.08918913 0.8098261  0.72      ], Reward: 2.0\n",
            "Processed Log → State: [0.399296   0.5859373  0.09909725 0.799991   0.75      ], Reward: 6.0\n",
            "Processed Log → State: [0.43269318 0.5521756  0.1152095  0.84454143 0.78      ], Reward: 7.0\n",
            "Processed Log → State: [0.4062605  0.56324905 0.11898474 0.8459755  0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.4016719 0.5635794 0.1347661 0.8855923 0.84     ], Reward: 3.0\n",
            "Processed Log → State: [0.3912662  0.54155463 0.16491844 0.8230218  0.87      ], Reward: 7.0\n",
            "Processed Log → State: [0.38875425 0.54008085 0.16537678 0.82961315 0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.24285024 0.36159948 0.07059249 0.6546557  0.03      ], Reward: 6.0\n",
            "Processed Log → State: [0.25676674 0.35043254 0.1168488  0.63673836 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.27362654 0.362454   0.08597139 0.6187248  0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.3348201  0.35345688 0.10249756 0.6465591  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.30164865 0.4360386  0.11883955 0.5738177  0.15      ], Reward: 1.0\n",
            "Processed Log → State: [0.3607604  0.5538259  0.07500466 0.57746637 0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.3882188  0.4666806  0.11184924 0.52971387 0.21      ], Reward: 1.0\n",
            "Processed Log → State: [0.43165824 0.52661324 0.18103337 0.48980317 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.4125424  0.5623338  0.18236683 0.4090272  0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.4152681  0.56921947 0.18971945 0.36588955 0.3       ], Reward: 8.0\n",
            "Processed Log → State: [0.49896652 0.49929094 0.21775047 0.44212875 0.33      ], Reward: 8.0\n",
            "Processed Log → State: [0.52407604 0.5809174  0.25340417 0.39936024 0.36      ], Reward: 8.0\n",
            "Processed Log → State: [0.5687858  0.68832004 0.31944123 0.32375497 0.39      ], Reward: 5.0\n",
            "Processed Log → State: [0.56029737 0.7040751  0.355764   0.31683224 0.42      ], Reward: 3.0\n",
            "Processed Log → State: [0.49599978 0.6594286  0.38363537 0.31270206 0.45      ], Reward: 5.0\n",
            "Processed Log → State: [0.5585449  0.6866463  0.36056077 0.2912942  0.48      ], Reward: -2.0\n",
            "Processed Log → State: [0.56820714 0.6520538  0.39391944 0.25738743 0.51      ], Reward: 2.0\n",
            "Processed Log → State: [0.5900554  0.68697846 0.33914185 0.25979707 0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.5339321  0.7504742  0.33436537 0.30561742 0.57      ], Reward: -1.0\n",
            "Processed Log → State: [0.55122364 0.736821   0.32671508 0.28529823 0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.5859489  0.80356586 0.33559087 0.32775763 0.63      ], Reward: 8.0\n",
            "Processed Log → State: [0.66978353 0.77941144 0.3362388  0.36894628 0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.65005344 0.8647313  0.35106826 0.3791282  0.69      ], Reward: 3.0\n",
            "Processed Log → State: [0.68679154 0.9674043  0.33227316 0.4614662  0.72      ], Reward: 7.0\n",
            "Processed Log → State: [0.692558   0.97091514 0.35329652 0.45660818 0.75      ], Reward: 3.0\n",
            "Processed Log → State: [0.7070304  0.9837086  0.32756776 0.45504045 0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.6784054  0.9953484  0.34451243 0.49150473 0.81      ], Reward: 3.0\n",
            "Processed Log → State: [0.6554333  0.9532465  0.4109724  0.49931872 0.84      ], Reward: 8.0\n",
            "Processed Log → State: [0.75533926 0.9564771  0.40084556 0.53828615 0.87      ], Reward: 6.0\n",
            "Processed Log → State: [0.7951619  0.93551683 0.42188945 0.5466311  0.9       ], Reward: 2.0\n",
            "Processed Log → State: [0.24563242 0.33383244 0.09696046 0.55800235 0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.20978133 0.37765905 0.23326077 0.5655278  0.06      ], Reward: 5.0\n",
            "Processed Log → State: [0.22332785 0.39464924 0.25295687 0.48393312 0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.2536292  0.38700557 0.25285783 0.38831833 0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.30905038 0.4499439  0.28038657 0.33034393 0.15      ], Reward: 0.0\n",
            "Processed Log → State: [0.32834524 0.5027535  0.32674435 0.2391831  0.18      ], Reward: 3.0\n",
            "Processed Log → State: [0.38364354 0.5109178  0.32048696 0.21598342 0.21      ], Reward: 3.0\n",
            "Processed Log → State: [0.32040638 0.5388574  0.30815667 0.2062621  0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.33888128 0.55115676 0.2959899  0.19211975 0.27      ], Reward: 1.0\n",
            "Processed Log → State: [0.38199633 0.5409096  0.33017236 0.11445628 0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.4263452  0.6052601  0.41943735 0.09245573 0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.4309577  0.61641866 0.5565535  0.13436614 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.45531866 0.6235728  0.5975511  0.16787723 0.39      ], Reward: 1.0\n",
            "Processed Log → State: [0.53332776 0.6633545  0.66522205 0.17183337 0.42      ], Reward: 7.0\n",
            "Processed Log → State: [0.5405894  0.69975364 0.69131315 0.1535528  0.45      ], Reward: 1.0\n",
            "Processed Log → State: [0.5715149  0.71837795 0.7467531  0.11288412 0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.59394264 0.71107507 0.8688649  0.1278241  0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.5758046  0.7579261  0.9269907  0.27162945 0.54      ], Reward: 8.0\n",
            "Processed Log → State: [0.6886071  0.7824144  0.9060481  0.21329007 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.6514638  0.80397767 0.92635965 0.26548043 0.6       ], Reward: 6.0\n",
            "Processed Log → State: [0.73148656 0.8279788  0.9833947  0.28236577 0.63      ], Reward: 3.0\n",
            "Processed Log → State: [0.71273565 0.85789144 0.96355695 0.24222611 0.66      ], Reward: 0.0\n",
            "Processed Log → State: [0.7018646  0.88473415 0.9369122  0.24773455 0.69      ], Reward: 0.0\n",
            "Processed Log → State: [0.79230255 0.89323246 0.981279   0.24797003 0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.8714282  0.8663104  0.94445616 0.31964692 0.75      ], Reward: 6.0\n",
            "Processed Log → State: [0.8379404  0.8775399  1.         0.31637564 0.78      ], Reward: 3.0\n",
            "Processed Log → State: [0.84068435 0.84254867 1.         0.28082535 0.81      ], Reward: 0.0\n",
            "Processed Log → State: [0.7789646  0.8516073  0.9916985  0.31508482 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.7609421  0.81670666 1.         0.3359356  0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.7469765  0.7860825  1.         0.37289125 0.9       ], Reward: 1.0\n",
            "Processed Log → State: [0.18892427 0.31828097 0.06773967 0.6288926  0.03      ], Reward: 2.0\n",
            "Processed Log → State: [0.21488951 0.37709227 0.09745593 0.70534295 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.22548564 0.40887502 0.10509393 0.7199938  0.09      ], Reward: 7.0\n",
            "Processed Log → State: [0.2238537  0.43564636 0.1056593  0.8140278  0.12      ], Reward: 3.0\n",
            "Processed Log → State: [0.25023124 0.43471164 0.11467992 0.85361665 0.15      ], Reward: 2.0\n",
            "Processed Log → State: [0.2624227  0.45663416 0.12278633 0.915623   0.18      ], Reward: 1.0\n",
            "Processed Log → State: [0.28970173 0.48653105 0.12800442 0.9218426  0.21      ], Reward: 8.0\n",
            "Processed Log → State: [0.29107043 0.5011996  0.15637733 0.924577   0.24      ], Reward: 3.0\n",
            "Processed Log → State: [0.28441504 0.5095824  0.17212376 0.94107044 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.30788553 0.49726573 0.17489758 0.96752274 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.36047772 0.5325112  0.17344178 1.         0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.41534156 0.5650284  0.16693494 1.         0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.447978   0.5812482  0.17223583 1.         0.39      ], Reward: 5.0\n",
            "Processed Log → State: [0.45121908 0.6005815  0.19056761 1.         0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.49478337 0.6084454  0.19607058 1.         0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.49124566 0.59684175 0.2023099  1.         0.48      ], Reward: 7.0\n",
            "Processed Log → State: [0.5176917  0.61668974 0.18568751 1.         0.51      ], Reward: 3.0\n",
            "Processed Log → State: [0.56733394 0.6467009  0.17563047 0.99979705 0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.5750975  0.6587177  0.18823054 0.99911326 0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.59206563 0.6660522  0.17300655 1.         0.6       ], Reward: 3.0\n",
            "Processed Log → State: [0.5946366  0.70114297 0.17354433 0.95766926 0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.6183358  0.7243258  0.16758673 0.9645436  0.66      ], Reward: 8.0\n",
            "Processed Log → State: [0.5764617  0.70757055 0.15385848 1.         0.69      ], Reward: 2.0\n",
            "Processed Log → State: [0.5794826  0.7155195  0.15436666 1.         0.72      ], Reward: 7.0\n",
            "Processed Log → State: [0.5923645  0.7227317  0.15930311 1.         0.75      ], Reward: 8.0\n",
            "Processed Log → State: [0.62289274 0.7284166  0.1643731  0.99676174 0.78      ], Reward: 1.0\n",
            "Processed Log → State: [0.6169849  0.7121098  0.17640321 0.9778551  0.81      ], Reward: 0.0\n",
            "Processed Log → State: [0.63399065 0.70766896 0.19718224 0.9507277  0.84      ], Reward: 4.0\n",
            "Processed Log → State: [0.65760434 0.71249646 0.21151686 0.9678165  0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.62483627 0.73588556 0.22258833 0.995824   0.9       ], Reward: 3.0\n",
            "Processed Log → State: [0.23426844 0.36007085 0.16825508 0.62229586 0.03      ], Reward: 0.0\n",
            "Processed Log → State: [0.21990883 0.44821978 0.2079728  0.57438487 0.06      ], Reward: 3.0\n",
            "Processed Log → State: [0.2484365  0.45466563 0.20320185 0.58579    0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.35037732 0.47017047 0.2682681  0.55145824 0.12      ], Reward: 8.0\n",
            "Processed Log → State: [0.34640312 0.492381   0.33826014 0.48860207 0.15      ], Reward: 2.0\n",
            "Processed Log → State: [0.382221   0.5565817  0.38975    0.41654268 0.18      ], Reward: 5.0\n",
            "Processed Log → State: [0.38117358 0.5685545  0.42543504 0.39210868 0.21      ], Reward: 2.0\n",
            "Processed Log → State: [0.492708   0.607196   0.51485586 0.40076736 0.24      ], Reward: 2.0\n",
            "Processed Log → State: [0.42531186 0.6519842  0.5413509  0.38541964 0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.47026142 0.6575479  0.637535   0.403093   0.3       ], Reward: 1.0\n",
            "Processed Log → State: [0.5084722  0.69634765 0.76454395 0.43238142 0.33      ], Reward: 0.0\n",
            "Processed Log → State: [0.5133599 0.7184168 0.7739246 0.4003612 0.36     ], Reward: 8.0\n",
            "Processed Log → State: [0.47039956 0.75883174 0.8263751  0.4259635  0.39      ], Reward: 7.0\n",
            "Processed Log → State: [0.44205993 0.77875006 0.82877314 0.4372792  0.42      ], Reward: 6.0\n",
            "Processed Log → State: [0.46474123 0.7872545  0.8627053  0.32373106 0.45      ], Reward: 3.0\n",
            "Processed Log → State: [0.46623066 0.7563209  0.885302   0.25427404 0.48      ], Reward: 4.0\n",
            "Processed Log → State: [0.55639935 0.7598199  0.8781557  0.2501203  0.51      ], Reward: 6.0\n",
            "Processed Log → State: [0.62161565 0.75371253 0.81962323 0.3148223  0.54      ], Reward: 1.0\n",
            "Processed Log → State: [0.6959335  0.8017788  0.8594041  0.30409977 0.57      ], Reward: 3.0\n",
            "Processed Log → State: [0.727452   0.7959672  0.83622205 0.19178848 0.6       ], Reward: 2.0\n",
            "Processed Log → State: [0.8174083  0.8134569  0.8798467  0.24663733 0.63      ], Reward: 0.0\n",
            "Processed Log → State: [0.7937044 0.7964234 0.8351864 0.3168617 0.66     ], Reward: 4.0\n",
            "Processed Log → State: [0.78415036 0.7835872  0.8517365  0.38436332 0.69      ], Reward: 1.0\n",
            "Processed Log → State: [0.70106393 0.7997096  0.7951871  0.3204272  0.72      ], Reward: 1.0\n",
            "Processed Log → State: [0.6474192  0.7871489  0.76135075 0.3055273  0.75      ], Reward: -1.0\n",
            "Processed Log → State: [0.66843873 0.8328348  0.726501   0.32181728 0.78      ], Reward: 7.0\n",
            "Processed Log → State: [0.77410495 0.7841869  0.7329122  0.31958374 0.81      ], Reward: 0.0\n",
            "Processed Log → State: [0.77578735 0.80789876 0.76424396 0.38040742 0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.8326356  0.8285833  0.8156228  0.44279322 0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.92013156 0.79842246 0.8340286  0.45810756 0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.2216058  0.34828007 0.05355056 0.63696563 0.03      ], Reward: 3.0\n",
            "Processed Log → State: [0.20932478 0.37384793 0.05996611 0.6140062  0.06      ], Reward: 8.0\n",
            "Processed Log → State: [0.21295951 0.3596213  0.04925139 0.7228143  0.09      ], Reward: 3.0\n",
            "Processed Log → State: [0.19945739 0.3614688  0.04480979 0.75657827 0.12      ], Reward: 1.0\n",
            "Processed Log → State: [0.22816296 0.35273036 0.06057844 0.73962694 0.15      ], Reward: 3.0\n",
            "Processed Log → State: [0.23870139 0.3852655  0.0560988  0.72951514 0.18      ], Reward: 8.0\n",
            "Processed Log → State: [0.28137678 0.44258478 0.06250297 0.80775136 0.21      ], Reward: -1.0\n",
            "Processed Log → State: [0.3093052  0.47367686 0.07295168 0.81309605 0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.31818998 0.438149   0.06690783 0.87269586 0.27      ], Reward: 3.0\n",
            "Processed Log → State: [0.33788064 0.43371382 0.06394153 0.88613755 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.36705372 0.46887237 0.07622159 1.         0.33      ], Reward: 6.0\n",
            "Processed Log → State: [0.37682673 0.4542383  0.0787831  0.95251894 0.36      ], Reward: 1.0\n",
            "Processed Log → State: [0.40394974 0.47249943 0.07447161 0.8963344  0.39      ], Reward: 7.0\n",
            "Processed Log → State: [0.42809185 0.45474157 0.08059654 0.99349743 0.42      ], Reward: 1.0\n",
            "Processed Log → State: [0.42513892 0.47737595 0.08946513 0.99353904 0.45      ], Reward: 2.0\n",
            "Processed Log → State: [0.41422185 0.48767796 0.08652449 1.         0.48      ], Reward: 0.0\n",
            "Processed Log → State: [0.41973493 0.5201311  0.10092815 1.         0.51      ], Reward: 5.0\n",
            "Processed Log → State: [0.44617635 0.5519107  0.10684322 0.960791   0.54      ], Reward: 0.0\n",
            "Processed Log → State: [0.4796628  0.5713394  0.09494425 0.9866465  0.57      ], Reward: 8.0\n",
            "Processed Log → State: [0.5008096  0.5736141  0.0872934  0.97034144 0.6       ], Reward: 1.0\n",
            "Processed Log → State: [0.52418333 0.5854001  0.08807931 0.99989283 0.63      ], Reward: 1.0\n",
            "Processed Log → State: [0.51407284 0.5801066  0.09650454 0.99465925 0.66      ], Reward: 3.0\n",
            "Processed Log → State: [0.5250607  0.55809253 0.10366292 0.97191936 0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.5229484  0.5409077  0.11645547 1.         0.72      ], Reward: 0.0\n",
            "Processed Log → State: [0.5460639  0.52967185 0.12668337 0.98340017 0.75      ], Reward: 0.0\n",
            "Processed Log → State: [0.5240909  0.5347583  0.12155933 0.9986751  0.78      ], Reward: 8.0\n",
            "Processed Log → State: [0.51278734 0.5521874  0.131356   0.97228277 0.81      ], Reward: 5.0\n",
            "Processed Log → State: [0.5455801  0.55775756 0.14010859 1.         0.84      ], Reward: 1.0\n",
            "Processed Log → State: [0.5211828  0.57235426 0.17779286 1.         0.87      ], Reward: 8.0\n",
            "Processed Log → State: [0.51283    0.5585234  0.19464548 0.9438658  0.9       ], Reward: 8.0\n",
            "Processed Log → State: [0.23647554 0.3407027  0.12926863 0.6078615  0.03      ], Reward: 1.0\n",
            "Processed Log → State: [0.39745945 0.3510116  0.15822937 0.569271   0.06      ], Reward: 0.0\n",
            "Processed Log → State: [0.50221837 0.39599407 0.32095322 0.55971    0.09      ], Reward: 1.0\n",
            "Processed Log → State: [0.58363396 0.39578888 0.40266377 0.48418093 0.12      ], Reward: 2.0\n",
            "Processed Log → State: [0.67524916 0.38871974 0.42452806 0.51289093 0.15      ], Reward: 0.0\n",
            "Processed Log → State: [0.6936663  0.39319304 0.4616563  0.47836185 0.18      ], Reward: 2.0\n",
            "Processed Log → State: [0.6962411  0.41633874 0.5520088  0.48484907 0.21      ], Reward: 7.0\n",
            "Processed Log → State: [0.6674253  0.47124743 0.64594656 0.4553391  0.24      ], Reward: 1.0\n",
            "Processed Log → State: [0.62407094 0.53711414 0.6005832  0.4456032  0.27      ], Reward: 8.0\n",
            "Processed Log → State: [0.6404929  0.58501565 0.6752643  0.38702008 0.3       ], Reward: 3.0\n",
            "Processed Log → State: [0.7378345  0.64586234 0.7211481  0.3166627  0.33      ], Reward: 1.0\n",
            "Processed Log → State: [0.8599462  0.7174074  0.7789069  0.23888516 0.36      ], Reward: 3.0\n",
            "Processed Log → State: [0.77809507 0.7544227  0.78210956 0.18426526 0.39      ], Reward: 4.0\n",
            "Processed Log → State: [0.8268366  0.7489354  0.7502769  0.16712497 0.42      ], Reward: 7.0\n",
            "Processed Log → State: [0.89891404 0.7503651  0.8039394  0.15813468 0.45      ], Reward: 8.0\n",
            "Processed Log → State: [0.9962033  0.7460646  0.82785934 0.16720714 0.48      ], Reward: 6.0\n",
            "Processed Log → State: [1.        0.7533404 0.7956502 0.2320437 0.51     ], Reward: 7.0\n",
            "Processed Log → State: [1.         0.75571126 0.9419272  0.2134268  0.54      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.79010063 0.8978801  0.28251287 0.57      ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.8199031  0.85362965 0.2963851  0.6       ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.8059565  0.83305895 0.34337363 0.63      ], Reward: 6.0\n",
            "Processed Log → State: [1.         0.8323804  0.7430672  0.35425717 0.66      ], Reward: 1.0\n",
            "Processed Log → State: [1.         0.81529194 0.8216154  0.41985768 0.69      ], Reward: 6.0\n",
            "Processed Log → State: [0.9320291  0.7901496  0.80349046 0.45680335 0.72      ], Reward: 0.0\n",
            "Processed Log → State: [1.         0.7854587  0.8298708  0.41118273 0.75      ], Reward: 5.0\n",
            "Processed Log → State: [0.9587057  0.81237864 0.84159344 0.44430426 0.78      ], Reward: 7.0\n",
            "Processed Log → State: [0.9752469  0.8145863  0.84433055 0.5191825  0.81      ], Reward: 1.0\n",
            "Processed Log → State: [0.9818468  0.7918901  0.83372754 0.5120362  0.84      ], Reward: 3.0\n",
            "Processed Log → State: [1.         0.7438355  0.7902749  0.43849376 0.87      ], Reward: 1.0\n",
            "Processed Log → State: [0.9693826  0.76848    0.7703369  0.44450247 0.9       ], Reward: 6.0\n",
            "Training complete and model saved.\n",
            "Recommended action after training on batch: 4\n"
          ]
        }
      ]
    }
  ]
}