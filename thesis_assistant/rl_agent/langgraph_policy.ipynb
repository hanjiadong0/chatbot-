{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# RL Configuration Manager\n","\n","This module provides a centralized configuration interface for the RL-based educational coaching system. It manages all static settings required by the agent and environment to interpret observations, execute actions, and compute rewards.\n","\n","The configuration is saved and loaded from a JSON file (`rl_config.json`). If the file does not exist, a default configuration is created automatically. This ensures a reproducible setup and simplifies testing across modules.\n","\n","## Configuration Structure\n","\n","- `state_variables`:  \n","  Defines the numerical features observed by the RL agent at each time step. These features represent the student’s academic state, ethical posture, writing progression, and contextual traits.\n","\n","- `actions`:  \n","  A dictionary of discrete, recommendation-style actions the agent can select. Each key is an action ID (e.g. `\"eth_0\"`), and each value is a human-readable description.\n","\n","- `reward_config`:  \n","  Defines the scalar reward shaping used during training. Rewards and penalties reflect fluency, ethical alignment, advisor trust, autonomy, and creativity.\n","\n","- `action_effects`:  \n","  Maps agent actions to updates in the simulated state. Each action has associated side-effects that alter one or more student-related variables (e.g., reducing `ai_usage` or increasing `advisor_trust`).\n","\n","## Usage Example\n","\n","```python\n","\n","from rl_config_manager import RLConfigManager\n","\n","# Load existing configuration or initialize default\n","config = RLConfigManager.load_config()\n","\n","# Access specific parts\n","print(\"Available state variables:\")\n","for var in config[\"state_variables\"]:\n","    print(\"-\", var)"],"metadata":{"id":"nU9SgNrzw42q"}},{"cell_type":"code","source":["# rl_config_manager.py\n","\"\"\"\n","Adaptive RL Optimizer Configuration Manager\n","\n","Defines and enforces the JSON schema for the RL-based academic coaching agent’s\n","centralized configuration. Uses Pydantic for validation and metadata on versioning,\n","timestamps, reward rationales, and action-to-state effect mappings.\n","\"\"\"\n","import json\n","import os\n","import shutil\n","from datetime import datetime\n","from pydantic import BaseModel, Field, field_validator\n","from typing import Dict, List, Any\n","\n","\n","# ----------------------------- Schema Definitions -----------------------------\n","\n","\n","class RewardItem(BaseModel):\n","    \"\"\"\n","    Represents a single reward entry in the multi-objective shaping scheme.\n","\n","    Attributes:\n","        value (float):\n","            The numerical reward (positive or negative) assigned for this signal.\n","        justification (str):\n","            A brief rationale (min. 5 characters) explaining why this reward level was chosen.\n","        risk (str):\n","            A description (min. 5 characters) of any potential downside or misuse if over-emphasized.\n","    \"\"\"\n","    value: float\n","    justification: str = Field(..., min_length=5)\n","    risk: str = Field(..., min_length=5)\n","\n","class ConfigSchema(BaseModel):\n","    \"\"\"\n","    Full schema for the RL configuration file. Validates metadata, state/action definitions,\n","    reward shaping, and the mapping from chosen actions to changes in latent state variables.\n","\n","    Attributes:\n","        config_version (float):\n","            Version number of the configuration. Must be ≥ 1.0 to ensure compatibility.\n","        created_at (str):\n","            ISO-formatted timestamp marking when this config was generated or last updated.\n","        state_variables (List[str]):\n","            Ordered list of state feature names that the RL agent will observe.\n","        actions (Dict[str, str]):\n","            Mapping from action keys (e.g., \"eth_0\") to human-readable labels.\n","        reward_config (Dict[str, RewardItem]):\n","            Detailed reward items for each supervision signal, including value, justification, and risk.\n","        action_effects (Dict[str, Dict[str, float]]):\n","            Defines how each action modifies one or more state variables, as a delta change.\n","    \"\"\"\n","    config_version: float\n","    created_at:       str\n","    state_variables:  List[str]\n","    actions:          Dict[str, str]\n","    reward_config:    Dict[str, RewardItem]\n","    action_effects:   Dict[str, Dict[str, float]]\n","\n","    @field_validator(\"config_version\")\n","    @classmethod\n","    def validate_version(cls, v):\n","        \"\"\"\n","        Ensure the configuration version is compatible with the agent's codebase.\n","\n","        Args:\n","            v (float): The version number provided in the JSON.\n","\n","        Returns:\n","            float: The same version number, if it meets requirements.\n","\n","        Raises:\n","            ValueError: If v < 1.0, indicating an unsupported config format.\n","        \"\"\"\n","        if v < 1.0:\n","            raise ValueError(\"Config version must be ≥ 1.0\")\n","        return v\n","# ----------------------------- Configuration Manager -----------------------------\n","class ConfigIO:\n","    \"\"\"\n","    Pure I/O: load & save the RL optimizer’s JSON config, with safe versioning and backups.\n","\n","    - If no config file exists, write out a default config (DEFAULT).\n","    - On first save(): stamps `created_at`.\n","    - On every save(): increments `config_version` by +0.1 and makes a timestamped backup of the old file.\n","    \"\"\"\n","    CONFIG_FILE = \"rl_config.json\"\n","    BACKUP_DIR  = \"config_backups\"\n","\n","    # Default config skeleton—matches your full specification\n","    DEFAULT: Dict[str, Any] = {\n","        \"config_version\": 1.0,\n","        \"created_at\":     \"\",   # filled in when we first save()\n","        \"state_variables\": [\n","            \"ai_usage\", \"ethical_flags\", \"advisor_trust\",\n","            \"thesis_quality\", \"deadline_ratio\", \"thesis_difficulty\",\n","            \"student_autonomy\", \"language_proficiency\",\n","            \"emotional_state\", \"creativity_score\", \"timestep\"\n","        ],\n","        \"actions\": {\n","            \"eth_0\": \"Display ethical reminder\",\n","            \"eth_1\": \"Propose AI restriction\",\n","            \"eth_2\": \"Recommend advisor check-in\",\n","            \"eth_3\": \"Log academic concern\",\n","            \"brain_0\": \"Prompt open-ended reflection\",\n","            \"brain_1\": \"Offer question inversion\",\n","            \"brain_2\": \"Stimulate cross-topic merge\",\n","            \"brain_3\": \"Show novelty heatmap\",\n","            \"write_0\": \"Suggest rewriting section\",\n","            \"write_1\": \"Recommend outline reform\",\n","            \"write_2\": \"Display writing tip\",\n","            \"write_3\": \"Enable feedback loop\",\n","            \"emo_0\": \"Encourage autonomy\",\n","            \"emo_1\": \"Acknowledge deadline stress\",\n","            \"emo_2\": \"Suggest micro-break\",\n","            \"emo_3\": \"Offer motivational boost\"\n","        },\n","        \"reward_config\": {\n","            \"fluency_improved\": {\n","                \"value\": 1.5,\n","                \"justification\": \"Improves clarity and coherence\",\n","                \"risk\": \"May incentivize style over substance\"\n","            },\n","            \"trust_earned\": {\n","                \"value\": 2.0,\n","                \"justification\": \"Advisor feedback acknowledged and used\",\n","                \"risk\": \"May reward form without deep content change\"\n","            },\n","            \"creativity_expressed\": {\n","                \"value\": 2.5,\n","                \"justification\": \"Encourages safe novelty and synthesis\",\n","                \"risk\": \"May drift into irrelevant tangents\"\n","            },\n","            \"autonomy_respected\": {\n","                \"value\": 1.0,\n","                \"justification\": \"Student took initiative\",\n","                \"risk\": \"Passive neglect might appear as autonomy\"\n","            },\n","            \"ai_dependency_violation\": {\n","                \"value\": -4.0,\n","                \"justification\": \"Detected AI overuse\",\n","                \"risk\": \"Could punish legitimate drafting support\"\n","            },\n","            \"ethical_boundary_crossed\": {\n","                \"value\": -6.0,\n","                \"justification\": \"Clear breach of academic norms\",\n","                \"risk\": \"Non-compensable — agent must intervene\"\n","            },\n","            \"deadline_panic_detected\": {\n","                \"value\": -1.0,\n","                \"justification\": \"Urgency spike detected\",\n","                \"risk\": \"Might suppress productive deadline use\"\n","            },\n","            \"milestone_completed\": {\n","                \"value\": 5.0,\n","                \"justification\": \"Goal achieved within scope\",\n","                \"risk\": \"May mask ethics issues if used alone\"\n","            },\n","            \"novel_but_safe\": {\n","                \"value\": 3.0,\n","                \"justification\": \"Original idea aligned with context\",\n","                \"risk\": \"Requires semantic checking\"\n","            },\n","            \"supervisor_disappointment\": {\n","                \"value\": -5.0,\n","                \"justification\": \"Advisor flags trust breakdown\",\n","                \"risk\": \"Recovery should be possible over time\"\n","            }\n","        },\n","        \"action_effects\": {\n","            \"eth_0\": {\"ethical_flags\": -0.1},\n","            \"eth_1\": {\"ai_usage\": -0.2},\n","            \"eth_2\": {\"advisor_trust\": 0.15},\n","            \"eth_3\": {\"ethical_flags\": 0.2, \"advisor_trust\": -0.3},\n","            \"brain_0\": {\"creativity_score\": 0.05},\n","            \"brain_1\": {\"creativity_score\": 0.07},\n","            \"brain_2\": {\"creativity_score\": 0.10},\n","            \"brain_3\": {\"thesis_quality\": 0.05},\n","            \"write_0\": {\"thesis_quality\": 0.10},\n","            \"write_1\": {\"thesis_quality\": 0.07},\n","            \"write_2\": {\"thesis_quality\": 0.05},\n","            \"write_3\": {\"thesis_quality\": 0.05, \"advisor_trust\": 0.1},\n","            \"emo_0\": {\"student_autonomy\": 0.1},\n","            \"emo_1\": {\"emotional_state\": -0.05},\n","            \"emo_2\": {\"emotional_state\": 0.1},\n","            \"emo_3\": {\"emotional_state\": 0.15}\n","        }\n","    }\n","\n","    @classmethod\n","    def load(cls) -> Dict[str, Any]:\n","        \"\"\"\n","        Load the JSON config from disk. If missing, write DEFAULT → disk then load.\n","        \"\"\"\n","        if not os.path.exists(cls.CONFIG_FILE):\n","            os.makedirs(cls.BACKUP_DIR, exist_ok=True)\n","            with open(cls.CONFIG_FILE, \"w\") as f:\n","                json.dump(cls.DEFAULT, f, indent=4)\n","        with open(cls.CONFIG_FILE, \"r\") as f:\n","            return json.load(f)\n","\n","    @classmethod\n","    def save(cls, config: Dict[str, Any]) -> None:\n","        \"\"\"\n","        Persist `config` back to disk safely:\n","\n","        1) On first ever save, fill in `created_at`.\n","        2) Always bump `config_version` by +0.1.\n","        3) Copy the old file into `config_backups/rl_config_<timestamp>.json`.\n","        4) Overwrite the main JSON.\n","        \"\"\"\n","        # 1) stamp created_at if missing\n","        if not config.get(\"created_at\"):\n","            config[\"created_at\"] = datetime.utcnow().isoformat()\n","\n","        # 2) bump version\n","        config[\"config_version\"] = round(config.get(\"config_version\", 1.0) + 0.1, 2)\n","\n","        # 3) backup old\n","        if os.path.exists(cls.CONFIG_FILE):\n","            ts = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n","            os.makedirs(cls.BACKUP_DIR, exist_ok=True)\n","            shutil.copy(\n","                cls.CONFIG_FILE,\n","                os.path.join(cls.BACKUP_DIR, f\"rl_config_{ts}.json\")\n","            )\n","\n","        # 4) write new\n","        with open(cls.CONFIG_FILE, \"w\") as f:\n","            json.dump(config, f, indent=4)\n","\n","class StateVarManager:\n","    \"\"\"CRUD for state_variables through a validated ConfigSchema.\"\"\"\n","    def __init__(self, io: ConfigIO):\n","        self.io    = io\n","        self.model = ConfigSchema(**io.load())\n","\n","    def list(self) -> List[str]:\n","        return self.model.state_variables\n","\n","    def add(self, var: str):\n","        if var not in self.model.state_variables:\n","            self.model.state_variables.append(var)\n","            self.io.save(self.model.model_dump())\n","\n","    def remove(self, var: str):\n","        if var in self.model.state_variables:\n","            self.model.state_variables.remove(var)\n","            self.io.save(self.model.model_dump())\n","\n","class ActionManager:\n","    \"\"\"CRUD for actions & their effects, via a validated ConfigSchema.\"\"\"\n","    def __init__(self, io: ConfigIO):\n","        self.io    = io\n","        self.model = ConfigSchema(**io.load())\n","\n","    def list(self) -> Dict[str,str]:\n","        return self.model.actions\n","\n","    def add(self, key:str, label:str):\n","        self.model.actions[key] = label\n","        self.io.save(self.model.model_dump())\n","\n","    def remove(self, key:str):\n","        self.model.actions.pop(key, None)\n","        self.model.action_effects.pop(key, None)\n","        self.io.save(self.model.model_dump())\n","\n","    def list_effects(self) -> Dict[str,Dict[str,float]]:\n","        return self.model.action_effects\n","\n","    def set_effect(self, action_key:str, var:str, delta:float):\n","        self.model.action_effects.setdefault(action_key, {})[var] = delta\n","        self.io.save(self.model.model_dump())\n","\n","    def remove_effect(self, action_key:str, var:str):\n","        self.model.action_effects.get(action_key, {}).pop(var, None)\n","        self.io.save(self.model.model_dump())\n","\n","class RewardManager:\n","    \"\"\"CRUD for reward_config (with full RewardItem metadata).\"\"\"\n","    def __init__(self, io: ConfigIO):\n","        self.io    = io\n","        self.model = ConfigSchema(**io.load())\n","\n","    def list(self) -> Dict[str,RewardItem]:\n","        return self.model.reward_config\n","\n","    def set(self, key:str, item:RewardItem):\n","        self.model.reward_config[key] = item\n","        self.io.save(self.model.model_dump())\n","\n","    def remove(self, key:str):\n","        self.model.reward_config.pop(key, None)\n","        self.io.save(self.model.model_dump())\n","\n","\n","# -------------------------- Top-level Façade Class ----------------------------\n","\n","class RLConfigManager:\n","    \"\"\"\n","    Unified facade: bundles I/O + per-domain managers, all validated\n","    by ConfigSchema on each save() call.\n","    \"\"\"\n","    def __init__(self):\n","        self.io   = ConfigIO()\n","        self.vars = StateVarManager(self.io)\n","        self.act  = ActionManager(self.io)\n","        self.rwd  = RewardManager(self.io)\n","\n","    @classmethod\n","    def default_config(cls) -> Dict[str, Any]:\n","        \"\"\"Returns the default configuration dictionary.\"\"\"\n","        return ConfigIO.DEFAULT\n","\n","    def load_config(self) -> Dict[str, Any]:\n","        \"\"\"Loads the configuration from disk and returns it as a dictionary.\"\"\"\n","        return self.io.load()\n","\n","    def save_config(self, config: Dict[str, Any]) -> None:\n","        \"\"\"Saves the given configuration dictionary to disk.\"\"\"\n","        self.io.save(config)\n","\n","\n","# ------------------------------ Quick Smoke-test ------------------------------\n","\n","if __name__==\"__main__\":\n","    print(\"--- Smoke Test ---\")\n","    # Create a temporary configuration object without saving\n","    temp_io = type('TempConfigIO', (object,), {\n","        'load': lambda self: ConfigIO.DEFAULT,\n","        'save': lambda self, config: None # Do nothing on save\n","    })()\n","    mgr = RLConfigManager()\n","    mgr.io = temp_io # Replace the real IO with the temporary one\n","    mgr.vars = StateVarManager(mgr.io)\n","    mgr.act = ActionManager(mgr.io)\n","    mgr.rwd = RewardManager(mgr.io)\n","\n","\n","    print(\"\\nState Variables:\")\n","    initial_vars = mgr.vars.list()\n","    print(f\"- Initial: {len(initial_vars)} variables\")\n","    mgr.vars.add(\"time_spent_recent\")\n","    print(\"- Added: 'time_spent_recent'\")\n","    print(f\"- After add: {len(mgr.vars.list())} variables\")\n","    mgr.vars.remove(\"time_spent_recent\")\n","    print(\"- Removed: 'time_spent_recent'\")\n","    print(f\"- After remove: {len(mgr.vars.list())} variables\")\n","\n","\n","    print(\"\\nActions:\")\n","    initial_actions = mgr.act.list()\n","    print(f\"- Initial: {len(initial_actions)} actions\")\n","    mgr.act.add(\"tmp_0\",\"Demo action\")\n","    print(\"- Added: 'tmp_0': 'Demo action'\")\n","    print(f\"- With tmp_0: {len(mgr.act.list())} actions\")\n","    mgr.act.remove(\"tmp_0\")\n","    print(\"- Removed: 'tmp_0'\")\n","    print(f\"- After remove: {len(mgr.act.list())} actions\")\n","\n","\n","    print(\"\\nEffects:\")\n","    initial_effects = mgr.act.list_effects()\n","    print(f\"- Initial effects for 'eth_0': {initial_effects.get('eth_0', {})}\")\n","    mgr.act.set_effect(\"eth_0\",\"ai_usage\",-0.05)\n","    print(\"- Set effect for 'eth_0': 'ai_usage' = -0.05\")\n","    print(f\"- After set effects for 'eth_0': {mgr.act.list_effects().get('eth_0', {})}\")\n","    mgr.act.remove_effect(\"eth_0\",\"ai_usage\")\n","    print(\"- Removed effect for 'eth_0': 'ai_usage'\")\n","    print(f\"- After remove effects for 'eth_0': {mgr.act.list_effects().get('eth_0', {})}\")\n","\n","\n","    print(\"\\nRewards:\")\n","    initial_rewards = mgr.rwd.list()\n","    print(f\"- Initial: {len(initial_rewards)} rewards\")\n","    demo = RewardItem(\n","        value=0.7,\n","        justification=\"Demo done\",\n","        risk=\"minimal potential\"\n","    )\n","    mgr.rwd.set(\"demo_reward\", demo)\n","    print(f\"- Added 'demo_reward': {demo}\")\n","    print(f\"- With demo_reward: {len(mgr.rwd.list())} rewards\")\n","    mgr.rwd.remove(\"demo_reward\")\n","    print(\"- Removed: 'demo_reward'\")\n","    print(f\"- After remove: {len(mgr.rwd.list())} rewards\")\n","\n","\n","    print(\"\\n--- Smoke Test Complete ---\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4aRzt6bV5e5Q","outputId":"e5c42435-6ea7-4c0d-9786-711bd24274b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Smoke Test ---\n","\n","State Variables:\n","- Initial: 11 variables\n","- Added: 'time_spent_recent'\n","- After add: 12 variables\n","- Removed: 'time_spent_recent'\n","- After remove: 11 variables\n","\n","Actions:\n","- Initial: 16 actions\n","- Added: 'tmp_0': 'Demo action'\n","- With tmp_0: 17 actions\n","- Removed: 'tmp_0'\n","- After remove: 16 actions\n","\n","Effects:\n","- Initial effects for 'eth_0': {'ethical_flags': -0.1}\n","- Set effect for 'eth_0': 'ai_usage' = -0.05\n","- After set effects for 'eth_0': {'ethical_flags': -0.1, 'ai_usage': -0.05}\n","- Removed effect for 'eth_0': 'ai_usage'\n","- After remove effects for 'eth_0': {'ethical_flags': -0.1}\n","\n","Rewards:\n","- Initial: 10 rewards\n","- Added 'demo_reward': value=0.7 justification='Demo done' risk='minimal potential'\n","- With demo_reward: 11 rewards\n","- Removed: 'demo_reward'\n","- After remove: 10 rewards\n","\n","--- Smoke Test Complete ---\n"]}]},{"cell_type":"code","source":["# --- This cell contains code intended for two files:\n","# --- langgraph_policy.py and thesis_modules.py\n","# --- Clear separators are used for easy splitting later.\n","\n","# --- START OF FILE: thesis_modules.py ---\n","\n","# --- MODULE 0: State Definition ---\n","from typing import TypedDict, List, Dict, Any\n","# LangGraph and related imports will go in langgraph_policy.py\n","# from langgraph.graph import StateGraph, END\n","\n","# Assuming RLConfigManager and RewardItem are defined in a separate file (e.g., rl_config_manager.py)\n","# from rl_config_manager import RLConfigManager, RewardItem\n","\n","class CoreState(TypedDict):\n","    \"\"\"\n","    Core state variables for the thesis assistant.\n","\n","    These variables represent the student’s current status across different\n","    dimensions relevant to the RL policy.\n","    \"\"\"\n","    stage: str # Current stage of the thesis (e.g., planning, drafting, editing)\n","    advisor_trust: float # Level of trust from the advisor (e.g., 0.0 to 1.0)\n","    creativity_score: float # Metric for the novelty and originality of ideas (e.g., 0.0 to 1.0)\n","    ethical_flags: float # Aggregated score indicating potential ethical concerns (e.g., 0.0 to 1.0, higher is worse)\n","    ai_usage: float # Metric for the level of AI assistance used (e.g., 0.0 to 1.0)\n","    thesis_quality: float # Estimated quality of the thesis content (e.g., 0.0 to 1.0)\n","    deadline_ratio: float # Progress towards the deadline (e.g., 0.0 to 1.0)\n","    thesis_difficulty: float # Perceived difficulty of the thesis topic/task (e.g., 0.0 to 1.0)\n","    student_autonomy: float # Level of student self-direction and initiative (e.g., 0.0 to 1.0)\n","    language_proficiency: float # Assessment of writing and language skills (e.g., 0.0 to 1.0)\n","    emotional_state: float # Proxy for the student's emotional well-being (e.g., 0.0 to 1.0, higher is better)\n","    timestep: int # Current simulation timestep\n","\n","class ThesisState(TypedDict):\n","    \"\"\"\n","    Full state definition for the LangGraph.\n","\n","    Combines core state variables with policy execution trace, logs,\n","    and the configuration dictionary.\n","    \"\"\"\n","    core: CoreState # The core state variables\n","    policy_trace: List[str] # List of action keys executed in sequence\n","    log: List[str] # Log of messages or events generated by actions\n","    config: Dict[str, Any] # The loaded RL configuration\n","\n","\n","# --- MODULE 1: Dummy Action Modules (Intended for thesis_modules.py) ---\n","\n","# ethics_module.py\n","def display_eth_warning(state: ThesisState) -> ThesisState:\n","    \"\"\"\n","    Dummy action: Logs an ethics warning.\n","\n","    Args:\n","        state (ThesisState): The current state of the LangGraph.\n","\n","    Returns:\n","        ThesisState: The updated state.\n","    \"\"\"\n","    state[\"log\"].append(\"Ethics warning issued.\")\n","    # No core state changes in this dummy\n","    return state\n","\n","def propose_ai_restriction(state: ThesisState) -> ThesisState:\n","    \"\"\"\n","    Dummy action: Logs a proposed AI restriction.\n","\n","    Args:\n","        state (ThesisState): The current state of the LangGraph.\n","\n","    Returns:\n","        ThesisState: The updated state.\n","    \"\"\"\n","    state[\"log\"].append(\"AI restriction proposed.\")\n","    # No core state changes in this dummy\n","    return state\n","\n","def recommend_advisor_checkin(state: ThesisState) -> ThesisState:\n","    \"\"\"\n","    Dummy action: Logs a recommended advisor check-in.\n","\n","    Args:\n","        state (ThesisState): The current state of the LangGraph.\n","\n","    Returns:\n","        ThesisState: The updated state.\n","    \"\"\"\n","    state[\"log\"].append(\"Advisor check-in recommended.\")\n","    # No core state changes in this dummy\n","    return state\n","\n","def log_academic_concern(state: ThesisState) -> ThesisState:\n","    \"\"\"\n","    Dummy action: Logs an academic concern.\n","\n","    Args:\n","        state (ThesisState): The current state of the LangGraph.\n","\n","    Returns:\n","        ThesisState: The updated state.\n","    \"\"\"\n","    state[\"log\"].append(\"Academic concern logged.\")\n","    # No core state changes in this dummy\n","    return state\n","\n","\n","ethics_module_actions = {\n","    \"eth_0\": display_eth_warning,\n","    \"eth_1\": propose_ai_restriction,\n","    \"eth_2\": recommend_advisor_checkin,\n","    \"eth_3\": log_academic_concern,\n","}\n","\n","\n","# writing_support_module.py\n","def suggest_rewrite(state: ThesisState) -> ThesisState:\n","    \"\"\"\n","    Dummy action: Logs a rewrite suggestion.\n","\n","    Args:\n","        state (ThesisState): The current state of the LangGraph.\n","\n","    Returns:\n","        ThesisState: The updated state.\n","    \"\"\"\n","    state[\"log\"].append(\"Rewrite suggested.\")\n","    # No core state changes in this dummy\n","    return state\n","\n","def recommend_outline_reform(state: ThesisState) -> ThesisState:\n","    \"\"\"\n","    Dummy action: Logs a recommendation for outline reform.\n","\n","    Args:\n","        state (ThesisState): The current state of the LangGraph.\n","\n","    Returns:\n","        ThesisState: The updated state.\n","    \"\"\"\n","    state[\"log\"].append(\"Outline reform recommended.\")\n","    # No core state changes in this dummy\n","    return state\n","\n","def display_writing_tip(state: ThesisState) -> ThesisState:\n","    \"\"\"\n","    Dummy action: Logs a writing tip display.\n","\n","    Args:\n","        state (ThesisState): The current state of the LangGraph.\n","\n","    Returns:\n","        ThesisState: The updated state.\n","    \"\"\"\n","    state[\"log\"].append(\"Writing tip displayed.\")\n","    # No core state changes in this dummy\n","    return state\n","\n","def enable_feedback_loop(state: ThesisState) -> ThesisState:\n","    \"\"\"\n","    Dummy action: Logs enabling feedback loop.\n","\n","    Args:\n","        state (ThesisState): The current state of the LangGraph.\n","\n","    Returns:\n","        ThesisState: The updated state.\n","    \"\"\"\n","    state[\"log\"].append(\"Feedback loop enabled.\")\n","    # No core state changes in this dummy\n","    return state\n","\n","\n","writing_support_module_actions = {\n","    \"write_0\": suggest_rewrite,\n","    \"write_1\": recommend_outline_reform,\n","    \"write_2\": display_writing_tip,\n","    \"write_3\": enable_feedback_loop,\n","}\n","\n","# emotion_support_module.py\n","def encourage_autonomy(state: ThesisState) -> ThesisState:\n","    \"\"\"\n","    Dummy action: Logs encouragement of autonomy.\n","\n","    Args:\n","        state (ThesisState): The current state of the LangGraph.\n","\n","    Returns:\n","        ThesisState: The updated state.\n","    \"\"\"\n","    state[\"log\"].append(\"Autonomy encouraged.\")\n","    # No core state changes in this dummy\n","    return state\n","\n","def acknowledge_deadline_stress(state: ThesisState) -> ThesisState:\n","    \"\"\"\n","    Dummy action: Logs acknowledging deadline stress.\n","\n","    Args:\n","        state (ThesisState): The current state of the LangGraph.\n","\n","    Returns:\n","        ThesisState: The updated state.\n","    \"\"\"\n","    state[\"log\"].append(\"Deadline stress acknowledged.\")\n","    # No core state changes in this dummy\n","    return state\n","\n","def suggest_micro_break(state: ThesisState) -> ThesisState:\n","    \"\"\"\n","    Dummy action: Logs suggesting a micro break.\n","\n","    Args:\n","        state (ThesisState): The current state of the LangGraph.\n","\n","    Returns:\n","        ThesisState: The updated state.\n","    \"\"\"\n","    state[\"log\"].append(\"Micro break suggested.\")\n","    # No core state changes in this dummy\n","    return state\n","\n","def offer_motivational_boost(state: ThesisState) -> ThesisState:\n","    \"\"\"\n","    Dummy action: Logs offering a motivational boost.\n","\n","    Args:\n","        state (ThesisState): The current state of the LangGraph.\n","\n","    Returns:\n","        ThesisState: The updated state.\n","    \"\"\"\n","    state[\"log\"].append(\"Motivational boost offered.\")\n","    # No core state changes in this dummy\n","    return state\n","\n","emotion_support_module_actions = {\n","    \"emo_0\": encourage_autonomy,\n","    \"emo_1\": acknowledge_deadline_stress,\n","    \"emo_2\": suggest_micro_break,\n","    \"emo_3\": offer_motivational_boost,\n","}\n","\n","# idea_brainstorm_module.py\n","def prompt_open_ended_reflection(state: ThesisState) -> ThesisState:\n","    \"\"\"\n","    Dummy action: Logs prompting open-ended reflection.\n","\n","    Args:\n","        state (ThesisState): The current state of the LangGraph.\n","\n","    Returns:\n","        ThesisState: The updated state.\n","    \"\"\"\n","    state[\"log\"].append(\"Open-ended reflection prompted.\")\n","    # No core state changes in this dummy\n","    return state\n","\n","def offer_question_inversion(state: ThesisState) -> ThesisState:\n","    \"\"\"\n","    Dummy action: Logs offering question inversion.\n","\n","    Args:\n","        state (ThesisState): The current state of the LangGraph.\n","\n","    Returns:\n","        ThesisState: The updated state.\n","    \"\"\"\n","    state[\"log\"].append(\"Question inversion offered.\")\n","    # No core state changes in this dummy\n","    return state\n","\n","def stimulate_cross_topic_merge(state: ThesisState) -> ThesisState:\n","    \"\"\"\n","    Dummy action: Logs stimulating cross-topic merge.\n","\n","    Args:\n","        state (ThesisState): The current state of the LangGraph.\n","\n","    Returns:\n","        ThesisState: The updated state.\n","    \"\"\"\n","    state[\"log\"].append(\"Cross-topic merge stimulated.\")\n","    # No core state changes in this dummy\n","    return state\n","\n","def show_novelty_heatmap(state: ThesisState) -> ThesisState:\n","    \"\"\"\n","    Dummy action: Logs showing novelty heatmap.\n","\n","    Args:\n","        state (ThesisState): The current state of the LangGraph.\n","\n","    Returns:\n","        ThesisState: The updated state.\n","    \"\"\"\n","    state[\"log\"].append(\"Novelty heatmap shown.\")\n","    # No core state changes in this dummy\n","    return state\n","\n","idea_brainstorm_module_actions = {\n","    \"brain_0\": prompt_open_ended_reflection,\n","    \"brain_1\": offer_question_inversion,\n","    \"brain_2\": stimulate_cross_topic_merge,\n","    \"brain_3\": show_novelty_heatmap,\n","}\n","\n","\n","# Combine all actions from modules into a single dictionary\n","# This dictionary is intended to be imported by langgraph_policy.py\n","__actions__ = {\n","    **ethics_module_actions,\n","    **writing_support_module_actions,\n","    **emotion_support_module_actions,\n","    **idea_brainstorm_module_actions,\n","}\n","\n","# --- END OF FILE: thesis_modules.py ---\n","\n","\n","# --- START OF FILE: langgraph_policy.py ---\n","\n","# Import necessary modules\n","from langgraph.graph import StateGraph, END\n","from typing import Dict, Any, List # Import necessary types\n","\n","# Assuming ThesisState and __actions__ are defined in thesis_modules.py\n","# from thesis_modules import ThesisState, __actions__\n","\n","# Assuming RLConfigManager is defined in rl_config_manager.py\n","# from rl_config_manager import RLConfigManager\n","\n","# --- MODULE 2: Decorator for Action Effects ---\n","\n","def apply_action_effects(action_key: str, effects_config: Dict[str, Dict[str, float]]):\n","    \"\"\"\n","    Decorator to apply action effects to the state after an action function executes.\n","\n","    This decorator wraps an action function. After the original action function\n","    updates the state (e.g., logs a message), this decorator applies the\n","    numerical changes to the core state variables as defined in the\n","    `action_effects` section of the configuration for the given `action_key`.\n","    It also appends the executed action's key to the `policy_trace`.\n","\n","    Args:\n","        action_key (str): The key of the action being executed (e.g., \"eth_0\").\n","        effects_config (Dict[str, Dict[str, float]]): A dictionary mapping\n","            action keys to dictionaries of state variable changes (deltas).\n","\n","    Returns:\n","        Callable: The decorated action function.\n","    \"\"\"\n","    def decorator(fn):\n","        def wrapped(state: ThesisState) -> ThesisState:\n","            # Execute the original action function\n","            state = fn(state)\n","\n","            # Apply action effects if defined for the action_key\n","            delta = effects_config.get(action_key, {})\n","            for key, val in delta.items():\n","                # Ensure the state variable exists in core state before applying effect\n","                if key in state[\"core\"]:\n","                    state[\"core\"][key] += val\n","                # Optional: Add logging for applied effects\n","                # state[\"log\"].append(f\"Applied effect for {action_key}: {key} changed by {val:+}\")\n","\n","            # Append the action to the policy trace\n","            state[\"policy_trace\"].append(action_key)\n","\n","            # Increment timestep after each action\n","            state[\"core\"][\"timestep\"] += 1\n","\n","            return state\n","        return wrapped\n","    return decorator\n","\n","\n"],"metadata":{"id":"d9mJAFrb35ER","executionInfo":{"status":"ok","timestamp":1751187951637,"user_tz":-120,"elapsed":62,"user":{"displayName":"jiadong han","userId":"04626122069249233059"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"543ee948","executionInfo":{"status":"ok","timestamp":1751187956482,"user_tz":-120,"elapsed":144,"user":{"displayName":"jiadong han","userId":"04626122069249233059"}},"outputId":"4c947076-36da-41a9-b3ce-df2081548b24"},"source":["# --- MODULE 3: LangGraph Policy Builder ---\n","\n","def build_policy_graph(config: Dict[str, Any]) -> StateGraph:\n","    \"\"\"\n","    Builds the LangGraph policy from the loaded configuration and action modules.\n","\n","    This function constructs the LangGraph dynamically. It adds nodes for each\n","    action defined in the config (and available in `__actions__`), applies\n","    the `apply_action_effects` decorator to each action node, and sets up\n","    conditional edges based on the `route_action` function and the\n","    `action_transitions` from the configuration.\n","\n","    Args:\n","        config (Dict[str, Any]): The loaded configuration dictionary,\n","            expected to contain \"actions\", \"action_effects\", and \"action_transitions\".\n","\n","    Returns:\n","        StateGraph: The compiled LangGraph ready for invocation.\n","    \"\"\"\n","    graph = StateGraph(ThesisState)\n","\n","    action_effects = config.get(\"action_effects\", {})\n","    action_transitions = config.get(\"action_transitions\", {})\n","    actions = config.get(\"actions\", {})\n","\n","    # Add nodes for each action defined in the combined actions dictionary\n","    # Only add nodes for actions that are present in the config's \"actions\"\n","    for action_key, action_fn in __actions__.items():\n","        if action_key in actions:\n","             # Wrap the action function with the apply_action_effects decorator\n","             decorated_action_fn = apply_action_effects(action_key, action_effects)(action_fn)\n","             graph.add_node(action_key, decorated_action_fn)\n","\n","    # Add the END node, which signifies the termination of a policy rollout\n","    graph.add_node(\"END\", lambda state: state) # END node doesn't modify state\n","\n","\n","    # Add conditional edges from each action node based on the routing function\n","    # After an action node executes, it transitions to the next node determined by route_action.\n","    for action_key in actions.keys():\n","         graph.add_conditional_edges(\n","             action_key, # The node to route FROM (an action node)\n","             route_action, # The function that determines the next node\n","             # No mapping needed here; route_action returns the target node name directly.\n","             # LangGraph uses the string returned by route_action as the next node name.\n","         )\n","\n","    # Define a special entry point node that immediately routes\n","    graph.add_node(\"start_node\", lambda state: state) # Dummy node, doesn't modify state\n","\n","    # Set the entry point to the dummy start node\n","    graph.set_entry_point(\"start_node\")\n","\n","    # Add a conditional edge from the start node using the route_action function\n","    # This will determine the first actual action to take based on the initial state\n","    graph.add_conditional_edges(\n","        \"start_node\", # Route from the dummy start node\n","        route_action,  # Use route_action to determine the next node\n","    )\n","\n","    return graph.compile()\n","\n","# --- MODULE 4: Routing Function ---\n","\n","def route_action(state: ThesisState) -> str:\n","    \"\"\"\n","    Determines the next action based on the current state and action transitions in the config.\n","\n","    This function serves as the policy's core logic. It looks at the current\n","    state (specifically the `policy_trace` to find the last action) and the\n","    `action_transitions` defined in the config to decide the next action.\n","    In a real RL scenario, this would be replaced by a trained policy model\n","    that takes the state as input and outputs the next action.\n","\n","    Args:\n","        state (ThesisState): The current state of the LangGraph.\n","\n","    Returns:\n","        str: The key of the next action node to transition to, or \"END\" to stop.\n","    \"\"\"\n","    config = state.get(\"config\", {})\n","    action_transitions = config.get(\"action_transitions\", {})\n","    actions = config.get(\"actions\", {})\n","    policy_trace = state.get(\"policy_trace\", [])\n","\n","    # If no actions have been taken yet (first step), determine the starting action.\n","    # For this example, we'll just pick the first action key from the config.\n","    if not policy_trace:\n","        action_keys = list(actions.keys())\n","        return action_keys[0] if action_keys else END\n","\n","    # Get the key of the last action that was executed.\n","    last_action = policy_trace[-1]\n","\n","    # Look up the transitions defined for the last action in the config.\n","    if last_action in action_transitions:\n","        transitions = action_transitions[last_action]\n","        # Assuming a simple transition structure like {\"next\": \"next_action_key\"}\n","        # In a more complex scenario, transitions could be conditional based on state.\n","        next_node = transitions.get(\"next\", END)\n","        return next_node\n","    else:\n","        # If no transition is defined for the last action, terminate the graph.\n","        return END\n","\n","# --- END OF FILE: langgraph_policy.py ---\n","\n","\n","# --- MODULE 5: Simulate One Run (Demo) ---\n","\n","if __name__ == \"__main__\":\n","    print(\"--- LangGraph Policy Demo ---\")\n","\n","    # Assuming RLConfigManager is available (from the previous cell)\n","    # from rl_config_manager import RLConfigManager\n","\n","    # Load configuration using the RLConfigManager\n","    try:\n","        mgr = RLConfigManager()\n","        config = mgr.load_config()\n","        print(\"✅ Configuration loaded.\")\n","\n","        # Ensure action_transitions exist for the demo, add if not\n","        if \"action_transitions\" not in config:\n","            config[\"action_transitions\"] = {\n","                \"eth_0\": {\"next\": \"write_0\"},\n","                \"write_0\": {\"next\": \"emo_0\"}, # Added another transition\n","                \"emo_0\": {\"next\": \"END\"}    # Added transition to END\n","            }\n","            print(\"ℹ️ Added default action_transitions for demo.\")\n","            # Note: This change is only in the 'config' dictionary in memory for the demo.\n","            # To make it persistent, you would need to call mgr.save_config(config)\n","\n","    except Exception as e:\n","        print(f\"❌ Failed to load configuration: {e}\")\n","        # Fallback to a basic default config if loading fails\n","        config = {\n","            \"actions\": {\n","                \"dummy_action_1\": \"A first dummy action\",\n","                \"dummy_action_2\": \"A second dummy action\",\n","            },\n","            \"action_effects\": {},\n","            \"action_transitions\": {\n","                \"dummy_action_1\": {\"next\": \"dummy_action_2\"},\n","                \"dummy_action_2\": {\"next\": \"END\"},\n","            },\n","            \"state_variables\": [],\n","             \"reward_config\": {},\n","        }\n","        print(\"⚠️ Using fallback configuration.\")\n","\n","\n","    # Define the initial state for the simulation\n","    initial_state: ThesisState = {\n","        \"core\": {\n","            \"stage\": \"planning\",\n","            \"advisor_trust\": 0.5,\n","            \"creativity_score\": 0.5,\n","            \"ethical_flags\": 0.5,\n","            \"ai_usage\": 0.5,\n","            \"thesis_quality\": 0.5,\n","            \"deadline_ratio\": 0.0, # Start at the beginning\n","            \"thesis_difficulty\": 0.5,\n","            \"student_autonomy\": 0.5,\n","            \"language_proficiency\": 0.5,\n","            \"emotional_state\": 0.5,\n","            \"timestep\": 0,\n","        },\n","        \"policy_trace\": [], # Start with an empty trace\n","        \"log\": [], # Start with an empty log\n","        \"config\": config # Pass the loaded config to the state\n","    }\n","\n","    print(\"\\nInitial State:\", initial_state[\"core\"])\n","    print(\"Policy Trace:\", initial_state[\"policy_trace\"])\n","    print(\"Log:\", initial_state[\"log\"])\n","\n","\n","    # Build and run the LangGraph\n","    try:\n","        thesis_graph = build_policy_graph(config)\n","        print(\"\\n✅ LangGraph built.\")\n","\n","        # Invoke the graph to run the policy\n","        # The graph will start at the entry point ('start_node'),\n","        # which will immediately route based on route_action and the initial state.\n","        print(\"\\nRunning LangGraph...\")\n","        final_state = thesis_graph.invoke(initial_state)\n","        print(\"✅ LangGraph execution finished.\")\n","\n","        # Inspect the final state\n","        print(\"\\n--- Final State ---\")\n","        print(\"Trace:\", final_state[\"policy_trace\"])\n","        print(\"Log:\", final_state[\"log\"])\n","        print(\"Final core state:\", final_state[\"core\"])\n","\n","    except Exception as e:\n","        print(f\"\\n❌ An error occurred during LangGraph execution: {e}\")\n","\n","\n","    print(\"\\n--- Demo Complete ---\")"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["--- LangGraph Policy Demo ---\n","✅ Configuration loaded.\n","ℹ️ Added default action_transitions for demo.\n","\n","Initial State: {'stage': 'planning', 'advisor_trust': 0.5, 'creativity_score': 0.5, 'ethical_flags': 0.5, 'ai_usage': 0.5, 'thesis_quality': 0.5, 'deadline_ratio': 0.0, 'thesis_difficulty': 0.5, 'student_autonomy': 0.5, 'language_proficiency': 0.5, 'emotional_state': 0.5, 'timestep': 0}\n","Policy Trace: []\n","Log: []\n","\n","✅ LangGraph built.\n","\n","Running LangGraph...\n","✅ LangGraph execution finished.\n","\n","--- Final State ---\n","Trace: ['eth_0', 'write_0', 'emo_0']\n","Log: ['Ethics warning issued.', 'Rewrite suggested.', 'Autonomy encouraged.']\n","Final core state: {'stage': 'planning', 'advisor_trust': 0.5, 'creativity_score': 0.5, 'ethical_flags': 0.4, 'ai_usage': 0.5, 'thesis_quality': 0.6, 'deadline_ratio': 0.0, 'thesis_difficulty': 0.5, 'student_autonomy': 0.6, 'language_proficiency': 0.5, 'emotional_state': 0.5, 'timestep': 3}\n","\n","--- Demo Complete ---\n"]}]}]}