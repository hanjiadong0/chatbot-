{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RL Configuration Manager\n",
        "\n",
        "This module provides a centralized configuration interface for the RL-based educational coaching system. It manages all static settings required by the agent and environment to interpret observations, execute actions, and compute rewards.\n",
        "\n",
        "The configuration is saved and loaded from a JSON file (`rl_config.json`). If the file does not exist, a default configuration is created automatically. This ensures a reproducible setup and simplifies testing across modules.\n",
        "\n",
        "## Configuration Structure\n",
        "\n",
        "- `state_variables`:  \n",
        "  Defines the numerical features observed by the RL agent at each time step. These features represent the student’s academic state, ethical posture, writing progression, and contextual traits.\n",
        "\n",
        "- `actions`:  \n",
        "  A dictionary of discrete, recommendation-style actions the agent can select. Each key is an action ID (e.g. `\"eth_0\"`), and each value is a human-readable description.\n",
        "\n",
        "- `reward_config`:  \n",
        "  Defines the scalar reward shaping used during training. Rewards and penalties reflect fluency, ethical alignment, advisor trust, autonomy, and creativity.\n",
        "\n",
        "- `action_effects`:  \n",
        "  Maps agent actions to updates in the simulated state. Each action has associated side-effects that alter one or more student-related variables (e.g., reducing `ai_usage` or increasing `advisor_trust`).\n",
        "\n",
        "## Usage Example\n",
        "\n",
        "```python\n",
        "\n",
        "from rl_config_manager import RLConfigManager\n",
        "\n",
        "# Load existing configuration or initialize default\n",
        "config = RLConfigManager.load_config()\n",
        "\n",
        "# Access specific parts\n",
        "print(\"Available state variables:\")\n",
        "for var in config[\"state_variables\"]:\n",
        "    print(\"-\", var)"
      ],
      "metadata": {
        "id": "nU9SgNrzw42q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rl_config_manager.py\n",
        "\"\"\"\n",
        "Adaptive RL Optimizer Configuration Manager\n",
        "\n",
        "Defines and enforces the JSON schema for the RL-based academic coaching agent’s\n",
        "centralized configuration. Uses Pydantic for validation and metadata on versioning,\n",
        "timestamps, reward rationales, and action-to-state effect mappings.\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "\n",
        "# ----------------------------- Schema Definitions -----------------------------\n",
        "\n",
        "\n",
        "class RewardItem(BaseModel):\n",
        "    \"\"\"\n",
        "    Represents a single reward entry in the multi-objective shaping scheme.\n",
        "\n",
        "    Attributes:\n",
        "        value (float):\n",
        "            The numerical reward (positive or negative) assigned for this signal.\n",
        "        justification (str):\n",
        "            A brief rationale (min. 5 characters) explaining why this reward level was chosen.\n",
        "        risk (str):\n",
        "            A description (min. 5 characters) of any potential downside or misuse if over-emphasized.\n",
        "    \"\"\"\n",
        "    value: float\n",
        "    justification: str = Field(..., min_length=5)\n",
        "    risk: str = Field(..., min_length=5)\n",
        "\n",
        "class ConfigSchema(BaseModel):\n",
        "    \"\"\"\n",
        "    Full schema for the RL configuration file. Validates metadata, state/action definitions,\n",
        "    reward shaping, and the mapping from chosen actions to changes in latent state variables.\n",
        "\n",
        "    Attributes:\n",
        "        config_version (float):\n",
        "            Version number of the configuration. Must be ≥ 1.0 to ensure compatibility.\n",
        "        created_at (str):\n",
        "            ISO-formatted timestamp marking when this config was generated or last updated.\n",
        "        state_variables (List[str]):\n",
        "            Ordered list of state feature names that the RL agent will observe.\n",
        "        actions (Dict[str, str]):\n",
        "            Mapping from action keys (e.g., \"eth_0\") to human-readable labels.\n",
        "        reward_config (Dict[str, RewardItem]):\n",
        "            Detailed reward items for each supervision signal, including value, justification, and risk.\n",
        "        action_effects (Dict[str, Dict[str, float]]):\n",
        "            Defines how each action modifies one or more state variables, as a delta change.\n",
        "    \"\"\"\n",
        "    config_version: float\n",
        "    created_at:       str\n",
        "    state_variables:  List[str]\n",
        "    actions:          Dict[str, str]\n",
        "    reward_config:    Dict[str, RewardItem]\n",
        "    action_effects:   Dict[str, Dict[str, float]]\n",
        "\n",
        "    @field_validator(\"config_version\")\n",
        "    @classmethod\n",
        "    def validate_version(cls, v):\n",
        "        \"\"\"\n",
        "        Ensure the configuration version is compatible with the agent's codebase.\n",
        "\n",
        "        Args:\n",
        "            v (float): The version number provided in the JSON.\n",
        "\n",
        "        Returns:\n",
        "            float: The same version number, if it meets requirements.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If v < 1.0, indicating an unsupported config format.\n",
        "        \"\"\"\n",
        "        if v < 1.0:\n",
        "            raise ValueError(\"Config version must be ≥ 1.0\")\n",
        "        return v\n",
        "# ----------------------------- Configuration Manager -----------------------------\n",
        "class ConfigIO:\n",
        "    \"\"\"\n",
        "    Pure I/O: load & save the RL optimizer’s JSON config, with safe versioning and backups.\n",
        "\n",
        "    - If no config file exists, write out a default config (DEFAULT).\n",
        "    - On first save(): stamps `created_at`.\n",
        "    - On every save(): increments `config_version` by +0.1 and makes a timestamped backup of the old file.\n",
        "    \"\"\"\n",
        "    CONFIG_FILE = \"rl_config.json\"\n",
        "    BACKUP_DIR  = \"config_backups\"\n",
        "\n",
        "    # Default config skeleton—matches your full specification\n",
        "    DEFAULT: Dict[str, Any] = {\n",
        "        \"config_version\": 1.0,\n",
        "        \"created_at\":     \"\",   # filled in when we first save()\n",
        "        \"state_variables\": [\n",
        "            \"ai_usage\", \"ethical_flags\", \"advisor_trust\",\n",
        "            \"thesis_quality\", \"deadline_ratio\", \"thesis_difficulty\",\n",
        "            \"student_autonomy\", \"language_proficiency\",\n",
        "            \"emotional_state\", \"creativity_score\", \"timestep\"\n",
        "        ],\n",
        "        \"actions\": {\n",
        "            \"eth_0\": \"Display ethical reminder\",\n",
        "            \"eth_1\": \"Propose AI restriction\",\n",
        "            \"eth_2\": \"Recommend advisor check-in\",\n",
        "            \"eth_3\": \"Log academic concern\",\n",
        "            \"brain_0\": \"Prompt open-ended reflection\",\n",
        "            \"brain_1\": \"Offer question inversion\",\n",
        "            \"brain_2\": \"Stimulate cross-topic merge\",\n",
        "            \"brain_3\": \"Show novelty heatmap\",\n",
        "            \"write_0\": \"Suggest rewriting section\",\n",
        "            \"write_1\": \"Recommend outline reform\",\n",
        "            \"write_2\": \"Display writing tip\",\n",
        "            \"write_3\": \"Enable feedback loop\",\n",
        "            \"emo_0\": \"Encourage autonomy\",\n",
        "            \"emo_1\": \"Acknowledge deadline stress\",\n",
        "            \"emo_2\": \"Suggest micro-break\",\n",
        "            \"emo_3\": \"Offer motivational boost\"\n",
        "        },\n",
        "        \"reward_config\": {\n",
        "            \"fluency_improved\": {\n",
        "                \"value\": 1.5,\n",
        "                \"justification\": \"Improves clarity and coherence\",\n",
        "                \"risk\": \"May incentivize style over substance\"\n",
        "            },\n",
        "            \"trust_earned\": {\n",
        "                \"value\": 2.0,\n",
        "                \"justification\": \"Advisor feedback acknowledged and used\",\n",
        "                \"risk\": \"May reward form without deep content change\"\n",
        "            },\n",
        "            \"creativity_expressed\": {\n",
        "                \"value\": 2.5,\n",
        "                \"justification\": \"Encourages safe novelty and synthesis\",\n",
        "                \"risk\": \"May drift into irrelevant tangents\"\n",
        "            },\n",
        "            \"autonomy_respected\": {\n",
        "                \"value\": 1.0,\n",
        "                \"justification\": \"Student took initiative\",\n",
        "                \"risk\": \"Passive neglect might appear as autonomy\"\n",
        "            },\n",
        "            \"ai_dependency_violation\": {\n",
        "                \"value\": -4.0,\n",
        "                \"justification\": \"Detected AI overuse\",\n",
        "                \"risk\": \"Could punish legitimate drafting support\"\n",
        "            },\n",
        "            \"ethical_boundary_crossed\": {\n",
        "                \"value\": -6.0,\n",
        "                \"justification\": \"Clear breach of academic norms\",\n",
        "                \"risk\": \"Non-compensable — agent must intervene\"\n",
        "            },\n",
        "            \"deadline_panic_detected\": {\n",
        "                \"value\": -1.0,\n",
        "                \"justification\": \"Urgency spike detected\",\n",
        "                \"risk\": \"Might suppress productive deadline use\"\n",
        "            },\n",
        "            \"milestone_completed\": {\n",
        "                \"value\": 5.0,\n",
        "                \"justification\": \"Goal achieved within scope\",\n",
        "                \"risk\": \"May mask ethics issues if used alone\"\n",
        "            },\n",
        "            \"novel_but_safe\": {\n",
        "                \"value\": 3.0,\n",
        "                \"justification\": \"Original idea aligned with context\",\n",
        "                \"risk\": \"Requires semantic checking\"\n",
        "            },\n",
        "            \"supervisor_disappointment\": {\n",
        "                \"value\": -5.0,\n",
        "                \"justification\": \"Advisor flags trust breakdown\",\n",
        "                \"risk\": \"Recovery should be possible over time\"\n",
        "            }\n",
        "        },\n",
        "        \"action_effects\": {\n",
        "            \"eth_0\": {\"ethical_flags\": -0.1},\n",
        "            \"eth_1\": {\"ai_usage\": -0.2},\n",
        "            \"eth_2\": {\"advisor_trust\": 0.15},\n",
        "            \"eth_3\": {\"ethical_flags\": 0.2, \"advisor_trust\": -0.3},\n",
        "            \"brain_0\": {\"creativity_score\": 0.05},\n",
        "            \"brain_1\": {\"creativity_score\": 0.07},\n",
        "            \"brain_2\": {\"creativity_score\": 0.10},\n",
        "            \"brain_3\": {\"thesis_quality\": 0.05},\n",
        "            \"write_0\": {\"thesis_quality\": 0.10},\n",
        "            \"write_1\": {\"thesis_quality\": 0.07},\n",
        "            \"write_2\": {\"thesis_quality\": 0.05},\n",
        "            \"write_3\": {\"thesis_quality\": 0.05, \"advisor_trust\": 0.1},\n",
        "            \"emo_0\": {\"student_autonomy\": 0.1},\n",
        "            \"emo_1\": {\"emotional_state\": -0.05},\n",
        "            \"emo_2\": {\"emotional_state\": 0.1},\n",
        "            \"emo_3\": {\"emotional_state\": 0.15}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Load the JSON config from disk. If missing, write DEFAULT → disk then load.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(cls.CONFIG_FILE):\n",
        "            os.makedirs(cls.BACKUP_DIR, exist_ok=True)\n",
        "            with open(cls.CONFIG_FILE, \"w\") as f:\n",
        "                json.dump(cls.DEFAULT, f, indent=4)\n",
        "        with open(cls.CONFIG_FILE, \"r\") as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    @classmethod\n",
        "    def save(cls, config: Dict[str, Any]) -> None:\n",
        "        \"\"\"\n",
        "        Persist `config` back to disk safely:\n",
        "\n",
        "        1) On first ever save, fill in `created_at`.\n",
        "        2) Always bump `config_version` by +0.1.\n",
        "        3) Copy the old file into `config_backups/rl_config_<timestamp>.json`.\n",
        "        4) Overwrite the main JSON.\n",
        "        \"\"\"\n",
        "        # 1) stamp created_at if missing\n",
        "        if not config.get(\"created_at\"):\n",
        "            config[\"created_at\"] = datetime.utcnow().isoformat()\n",
        "\n",
        "        # 2) bump version\n",
        "        config[\"config_version\"] = round(config.get(\"config_version\", 1.0) + 0.1, 2)\n",
        "\n",
        "        # 3) backup old\n",
        "        if os.path.exists(cls.CONFIG_FILE):\n",
        "            ts = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "            os.makedirs(cls.BACKUP_DIR, exist_ok=True)\n",
        "            shutil.copy(\n",
        "                cls.CONFIG_FILE,\n",
        "                os.path.join(cls.BACKUP_DIR, f\"rl_config_{ts}.json\")\n",
        "            )\n",
        "\n",
        "        # 4) write new\n",
        "        with open(cls.CONFIG_FILE, \"w\") as f:\n",
        "            json.dump(config, f, indent=4)\n",
        "\n",
        "class StateVarManager:\n",
        "    \"\"\"CRUD for state_variables through a validated ConfigSchema.\"\"\"\n",
        "    def __init__(self, io: ConfigIO):\n",
        "        self.io    = io\n",
        "        self.model = ConfigSchema(**io.load())\n",
        "\n",
        "    def list(self) -> List[str]:\n",
        "        return self.model.state_variables\n",
        "\n",
        "    def add(self, var: str):\n",
        "        if var not in self.model.state_variables:\n",
        "            self.model.state_variables.append(var)\n",
        "            self.io.save(self.model.model_dump())\n",
        "\n",
        "    def remove(self, var: str):\n",
        "        if var in self.model.state_variables:\n",
        "            self.model.state_variables.remove(var)\n",
        "            self.io.save(self.model.model_dump())\n",
        "\n",
        "class ActionManager:\n",
        "    \"\"\"CRUD for actions & their effects, via a validated ConfigSchema.\"\"\"\n",
        "    def __init__(self, io: ConfigIO):\n",
        "        self.io    = io\n",
        "        self.model = ConfigSchema(**io.load())\n",
        "\n",
        "    def list(self) -> Dict[str,str]:\n",
        "        return self.model.actions\n",
        "\n",
        "    def add(self, key:str, label:str):\n",
        "        self.model.actions[key] = label\n",
        "        self.io.save(self.model.model_dump())\n",
        "\n",
        "    def remove(self, key:str):\n",
        "        self.model.actions.pop(key, None)\n",
        "        self.model.action_effects.pop(key, None)\n",
        "        self.io.save(self.model.model_dump())\n",
        "\n",
        "    def list_effects(self) -> Dict[str,Dict[str,float]]:\n",
        "        return self.model.action_effects\n",
        "\n",
        "    def set_effect(self, action_key:str, var:str, delta:float):\n",
        "        self.model.action_effects.setdefault(action_key, {})[var] = delta\n",
        "        self.io.save(self.model.model_dump())\n",
        "\n",
        "    def remove_effect(self, action_key:str, var:str):\n",
        "        self.model.action_effects.get(action_key, {}).pop(var, None)\n",
        "        self.io.save(self.model.model_dump())\n",
        "\n",
        "class RewardManager:\n",
        "    \"\"\"CRUD for reward_config (with full RewardItem metadata).\"\"\"\n",
        "    def __init__(self, io: ConfigIO):\n",
        "        self.io    = io\n",
        "        self.model = ConfigSchema(**io.load())\n",
        "\n",
        "    def list(self) -> Dict[str,RewardItem]:\n",
        "        return self.model.reward_config\n",
        "\n",
        "    def set(self, key:str, item:RewardItem):\n",
        "        self.model.reward_config[key] = item\n",
        "        self.io.save(self.model.model_dump())\n",
        "\n",
        "    def remove(self, key:str):\n",
        "        self.model.reward_config.pop(key, None)\n",
        "        self.io.save(self.model.model_dump())\n",
        "\n",
        "\n",
        "# -------------------------- Top-level Façade Class ----------------------------\n",
        "\n",
        "class RLConfigManager:\n",
        "    \"\"\"\n",
        "    Unified facade: bundles I/O + per-domain managers, all validated\n",
        "    by ConfigSchema on each save() call.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.io   = ConfigIO()\n",
        "        self.vars = StateVarManager(self.io)\n",
        "        self.act  = ActionManager(self.io)\n",
        "        self.rwd  = RewardManager(self.io)\n",
        "\n",
        "    @classmethod\n",
        "    def default_config(cls) -> Dict[str, Any]:\n",
        "        \"\"\"Returns the default configuration dictionary.\"\"\"\n",
        "        return ConfigIO.DEFAULT\n",
        "\n",
        "    def load_config(self) -> Dict[str, Any]:\n",
        "        \"\"\"Loads the configuration from disk and returns it as a dictionary.\"\"\"\n",
        "        return self.io.load()\n",
        "\n",
        "    def save_config(self, config: Dict[str, Any]) -> None:\n",
        "        \"\"\"Saves the given configuration dictionary to disk.\"\"\"\n",
        "        self.io.save(config)\n",
        "\n",
        "\n",
        "# ------------------------------ Quick Smoke-test ------------------------------\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    print(\"--- Smoke Test ---\")\n",
        "    # Create a temporary configuration object without saving\n",
        "    temp_io = type('TempConfigIO', (object,), {\n",
        "        'load': lambda self: ConfigIO.DEFAULT,\n",
        "        'save': lambda self, config: None # Do nothing on save\n",
        "    })()\n",
        "    mgr = RLConfigManager()\n",
        "    mgr.io = temp_io # Replace the real IO with the temporary one\n",
        "    mgr.vars = StateVarManager(mgr.io)\n",
        "    mgr.act = ActionManager(mgr.io)\n",
        "    mgr.rwd = RewardManager(mgr.io)\n",
        "\n",
        "\n",
        "    print(\"\\nState Variables:\")\n",
        "    initial_vars = mgr.vars.list()\n",
        "    print(f\"- Initial: {len(initial_vars)} variables\")\n",
        "    mgr.vars.add(\"time_spent_recent\")\n",
        "    print(\"- Added: 'time_spent_recent'\")\n",
        "    print(f\"- After add: {len(mgr.vars.list())} variables\")\n",
        "    mgr.vars.remove(\"time_spent_recent\")\n",
        "    print(\"- Removed: 'time_spent_recent'\")\n",
        "    print(f\"- After remove: {len(mgr.vars.list())} variables\")\n",
        "\n",
        "\n",
        "    print(\"\\nActions:\")\n",
        "    initial_actions = mgr.act.list()\n",
        "    print(f\"- Initial: {len(initial_actions)} actions\")\n",
        "    mgr.act.add(\"tmp_0\",\"Demo action\")\n",
        "    print(\"- Added: 'tmp_0': 'Demo action'\")\n",
        "    print(f\"- With tmp_0: {len(mgr.act.list())} actions\")\n",
        "    mgr.act.remove(\"tmp_0\")\n",
        "    print(\"- Removed: 'tmp_0'\")\n",
        "    print(f\"- After remove: {len(mgr.act.list())} actions\")\n",
        "\n",
        "\n",
        "    print(\"\\nEffects:\")\n",
        "    initial_effects = mgr.act.list_effects()\n",
        "    print(f\"- Initial effects for 'eth_0': {initial_effects.get('eth_0', {})}\")\n",
        "    mgr.act.set_effect(\"eth_0\",\"ai_usage\",-0.05)\n",
        "    print(\"- Set effect for 'eth_0': 'ai_usage' = -0.05\")\n",
        "    print(f\"- After set effects for 'eth_0': {mgr.act.list_effects().get('eth_0', {})}\")\n",
        "    mgr.act.remove_effect(\"eth_0\",\"ai_usage\")\n",
        "    print(\"- Removed effect for 'eth_0': 'ai_usage'\")\n",
        "    print(f\"- After remove effects for 'eth_0': {mgr.act.list_effects().get('eth_0', {})}\")\n",
        "\n",
        "\n",
        "    print(\"\\nRewards:\")\n",
        "    initial_rewards = mgr.rwd.list()\n",
        "    print(f\"- Initial: {len(initial_rewards)} rewards\")\n",
        "    demo = RewardItem(\n",
        "        value=0.7,\n",
        "        justification=\"Demo done\",\n",
        "        risk=\"minimal potential\"\n",
        "    )\n",
        "    mgr.rwd.set(\"demo_reward\", demo)\n",
        "    print(f\"- Added 'demo_reward': {demo}\")\n",
        "    print(f\"- With demo_reward: {len(mgr.rwd.list())} rewards\")\n",
        "    mgr.rwd.remove(\"demo_reward\")\n",
        "    print(\"- Removed: 'demo_reward'\")\n",
        "    print(f\"- After remove: {len(mgr.rwd.list())} rewards\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Smoke Test Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aRzt6bV5e5Q",
        "outputId": "e5c42435-6ea7-4c0d-9786-711bd24274b8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Smoke Test ---\n",
            "\n",
            "State Variables:\n",
            "- Initial: 11 variables\n",
            "- Added: 'time_spent_recent'\n",
            "- After add: 12 variables\n",
            "- Removed: 'time_spent_recent'\n",
            "- After remove: 11 variables\n",
            "\n",
            "Actions:\n",
            "- Initial: 16 actions\n",
            "- Added: 'tmp_0': 'Demo action'\n",
            "- With tmp_0: 17 actions\n",
            "- Removed: 'tmp_0'\n",
            "- After remove: 16 actions\n",
            "\n",
            "Effects:\n",
            "- Initial effects for 'eth_0': {'ethical_flags': -0.1}\n",
            "- Set effect for 'eth_0': 'ai_usage' = -0.05\n",
            "- After set effects for 'eth_0': {'ethical_flags': -0.1, 'ai_usage': -0.05}\n",
            "- Removed effect for 'eth_0': 'ai_usage'\n",
            "- After remove effects for 'eth_0': {'ethical_flags': -0.1}\n",
            "\n",
            "Rewards:\n",
            "- Initial: 10 rewards\n",
            "- Added 'demo_reward': value=0.7 justification='Demo done' risk='minimal potential'\n",
            "- With demo_reward: 11 rewards\n",
            "- Removed: 'demo_reward'\n",
            "- After remove: 10 rewards\n",
            "\n",
            "--- Smoke Test Complete ---\n"
          ]
        }
      ]
    }
  ]
}