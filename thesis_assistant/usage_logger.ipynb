{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "665e3a39"
      },
      "source": [
        "# Usage Logger\n",
        "\n",
        "This module provides an embedding-enhanced usage logging system for the RL-based thesis assistant.  \n",
        "It captures prompt-level interactions, infers user intent, and stores relevant metadata (e.g. thesis stage).  \n",
        "In addition, it leverages the OpenAI embedding API to represent prompts as high-dimensional vectors, enabling similarity search, memory-based reflection, and semantic RL conditioning.\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "The `UsageLogger` is designed to:\n",
        "\n",
        "- Log assistant usage events during live interaction or LangGraph rollout\n",
        "- Generate vector embeddings for each prompt using `text-embedding-3-small`\n",
        "- Persist embeddings to disk for reuse and inspection\n",
        "- Support similarity-based search over past usage sessions\n",
        "\n",
        "This logger is intended to support both:\n",
        "- **Semantic retrieval** (e.g. reflection on similar past queries)\n",
        "- **RL preprocessing** (e.g. embedding as state input)\n",
        "\n",
        "---\n",
        "\n",
        "## Logged Format\n",
        "\n",
        "Each log entry includes:\n",
        "\n",
        "| Field          | Description                               |\n",
        "|----------------|-------------------------------------------|\n",
        "| `timestamp`    | Timestamp when interaction occurred       |\n",
        "| `prompt`       | The raw prompt text (user input)          |\n",
        "| `intent`       | Inferred intent label or action ID        |\n",
        "| `thesis_stage` | Stage of the thesis (e.g. planning, writing) |\n",
        "\n",
        "Each embedding is stored with:\n",
        "- `embedding`: 1536-dimensional vector (as list or np.array)\n",
        "- `original_index`: index into the usage log array\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##  Main Methods\n",
        "\n",
        "### `log_usage(prompt, intent, thesis_stage)`\n",
        "\n",
        "Logs a single prompt event and generates its embedding.\n",
        "\n",
        "### `generate_embedding_for_log(log_entry)`\n",
        "\n",
        "Private method. Uses OpenAI API to convert a prompt into an embedding and stores it. Called automatically after each log_usage.\n",
        "\n",
        "### `generate_all_usage_embeddings(batch_size=100)`\n",
        "\n",
        "Batch-generates embeddings for any past logs that are missing them.\n",
        "\n",
        "\n",
        "### `find_similar_usage(query_prompt, n=3)`\n",
        "\n",
        "Finds the n most similar usage logs based on prompt embeddings.\n",
        "\n",
        "### `similar = logger.find_similar_usage(\"summarize discussion section\")`\n",
        "Return a list of\n",
        "\n",
        "```\n",
        "[\n",
        "  {\n",
        "    \"distance\": 0.14,\n",
        "    \"original_log\": {\n",
        "      \"prompt\": \"...\",\n",
        "      \"intent\": \"...\",\n",
        "      \"thesis_stage\": \"...\",\n",
        "      ...\n",
        "    }\n",
        "  },\n",
        "]\n",
        "\n",
        "```\n",
        "\n",
        "## File Persistence\n",
        "All embeddings are automatically saved to disk as usage_embeddings.json.\n",
        "The logger will load this file (if present) on startup.\n",
        "\n",
        "You may change this path by passing a custom filename:\n",
        "\n",
        "```\n",
        "logger = UsageLogger(openai_client, embeddings_file=\"my_embeddings.json\")\n",
        "```\n",
        "\n",
        "## Example Usage\n",
        "\n",
        "```\n",
        "from usage_logger import UsageLogger\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "logger = UsageLogger(client)\n",
        "\n",
        "# Log one interaction\n",
        "logger.log_usage(\"revise abstract for clarity\", \"write_2\", \"writing\")\n",
        "\n",
        "# Find similar usage\n",
        "matches = logger.find_similar_usage(\"abstract revision\")\n",
        "for m in matches:\n",
        "    print(m[\"distance\"], m[\"original_log\"][\"prompt\"])\n",
        "```\n",
        "## Integration Plan\n",
        "The UsageLogger is designed to work in parallel with:\n",
        "\n",
        "- LangGraph rollouts for logging actions, policy trace, and user prompts\n",
        "\n",
        "- DataPreprocessor  to inject embedding slices into the RL state vector\n",
        "\n",
        "- Reflection/memory modules for finding similar cases from past logs\n",
        "\n",
        "## Notes\n",
        "\n",
        "Embedding model: text-embedding-3-small\n",
        "\n",
        "Requires OpenAI client authentication (openai.api_key)\n",
        "\n",
        "Similarity search is based on cosine distance\n",
        "\n",
        "Embeddings are stored as 1536-dim vectors (np.array or list)\n",
        "\n",
        "This logger helps bridge semantic understanding and RL state input.\n",
        "Use it during interaction, simulation, or model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJsaoUqxuyQR"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from openai import OpenAI\n",
        "from scipy.spatial import distance\n",
        "import math\n",
        "import unittest\n",
        "from unittest.mock import MagicMock, patch, call\n",
        "import numpy as np\n",
        "import json # Import json for file operations\n",
        "import os # Import os for file path operations\n",
        "\n",
        "# print(\"## Improved UsageLogger Code and Tests\")\n",
        "# print(\"---\") # Separator for clarity\n",
        "\n",
        "# print(\"### UsageLogger Class Code\")\n",
        "\n",
        "class UsageLogger:\n",
        "    \"\"\"\n",
        "    Logs usage data for a thesis assistant, including user prompts, inferred\n",
        "    intents, and the relevant thesis stage.\n",
        "\n",
        "    It also integrates with the OpenAI API to generate embeddings for user\n",
        "    prompts, allowing for similarity searches to find related interactions.\n",
        "    Includes functionality for batch and continuous embedding generation.\n",
        "    Supports saving and loading embeddings to/from a file for persistence.\n",
        "\n",
        "    Attributes:\n",
        "        usage_logs (list): A list of dictionaries, where each dictionary\n",
        "                           represents a single usage log entry.\n",
        "        usage_embeddings (list): A list of dictionaries, where each dictionary\n",
        "                                 contains an embedding vector ('embedding')\n",
        "                                 and a reference to the original log entry's\n",
        "                                 index ('original_index').\n",
        "        client (OpenAI): An initialized OpenAI client instance used for API calls.\n",
        "        embeddings_file (str): The file path for storing usage embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, openai_client, embeddings_file=\"usage_embeddings.json\"):\n",
        "        \"\"\"\n",
        "        Opens UsageLogger with OpenAI client, empty lists for usage logs and\n",
        "        embeddings, and loads embeddings from a file if it exists.\n",
        "\n",
        "        Args:\n",
        "            openai_client: An initialized OpenAI client object for API interactions.\n",
        "            embeddings_file (str): File path for storing usage embeddings.\n",
        "        \"\"\"\n",
        "        self.usage_logs = []\n",
        "        self.usage_embeddings = []\n",
        "        self.client = openai_client\n",
        "        self.embeddings_file = embeddings_file\n",
        "        self._load_embeddings()\n",
        "\n",
        "\n",
        "    def _load_embeddings(self):\n",
        "        \"\"\"Loads usage embeddings from JSON file.\"\"\"\n",
        "        if os.path.exists(self.embeddings_file):\n",
        "            try:\n",
        "                with open(self.embeddings_file, 'r') as f:\n",
        "                    loaded_embeddings = json.load(f)\n",
        "                    for item in loaded_embeddings:\n",
        "                         if 'embedding' in item and isinstance(item['embedding'], list):\n",
        "                              item['embedding'] = np.array(item['embedding'])\n",
        "                    self.usage_embeddings = loaded_embeddings\n",
        "\n",
        "                # print(f\"Loaded {len(self.usage_embeddings)} embeddings from {self.embeddings_file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading embeddings from {self.embeddings_file}: {e}\")\n",
        "                self.usage_embeddings = []\n",
        "        else:\n",
        "            # print(f\"Embeddings file not found: {self.embeddings_file}. Starting with empty embeddings.\")\n",
        "            self.usage_embeddings = []\n",
        "\n",
        "\n",
        "    def _save_embeddings(self):\n",
        "        \"\"\"Saves the current usage embeddings to the specified JSON file.\"\"\"\n",
        "        try:\n",
        "            embeddings_to_save = []\n",
        "            for item in self.usage_embeddings:\n",
        "                 item_to_save = item.copy()\n",
        "                 if 'embedding' in item_to_save and isinstance(item_to_save['embedding'], np.ndarray):\n",
        "                      item_to_save['embedding'] = item_to_save['embedding'].tolist()\n",
        "                 embeddings_to_save.append(item_to_save)\n",
        "\n",
        "            with open(self.embeddings_file, 'w') as f:\n",
        "                json.dump(embeddings_to_save, f)\n",
        "            # print(f\"Saved {len(self.usage_embeddings)} embeddings to {self.embeddings_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving embeddings to {self.embeddings_file}: {e}\")\n",
        "\n",
        "\n",
        "    def log_usage(self, prompt, intent, thesis_stage=\"unknown\"):\n",
        "        \"\"\"\n",
        "        Logs new usage entry, generates embedding, and saves embeddings to file.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): User's input prompt text.\n",
        "            intent (str): Inferred intent of the prompt.\n",
        "            thesis_stage (str): Relevant thesis stage.\n",
        "        \"\"\"\n",
        "        log_entry = {\n",
        "            'timestamp': pd.Timestamp.now(),\n",
        "            'prompt': prompt,\n",
        "            'intent': intent,\n",
        "            'thesis_stage': thesis_stage\n",
        "        }\n",
        "        self.usage_logs.append(log_entry)\n",
        "        # print(f\"Usage logged: Timestamp={log_entry['timestamp']}, Prompt='{prompt}', Intent='{intent}', Thesis Stage='{thesis_stage}'\")\n",
        "\n",
        "        # print(\"Generating embedding for the new log entry...\")\n",
        "        self._generate_embedding_for_log(log_entry)\n",
        "\n",
        "\n",
        "    def _generate_embedding_for_log(self, log_entry):\n",
        "         \"\"\"\n",
        "         Generates embedding for a single log entry's prompt and stores it.\n",
        "\n",
        "         Args:\n",
        "             log_entry (dict): The log entry dictionary.\n",
        "         \"\"\"\n",
        "         try:\n",
        "             prompt_text = log_entry['prompt']\n",
        "             response = self.client.embeddings.create(\n",
        "                 model=\"text-embedding-3-small\",\n",
        "                 input=prompt_text\n",
        "             )\n",
        "             embedding = response.data[0].embedding\n",
        "             self.usage_embeddings.append({'embedding': embedding, 'original_index': len(self.usage_logs) - 1})\n",
        "             # print(f\"Generated embedding for log entry at index {len(self.usage_embeddings) - 1}\")\n",
        "\n",
        "             self._save_embeddings()\n",
        "\n",
        "         except Exception as e:\n",
        "             print(f\"Error generating embedding for log entry: {e}\")\n",
        "\n",
        "\n",
        "    def generate_all_usage_embeddings(self, batch_size=100):\n",
        "        \"\"\"\n",
        "        Generates embeddings for all logs without embeddings using OpenAI API.\n",
        "\n",
        "        Args:\n",
        "            batch_size (int): Number of log entries per API call batch.\n",
        "        \"\"\"\n",
        "        # print(f\"Generating embeddings for all usage logs using OpenAI API in batches of {batch_size}...\")\n",
        "        # Filter logs that do not have embeddings yet\n",
        "        logs_without_embeddings = []\n",
        "        existing_indices = {item['original_index'] for item in self.usage_embeddings}\n",
        "        for i, log_entry in enumerate(self.usage_logs):\n",
        "             if i not in existing_indices:\n",
        "                  logs_without_embeddings.append((i, log_entry))\n",
        "\n",
        "        if not logs_without_embeddings:\n",
        "             # print(\"All usage logs already have embeddings.\")\n",
        "             return\n",
        "\n",
        "        num_logs_to_process = len(logs_without_embeddings)\n",
        "        num_batches = math.ceil(num_logs_to_process / batch_size)\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start_index = i * batch_size\n",
        "            end_index = min((i + 1) * batch_size, num_logs_to_process)\n",
        "            batch_data = logs_without_embeddings[start_index:end_index]\n",
        "\n",
        "            batch_prompts = [log_entry['prompt'] for original_index, log_entry in batch_data]\n",
        "            batch_original_indices = [original_index for original_index, log_entry in batch_data]\n",
        "\n",
        "\n",
        "            if not batch_prompts:\n",
        "                # print(f\"Skipping empty batch {i+1}/{num_batches}.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                response = self.client.embeddings.create(\n",
        "                    model=\"text-embedding-3-small\",\n",
        "                    input=batch_prompts\n",
        "                )\n",
        "                for j, item in enumerate(response.data):\n",
        "                    self.usage_embeddings.append({\n",
        "                        'embedding': item.embedding,\n",
        "                        'original_index': batch_original_indices[j]\n",
        "                    })\n",
        "                # print(f\"Generated embeddings for batch {i+1}/{num_batches} ({start_index} to {end_index-1} of logs to process).\")\n",
        "            except Exception as e:\n",
        "                 print(f\"Error generating embeddings for batch {i+1}/{num_batches}: {e}\")\n",
        "\n",
        "        # print(f\"Generated {len(self.usage_embeddings) - len(existing_indices)} new embeddings. Total embeddings: {len(self.usage_embeddings)}\")\n",
        "\n",
        "        self._save_embeddings()\n",
        "\n",
        "\n",
        "    def find_similar_usage(self, query_prompt, n=3):\n",
        "        \"\"\"\n",
        "        Finds the n most similar usage logs based on embedding similarity.\n",
        "\n",
        "        Args:\n",
        "            query_prompt (str): Query prompt text.\n",
        "            n (int): Number of similar logs to return.\n",
        "\n",
        "        Returns:\n",
        "            list of dict: List of dictionaries for similar logs.\n",
        "        \"\"\"\n",
        "        if not self.usage_embeddings:\n",
        "            # print(\"No usage embeddings available to query. Please generate embeddings first.\")\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            query_response = self.client.embeddings.create(\n",
        "                model=\"text-embedding-3-small\",\n",
        "                input=query_prompt\n",
        "            )\n",
        "            query_embedding = query_response.data[0].embedding\n",
        "\n",
        "            distances = []\n",
        "            for item in self.usage_embeddings:\n",
        "                dist = distance.cosine(query_embedding, item['embedding'])\n",
        "                distances.append({\n",
        "                    \"distance\": dist,\n",
        "                    \"original_index\": item['original_index']\n",
        "                    })\n",
        "\n",
        "            distances_sorted = sorted(distances, key=lambda x: x['distance'])\n",
        "\n",
        "            similar_logs = []\n",
        "            for item in distances_sorted[0:n]:\n",
        "                if 0 <= item['original_index'] < len(self.usage_logs):\n",
        "                     original_log = self.usage_logs[item['original_index']]\n",
        "                     similar_logs.append({\n",
        "                         \"distance\": item['distance'],\n",
        "                         \"original_log\": original_log\n",
        "                     })\n",
        "                else:\n",
        "                    print(f\"Warning: Original log index {item['original_index']} out of bounds.\")\n",
        "\n",
        "            return similar_logs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during similar usage query: {e}\")\n",
        "            return []\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "8wjm_mRv6Pg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print(\"\\n### UsageLogger Test Cases\")\n",
        "\n",
        "class TestUsageLogger(unittest.TestCase):\n",
        "\n",
        "    @patch('__main__.UsageLogger._load_embeddings') # Patch _load_embeddings at the class level\n",
        "    def setUp(self, mock_load_embeddings):\n",
        "        \"\"\"Set up mock OpenAI client and UsageLogger instance before each test.\"\"\"\n",
        "        # print(\"\\nSetting up for a new test...\")\n",
        "        self.mock_openai_client = MagicMock()\n",
        "        self.mock_openai_client.embeddings = MagicMock()\n",
        "        self.test_embeddings_file = \"test_usage_embeddings.json\"\n",
        "        if os.path.exists(self.test_embeddings_file):\n",
        "            os.remove(self.test_embeddings_file)\n",
        "        # The UsageLogger will be initialized with _load_embeddings mocked\n",
        "        self.usage_logger = UsageLogger(openai_client=self.mock_openai_client, embeddings_file=self.test_embeddings_file)\n",
        "\n",
        "        # Ensure usage_embeddings is empty at the start of each test due to the patch\n",
        "        self.assertEqual(len(self.usage_logger.usage_embeddings), 0, \"_load_embeddings should be mocked and leave usage_embeddings empty\")\n",
        "        # print(\"Setup complete.\")\n",
        "\n",
        "\n",
        "    def tearDown(self):\n",
        "        \"\"\"Clean up the test embeddings file after each test.\"\"\"\n",
        "        # print(\"Cleaning up after test...\")\n",
        "        if os.path.exists(self.test_embeddings_file):\n",
        "            os.remove(self.test_embeddings_file)\n",
        "        print(\"Cleanup complete.\")\n",
        "\n",
        "\n",
        "    @patch('__main__.UsageLogger._save_embeddings') # Patch save_embeddings to prevent file writes during this test\n",
        "    def test_log_usage(self, mock_save_embeddings):\n",
        "        \"\"\"Test that log_usage correctly adds an entry and generates embedding.\"\"\"\n",
        "        # print(\"\\nTesting log_usage method...\")\n",
        "        prompt = \"How do I write a literature review?\"\n",
        "        intent = \"writing_support\"\n",
        "        thesis_stage = \"literature review\"\n",
        "\n",
        "        mock_embedding_vector = [0.1] * 1536\n",
        "        self.mock_openai_client.embeddings.create.return_value = MagicMock(data=[MagicMock(embedding=mock_embedding_vector)])\n",
        "        # print(f\"Mocked OpenAI embeddings.create to return embedding vector: {mock_embedding_vector[:5]}...\")\n",
        "\n",
        "        self.usage_logger.log_usage(prompt, intent, thesis_stage)\n",
        "\n",
        "        self.assertEqual(len(self.usage_logger.usage_logs), 1)\n",
        "        self.assertEqual(len(self.usage_logger.usage_embeddings), 1)\n",
        "\n",
        "        log_entry = self.usage_logger.usage_logs[0]\n",
        "        self.assertEqual(log_entry['prompt'], prompt)\n",
        "        self.assertEqual(log_entry['intent'], intent)\n",
        "        self.assertEqual(log_entry['thesis_stage'], thesis_stage)\n",
        "        self.assertIsInstance(log_entry['timestamp'], pd.Timestamp)\n",
        "\n",
        "        embedding_entry = self.usage_logger.usage_embeddings[0]\n",
        "        self.assertEqual(embedding_entry['original_index'], 0)\n",
        "        self.assertTrue(np.array_equal(embedding_entry['embedding'], mock_embedding_vector))\n",
        "\n",
        "        self.mock_openai_client.embeddings.create.assert_called_once_with(\n",
        "            model=\"text-embedding-3-small\",\n",
        "            input=prompt\n",
        "        )\n",
        "        mock_save_embeddings.assert_called_once()\n",
        "        # print(\"log_usage test completed.\")\n",
        "\n",
        "\n",
        "    @patch('builtins.print')\n",
        "    @patch('__main__.UsageLogger._save_embeddings') # Patch save_embeddings to prevent file writes during this test\n",
        "    def test_generate_all_usage_embeddings(self, mock_save_embeddings, mock_print):\n",
        "        \"\"\"Test that generate_all_usage_embeddings generates and saves embeddings in batches.\"\"\"\n",
        "        # print(\"\\nTesting generate_all_usage_embeddings method...\")\n",
        "        mock_embedding_vector = [0.1] * 1536\n",
        "        # Configure the mock create method to return a response with embeddings for each item in the input list\n",
        "        def mock_create_embeddings(model, input):\n",
        "            if isinstance(input, list):\n",
        "                # Return a list of mock data objects, each with the mock embedding\n",
        "                return MagicMock(data=[MagicMock(embedding=mock_embedding_vector) for _ in input])\n",
        "            else:\n",
        "                # This case should not be hit by generate_all_usage_embeddings with batching\n",
        "                return MagicMock(data=[MagicMock(embedding=mock_embedding_vector)])\n",
        "\n",
        "        self.mock_openai_client.embeddings.create.side_effect = mock_create_embeddings\n",
        "        # print(f\"Mocked OpenAI embeddings.create to return embedding vector: {mock_embedding_vector[:5]}...\")\n",
        "\n",
        "        # Add some usage logs directly without calling log_usage\n",
        "        self.usage_logger.usage_logs.extend([\n",
        "            {'timestamp': pd.Timestamp.now(), 'prompt': 'Log 1', 'intent': 'intent1', 'thesis_stage': 'stage1'},\n",
        "            {'timestamp': pd.Timestamp.now(), 'prompt': 'Log 2', 'intent': 'intent2', 'thesis_stage': 'stage2'},\n",
        "            {'timestamp': pd.Timestamp.now(), 'prompt': 'Log 3', 'intent': 'intent3', 'thesis_stage': 'stage3'},\n",
        "            {'timestamp': pd.Timestamp.now(), 'prompt': 'Log 4', 'intent': 'intent4', 'thesis_stage': 'stage4'},\n",
        "            {'timestamp': pd.Timestamp.now(), 'prompt': 'Log 5', 'intent': 'intent5', 'thesis_stage': 'stage5'},\n",
        "        ])\n",
        "\n",
        "\n",
        "        # Ensure usage_embeddings is empty before calling generate_all_usage_embeddings\n",
        "        self.usage_logger.usage_embeddings = []\n",
        "\n",
        "        # print(f\"Added {len(self.usage_logger.usage_logs)} usage logs directly. usage_embeddings size: {len(self.usage_logger.usage_embeddings)}\")\n",
        "\n",
        "\n",
        "        # Generate embeddings in batches\n",
        "        self.usage_logger.generate_all_usage_embeddings(batch_size=2)\n",
        "        # print(\"Called generate_all_usage_embeddings with batch_size=2.\")\n",
        "\n",
        "        # Assertions\n",
        "        self.assertEqual(len(self.usage_logger.usage_embeddings), 5)\n",
        "\n",
        "        self.assertEqual(self.mock_openai_client.embeddings.create.call_count, 3) # 5 logs, batch_size 2 -> 3 calls\n",
        "\n",
        "        # Manually check the call arguments\n",
        "        actual_calls = self.mock_openai_client.embeddings.create.call_args_list\n",
        "        self.assertEqual(len(actual_calls), 3)\n",
        "\n",
        "        # Expected inputs based on batch size 2\n",
        "        expected_inputs = [\n",
        "            [\"Log 1\", \"Log 2\"],\n",
        "            [\"Log 3\", \"Log 4\"],\n",
        "            [\"Log 5\"],\n",
        "        ]\n",
        "\n",
        "        for i, expected_input in enumerate(expected_inputs):\n",
        "            # Check the positional and keyword arguments of each call\n",
        "            # The call object stores args as a tuple and kwargs as a dictionary\n",
        "            self.assertEqual(actual_calls[i][0], ()) # No positional args expected\n",
        "            self.assertEqual(actual_calls[i][1]['model'], \"text-embedding-3-small\")\n",
        "            self.assertEqual(actual_calls[i][1]['input'], expected_input)\n",
        "\n",
        "\n",
        "        for i, embedding_entry in enumerate(self.usage_logger.usage_embeddings):\n",
        "            self.assertIn('embedding', embedding_entry)\n",
        "            self.assertIn('original_index', embedding_entry)\n",
        "            # Note: Due to any_order=True in assert_has_calls, we can't strictly check original_index order this way\n",
        "            self.assertTrue(np.array_equal(embedding_entry['embedding'], mock_embedding_vector))\n",
        "            self.assertTrue(0 <= embedding_entry['original_index'] < len(self.usage_logger.usage_logs))\n",
        "\n",
        "        mock_save_embeddings.assert_called_once()\n",
        "        # print(\"generate_all_usage_embeddings test completed.\")\n",
        "\n",
        "    @patch('builtins.print')\n",
        "    @patch('__main__.UsageLogger._save_embeddings') # Patch save_embeddings\n",
        "    def test_generate_all_usage_embeddings_empty_batch(self, mock_save_embeddings, mock_print):\n",
        "        \"\"\"Test generate_all_usage_embeddings with an empty or small batch scenario.\"\"\"\n",
        "        # print(\"\\nTesting generate_all_usage_embeddings with an empty or small batch scenario...\")\n",
        "        mock_embedding_vector = [0.1] * 1536\n",
        "        # Configure the mock create method to return a response with embeddings for each item in the input list\n",
        "        def mock_create_embeddings(model, input):\n",
        "            if isinstance(input, list):\n",
        "                return MagicMock(data=[MagicMock(embedding=mock_embedding_vector) for _ in input])\n",
        "            else:\n",
        "                return MagicMock(data=[MagicMock(embedding=mock_embedding_vector)])\n",
        "\n",
        "        self.mock_openai_client.embeddings.create.side_effect = mock_create_embeddings\n",
        "\n",
        "        # print(f\"Mocked OpenAI embeddings.create to return embedding vector: {mock_embedding_vector[:5]}...\")\n",
        "\n",
        "        # Scenario 1: Empty logs\n",
        "        # print(\"Testing with no logs...\")\n",
        "        self.usage_logger.generate_all_usage_embeddings(batch_size=5)\n",
        "        self.assertEqual(len(self.usage_logger.usage_embeddings), 0)\n",
        "        self.mock_openai_client.embeddings.create.assert_not_called()\n",
        "        mock_save_embeddings.assert_not_called() # No embeddings to save\n",
        "        # print(\"Assertion passed: No embeddings generated and API not called for empty logs.\")\n",
        "\n",
        "        # Scenario 2: Single log, batch size larger than log count\n",
        "        # print(\"Testing with a single log and larger batch size...\")\n",
        "        # Add a single log directly\n",
        "        self.usage_logger.usage_logs.append({'timestamp': pd.Timestamp.now(), 'prompt': 'Single Log', 'intent': 'single_intent', 'thesis_stage': 'single_stage'})\n",
        "        self.assertEqual(len(self.usage_logger.usage_logs), 1)\n",
        "\n",
        "        # Ensure usage_embeddings is empty before calling generate_all_usage_embeddings\n",
        "        self.usage_logger.usage_embeddings = []\n",
        "\n",
        "\n",
        "        self.mock_openai_client.embeddings.create.reset_mock()\n",
        "        mock_save_embeddings.reset_mock()\n",
        "\n",
        "        self.usage_logger.generate_all_usage_embeddings(batch_size=5)\n",
        "        self.assertEqual(len(self.usage_logger.usage_embeddings), 1)\n",
        "        self.mock_openai_client.embeddings.create.assert_called_once_with(model=\"text-embedding-3-small\", input=[\"Single Log\"])\n",
        "        mock_save_embeddings.assert_called_once()\n",
        "        # print(\"Assertion passed: Correctly handled single log with larger batch size.\")\n",
        "\n",
        "        # print(\"generate_all_usage_embeddings empty batch scenario test completed.\")\n",
        "\n",
        "\n",
        "    @patch('builtins.print')\n",
        "    @patch('__main__.UsageLogger._save_embeddings') # Patch save_embeddings\n",
        "    def test_find_similar_usage(self, mock_save_embeddings, mock_print):\n",
        "        \"\"\"Test that find_similar_usage correctly finds similar logs based on mock embeddings.\"\"\"\n",
        "        # print(\"\\nTesting find_similar_usage method...\")\n",
        "\n",
        "        # Manually set usage logs (these will be used to retrieve original logs)\n",
        "        self.usage_logger.usage_logs.extend([\n",
        "            {'timestamp': pd.Timestamp.now(), 'prompt': 'Log entry 0 content (related to literature)', 'intent': 'research', 'thesis_stage': 'literature review'},\n",
        "            {'timestamp': pd.Timestamp.now(), 'prompt': 'Log entry 1 content (related to methodology)', 'intent': 'methodology', 'thesis_stage': 'methodology'},\n",
        "            {'timestamp': pd.Timestamp.now(), 'prompt': 'Log entry 2 content (more literature review)', 'intent': 'writing', 'thesis_stage': 'literature review'},\n",
        "            {'timestamp': pd.Timestamp.now(), 'prompt': 'Log entry 3 content (data analysis)', 'intent': 'analysis', 'thesis_stage': 'results'},\n",
        "        ])\n",
        "\n",
        "        # print(f\"Manually added {len(self.usage_logger.usage_logs)} usage logs for similarity search test.\")\n",
        "\n",
        "        # Manually set query embedding\n",
        "        query_embedding_vector = np.array([0.1, 0.2, 0.3] + [0] * 1533) # Vector designed to be close to literature review embeddings\n",
        "        self.mock_openai_client.embeddings.create.return_value = MagicMock(data=[MagicMock(embedding=query_embedding_vector.tolist())]) # Return as list for mock\n",
        "        # print(f\"Mocked OpenAI embeddings.create for query to return embedding (first few values): {query_embedding_vector[:5]}...\")\n",
        "\n",
        "\n",
        "        # Manually set usage embeddings for controlled testing, ensuring literature review logs are closest\n",
        "        embedding_log1 = np.array([0.11, 0.21, 0.31] + [0] * 1533) # Very close to query_embedding\n",
        "        embedding_log2 = np.array([0.9, 0.8, 0.7] + [0] * 1533)    # Significantly different\n",
        "        embedding_log3 = np.array([0.13, 0.23, 0.33] + [0] * 1533) # Also very close to query_embedding\n",
        "        embedding_another = np.array([0.85, 0.75, 0.65] + [0] * 1533) # Significantly different\n",
        "\n",
        "        self.usage_logger.usage_embeddings = [\n",
        "            {'embedding': embedding_log1, 'original_index': 0}, # Literature review log\n",
        "            {'embedding': embedding_log2, 'original_index': 1}, # Methodology log\n",
        "            {'embedding': embedding_log3, 'original_index': 2}, # Another literature review log\n",
        "            {'embedding': embedding_another, 'original_index': 3}, # Data analysis log\n",
        "        ]\n",
        "        # Save these manual embeddings to the test file so _load_embeddings on subsequent calls works\n",
        "        self.usage_logger._save_embeddings()\n",
        "        # print(\"Manually set and saved usage embeddings for testing.\")\n",
        "\n",
        "\n",
        "        # Find similar usage (requesting top 2)\n",
        "        query_prompt = \"Query about literature review\"\n",
        "        # print(f\"Finding similar usage for query: '{query_prompt}' (n=2)\")\n",
        "        similar_logs = self.usage_logger.find_similar_usage(query_prompt, n=2)\n",
        "        # print(f\"Found {len(similar_logs)} similar logs.\")\n",
        "\n",
        "\n",
        "        # Assertions\n",
        "        self.mock_openai_client.embeddings.create.assert_called_once_with(model=\"text-embedding-3-small\", input=query_prompt)\n",
        "\n",
        "        self.assertEqual(len(similar_logs), 2)\n",
        "\n",
        "        # Calculate expected closest logs based on cosine distance\n",
        "        query_embedding = np.array(query_embedding_vector)\n",
        "        distances = []\n",
        "        for item in self.usage_logger.usage_embeddings:\n",
        "            dist = distance.cosine(query_embedding, item['embedding'])\n",
        "            distances.append({\n",
        "                \"distance\": dist,\n",
        "                \"original_index\": item['original_index']\n",
        "                })\n",
        "\n",
        "        # Sort by distance and then by original_index for deterministic order in case of tie\n",
        "        expected_closest = sorted(distances, key=lambda x: (x['distance'], x['original_index']))[:2]\n",
        "        # print(f\"Expected closest logs (distance, index): {expected_closest}\")\n",
        "\n",
        "        # Get the prompts for the expected closest logs\n",
        "        expected_log_prompts = [self.usage_logger.usage_logs[item['original_index']]['prompt'] for item in expected_closest]\n",
        "        # Get the prompts for the returned similar logs\n",
        "        returned_log_prompts = [log['original_log']['prompt'] for log in similar_logs]\n",
        "\n",
        "        # Assert that the returned prompts match the expected prompts\n",
        "        self.assertEqual(returned_log_prompts, expected_log_prompts)\n",
        "\n",
        "        # Also assert that the distances in the returned logs are close to the expected distances\n",
        "        self.assertTrue(np.isclose(similar_logs[0]['distance'], expected_closest[0]['distance']))\n",
        "        self.assertTrue(np.isclose(similar_logs[1]['distance'], expected_closest[1]['distance']))\n",
        "\n",
        "\n",
        "        # print(\"find_similar_usage test completed.\")\n",
        "\n",
        "\n",
        "    @patch('builtins.print')\n",
        "    def test_find_similar_usage_no_embeddings(self, mock_print):\n",
        "        \"\"\"Test that find_similar_usage returns empty list if no embeddings exist.\"\"\"\n",
        "        # print(\"\\nTesting find_similar_usage with no embeddings...\")\n",
        "        self.assertEqual(len(self.usage_logger.usage_embeddings), 0)\n",
        "\n",
        "        query_prompt = \"Some query\"\n",
        "        # print(f\"Finding similar usage for query: '{query_prompt}' with no embeddings.\")\n",
        "        similar_logs = self.usage_logger.find_similar_usage(query_prompt)\n",
        "        # print(f\"Found {len(similar_logs)} similar logs.\")\n",
        "\n",
        "        self.assertEqual(len(similar_logs), 0)\n",
        "        self.mock_openai_client.embeddings.create.assert_not_called()\n",
        "        # print(\"find_similar_usage with no embeddings test completed.\")\n",
        "\n",
        "\n",
        "    @patch('builtins.print')\n",
        "    def test_find_similar_usage_with_invalid_index(self, mock_print):\n",
        "        \"\"\"Test find_similar_usage handles cases where original_index might be invalid.\"\"\"\n",
        "        # print(\"\\nTest find_similar_usage handles invalid original_index...\")\n",
        "        self.usage_logger.usage_logs.extend([\n",
        "            {'timestamp': pd.Timestamp.now(), 'prompt': 'Valid Log 1', 'intent': 'test', 'thesis_stage': 'test'},\n",
        "            {'timestamp': pd.Timestamp.now(), 'prompt': 'Valid Log 2', 'intent': 'test', 'thesis_stage': 'test'}\n",
        "        ])\n",
        "        # print(f\"Manually added {len(self.usage_logger.usage_logs)} usage logs.\")\n",
        "\n",
        "        mock_embedding_vector = np.array([0.5] * 1536)\n",
        "        self.usage_logger.usage_embeddings = [\n",
        "            {'embedding': mock_embedding_vector, 'original_index': 0}, # Valid index\n",
        "            {'embedding': mock_embedding_vector, 'original_index': 99}, # Invalid index\n",
        "            {'embedding': mock_embedding_vector, 'original_index': 1}, # Valid index\n",
        "        ]\n",
        "        # print(f\"Manually set {len(self.usage_logger.usage_embeddings)} usage embeddings, including one with an invalid index.\")\n",
        "\n",
        "\n",
        "        query_embedding = np.array([0.51] * 1536)\n",
        "        self.mock_openai_client.embeddings.create.return_value = MagicMock(data=[MagicMock(embedding=query_embedding)])\n",
        "        # print(f\"Mocked OpenAI embeddings.create for query to return embedding (first few values): {query_embedding[:5]}...\")\n",
        "\n",
        "\n",
        "        query_prompt = \"Query text\"\n",
        "        # print(f\"Attempting to find similar usage for query: '{query_prompt}' with invalid index present.\")\n",
        "        similar_logs = self.usage_logger.find_similar_usage(query_prompt, n=3)\n",
        "        # print(f\"Found {len(similar_logs)} similar logs.\")\n",
        "\n",
        "        self.mock_openai_client.embeddings.create.assert_called_once_with(model=\"text-embedding-3-small\", input=query_prompt)\n",
        "\n",
        "        self.assertEqual(len(similar_logs), 2)\n",
        "\n",
        "        returned_original_indices = [log['original_log']['prompt'] for log in similar_logs]\n",
        "        expected_valid_prompts = ['Valid Log 1', 'Valid Log 2']\n",
        "        self.assertEqual(sorted(returned_original_indices), sorted(expected_valid_prompts))\n",
        "\n",
        "        mock_print.assert_any_call(f\"Warning: Original log index 99 out of bounds.\")\n",
        "        # print(\"find_similar_usage with invalid index test completed.\")\n",
        "\n",
        "\n",
        "    @patch('builtins.print')\n",
        "    def test_continuous_embedding_generation(self, mock_print):\n",
        "        \"\"\"Test that embeddings are generated automatically upon logging and saved.\"\"\"\n",
        "        # print(\"\\nTesting continuous embedding generation via log_usage...\")\n",
        "        mock_embedding_vector = [0.5] * 1536\n",
        "        self.mock_openai_client.embeddings.create.return_value = MagicMock(data=[MagicMock(embedding=mock_embedding_vector)])\n",
        "        # print(f\"Mocked OpenAI embeddings.create to return embedding vector (first few values): {mock_embedding_vector[:5]}...\")\n",
        "\n",
        "        self.usage_logger.log_usage(\"Prompt 1\", \"intent1\", \"stage1\")\n",
        "        self.usage_logger.log_usage(\"Prompt 2\", \"intent2\", \"stage2\")\n",
        "        self.usage_logger.log_usage(\"Prompt 3\", \"intent3\", \"stage3\")\n",
        "        # print(f\"Finished logging {len(self.usage_logger.usage_logs)} usage logs.\")\n",
        "\n",
        "        self.assertEqual(len(self.usage_logger.usage_embeddings), 3)\n",
        "        self.assertEqual(self.mock_openai_client.embeddings.create.call_count, 3)\n",
        "\n",
        "        for i in range(3):\n",
        "            self.assertEqual(self.usage_logger.usage_embeddings[i]['original_index'], i)\n",
        "            self.assertTrue(np.array_equal(self.usage_logger.usage_embeddings[i]['embedding'], mock_embedding_vector))\n",
        "\n",
        "        self.assertTrue(os.path.exists(self.test_embeddings_file))\n",
        "        with open(self.test_embeddings_file, 'r') as f:\n",
        "            saved_embeddings = json.load(f)\n",
        "        self.assertEqual(len(saved_embeddings), 3)\n",
        "        for i in range(3):\n",
        "             self.assertEqual(saved_embeddings[i]['original_index'], i)\n",
        "             self.assertTrue(np.array_equal(np.array(saved_embeddings[i]['embedding']), mock_embedding_vector))\n",
        "        # print(\"continuous embedding generation test completed.\")\n",
        "\n",
        "\n",
        "    @patch('builtins.print')\n",
        "    def test_load_embeddings_on_init(self, mock_print):\n",
        "        \"\"\"Test that embeddings are loaded from file upon initialization.\"\"\"\n",
        "        # print(\"\\nTesting loading embeddings on initialization...\")\n",
        "        dummy_embeddings = [\n",
        "            {'embedding': [0.1] * 1536, 'original_index': 0},\n",
        "            {'embedding': [0.2] * 1536, 'original_index': 1},\n",
        "        ]\n",
        "        with open(self.test_embeddings_file, 'w') as f:\n",
        "            json.dump(dummy_embeddings, f)\n",
        "        # print(f\"Created dummy embeddings file: {self.test_embeddings_file} with {len(dummy_embeddings)} entries.\")\n",
        "\n",
        "        # Initialize a new UsageLogger instance - _load_embeddings is NOT patched here\n",
        "        new_usage_logger = UsageLogger(openai_client=self.mock_openai_client, embeddings_file=self.test_embeddings_file)\n",
        "        # print(\"Initialized a new UsageLogger instance.\")\n",
        "\n",
        "        self.assertEqual(len(new_usage_logger.usage_embeddings), 2)\n",
        "\n",
        "        self.assertEqual(new_usage_logger.usage_embeddings[0]['original_index'], 0)\n",
        "        self.assertTrue(np.array_equal(new_usage_logger.usage_embeddings[0]['embedding'], np.array([0.1] * 1536)))\n",
        "        self.assertEqual(new_usage_logger.usage_embeddings[1]['original_index'], 1)\n",
        "        self.assertTrue(np.array_equal(new_usage_logger.usage_embeddings[1]['embedding'], np.array([0.2] * 1536)))\n",
        "\n",
        "        if os.path.exists(self.test_embeddings_file):\n",
        "            os.remove(self.test_embeddings_file)\n",
        "        # print(\"Cleaned up dummy embeddings file.\")\n",
        "\n",
        "        # print(\"Loading embeddings on initialization test completed.\")\n",
        "\n",
        "\n",
        "    @patch('builtins.print')\n",
        "    def test_generate_all_usage_embeddings_no_logs(self, mock_print):\n",
        "        \"\"\"Test generate_all_usage_embeddings when there are no logs.\"\"\"\n",
        "        # print(\"\\nTesting generate_all_usage_embeddings with no logs...\")\n",
        "        self.assertEqual(len(self.usage_logger.usage_logs), 0)\n",
        "\n",
        "        self.usage_logger.generate_all_usage_embeddings()\n",
        "        # print(\"Called generate_all_usage_embeddings.\")\n",
        "\n",
        "        self.assertEqual(len(self.usage_logger.usage_embeddings), 0)\n",
        "        self.mock_openai_client.embeddings.create.assert_not_called()\n",
        "        self.assertFalse(os.path.exists(self.test_embeddings_file))\n",
        "        # print(\"generate_all_usage_embeddings with no logs test completed.\")\n",
        "\n",
        "\n",
        "# This allows running the tests in a notebook environment\n",
        "# Note: In a standard Python script, you would use unittest.main()\n",
        "if __name__ == '__main__':\n",
        "    runner = unittest.TextTestRunner(verbosity=2)\n",
        "    suite = unittest.TestLoader().loadTestsFromTestCase(TestUsageLogger)\n",
        "    # print(\"\\n--- Running UsageLogger Tests ---\")\n",
        "    runner.run(suite)\n",
        "    # print(\"--- Finished Running UsageLogger Tests ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFTHZEzy6Jls",
        "outputId": "facd6e23-3294-465b-d636-b810a2f9556b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_continuous_embedding_generation (__main__.TestUsageLogger.test_continuous_embedding_generation)\n",
            "Test that embeddings are generated automatically upon logging and saved. ... ok\n",
            "test_find_similar_usage (__main__.TestUsageLogger.test_find_similar_usage)\n",
            "Test that find_similar_usage correctly finds similar logs based on mock embeddings. ... ok\n",
            "test_find_similar_usage_no_embeddings (__main__.TestUsageLogger.test_find_similar_usage_no_embeddings)\n",
            "Test that find_similar_usage returns empty list if no embeddings exist. ... ok\n",
            "test_find_similar_usage_with_invalid_index (__main__.TestUsageLogger.test_find_similar_usage_with_invalid_index)\n",
            "Test find_similar_usage handles cases where original_index might be invalid. ... ok\n",
            "test_generate_all_usage_embeddings (__main__.TestUsageLogger.test_generate_all_usage_embeddings)\n",
            "Test that generate_all_usage_embeddings generates and saves embeddings in batches. ... ok\n",
            "test_generate_all_usage_embeddings_empty_batch (__main__.TestUsageLogger.test_generate_all_usage_embeddings_empty_batch)\n",
            "Test generate_all_usage_embeddings with an empty or small batch scenario. ... ok\n",
            "test_generate_all_usage_embeddings_no_logs (__main__.TestUsageLogger.test_generate_all_usage_embeddings_no_logs)\n",
            "Test generate_all_usage_embeddings when there are no logs. ... ok\n",
            "test_load_embeddings_on_init (__main__.TestUsageLogger.test_load_embeddings_on_init)\n",
            "Test that embeddings are loaded from file upon initialization. ... ok\n",
            "test_log_usage (__main__.TestUsageLogger.test_log_usage)\n",
            "Test that log_usage correctly adds an entry and generates embedding. ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 9 tests in 0.063s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleanup complete.\n",
            "Cleanup complete.\n",
            "Cleanup complete.\n",
            "Cleanup complete.\n",
            "Cleanup complete.\n",
            "Cleanup complete.\n",
            "Cleanup complete.\n",
            "Cleanup complete.\n",
            "Cleanup complete.\n"
          ]
        }
      ]
    }
  ]
}